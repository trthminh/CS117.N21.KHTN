{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IDMD29aCuK11",
        "BXBl_KAWtrP0",
        "aZzCkKo-utwm",
        "dAlXSAURuyCV",
        "lUwA8fMyvAmV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Prepare"
      ],
      "metadata": {
        "id": "w5uj6UlQtFnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create something"
      ],
      "metadata": {
        "id": "3w9JB5JitexE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir human_input\n",
        "!mkdir shirt_input\n",
        "!mkdir our_data_folder\n",
        "%cd our_data_folder\n",
        "!mkdir test\n",
        "%cd test\n",
        "!mkdir cloth\n",
        "!mkdir cloth-mask\n",
        "!mkdir image\n",
        "!mkdir image-densepose\n",
        "!mkdir image-parse-agnostic-v3.2\n",
        "!mkdir image-parse-v3\n",
        "!mkdir openpose_img\n",
        "!mkdir openpose_json\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2yf5o_Dtgwo",
        "outputId": "08b6ef11-be74-44f0-9e8a-942452057b38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/our_data_folder\n",
            "/content/our_data_folder/test\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Openpose"
      ],
      "metadata": {
        "id": "6alLvutCtHrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/CMU-Perceptual-Computing-Lab/openpose.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  # see: https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949\n",
        "  # install new CMake becaue of CUDA10\n",
        "  !wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "  !tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "  # clone openpose\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  !sed -i 's/execute_process(COMMAND git checkout master WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/execute_process(COMMAND git checkout f019d0dfe86f49d1140961f8c7dec22130c83154 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/g' openpose/CMakeLists.txt\n",
        "  # install system dependencies\n",
        "  !apt-get -qq install -y libatlas-base-dev libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler libgflags-dev libgoogle-glog-dev liblmdb-dev opencl-headers ocl-icd-opencl-dev libviennacl-dev\n",
        "  # install python dependencies\n",
        "  !pip install -q youtube-dl\n",
        "  # build openpose\n",
        "  !cd openpose && rm -rf build || true && mkdir build && cd build && cmake .. -DUSE_CUDNN=OFF && make -j`nproc`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjfv_7m7tKuS",
        "outputId": "caa9c915-5624-4dbb-ccd0-3e9b84dac596"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Selecting previously unselected package libgflags2.2.\n",
            "(Reading database ... 122349 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libgflags2.2_2.2.2-1build1_amd64.deb ...\n",
            "Unpacking libgflags2.2 (2.2.2-1build1) ...\n",
            "Selecting previously unselected package libgflags-dev.\n",
            "Preparing to unpack .../01-libgflags-dev_2.2.2-1build1_amd64.deb ...\n",
            "Unpacking libgflags-dev (2.2.2-1build1) ...\n",
            "Selecting previously unselected package libgoogle-glog0v5.\n",
            "Preparing to unpack .../02-libgoogle-glog0v5_0.4.0-1build1_amd64.deb ...\n",
            "Unpacking libgoogle-glog0v5 (0.4.0-1build1) ...\n",
            "Selecting previously unselected package libgoogle-glog-dev.\n",
            "Preparing to unpack .../03-libgoogle-glog-dev_0.4.0-1build1_amd64.deb ...\n",
            "Unpacking libgoogle-glog-dev (0.4.0-1build1) ...\n",
            "Selecting previously unselected package libleveldb1d:amd64.\n",
            "Preparing to unpack .../04-libleveldb1d_1.22-3ubuntu2_amd64.deb ...\n",
            "Unpacking libleveldb1d:amd64 (1.22-3ubuntu2) ...\n",
            "Selecting previously unselected package libleveldb-dev:amd64.\n",
            "Preparing to unpack .../05-libleveldb-dev_1.22-3ubuntu2_amd64.deb ...\n",
            "Unpacking libleveldb-dev:amd64 (1.22-3ubuntu2) ...\n",
            "Selecting previously unselected package liblmdb-dev:amd64.\n",
            "Preparing to unpack .../06-liblmdb-dev_0.9.24-1_amd64.deb ...\n",
            "Unpacking liblmdb-dev:amd64 (0.9.24-1) ...\n",
            "Selecting previously unselected package libprotobuf-lite17:amd64.\n",
            "Preparing to unpack .../07-libprotobuf-lite17_3.6.1.3-2ubuntu5.2_amd64.deb ...\n",
            "Unpacking libprotobuf-lite17:amd64 (3.6.1.3-2ubuntu5.2) ...\n",
            "Selecting previously unselected package lmdb-doc.\n",
            "Preparing to unpack .../08-lmdb-doc_0.9.24-1_all.deb ...\n",
            "Unpacking lmdb-doc (0.9.24-1) ...\n",
            "Selecting previously unselected package libprotobuf-dev:amd64.\n",
            "Preparing to unpack .../09-libprotobuf-dev_3.6.1.3-2ubuntu5.2_amd64.deb ...\n",
            "Unpacking libprotobuf-dev:amd64 (3.6.1.3-2ubuntu5.2) ...\n",
            "Selecting previously unselected package libsnappy-dev:amd64.\n",
            "Preparing to unpack .../10-libsnappy-dev_1.1.8-1build1_amd64.deb ...\n",
            "Unpacking libsnappy-dev:amd64 (1.1.8-1build1) ...\n",
            "Selecting previously unselected package libviennacl-dev.\n",
            "Preparing to unpack .../11-libviennacl-dev_1.7.1+dfsg1-5ubuntu1_all.deb ...\n",
            "Unpacking libviennacl-dev (1.7.1+dfsg1-5ubuntu1) ...\n",
            "Selecting previously unselected package opencl-clhpp-headers.\n",
            "Preparing to unpack .../12-opencl-clhpp-headers_2.1.0~~git51-gc5063c3-1_all.deb ...\n",
            "Unpacking opencl-clhpp-headers (2.1.0~~git51-gc5063c3-1) ...\n",
            "Selecting previously unselected package opencl-headers.\n",
            "Preparing to unpack .../13-opencl-headers_2.2~2019.08.06-g0d5f18c-1_all.deb ...\n",
            "Unpacking opencl-headers (2.2~2019.08.06-g0d5f18c-1) ...\n",
            "Setting up libleveldb1d:amd64 (1.22-3ubuntu2) ...\n",
            "Setting up libleveldb-dev:amd64 (1.22-3ubuntu2) ...\n",
            "Setting up libprotobuf-lite17:amd64 (3.6.1.3-2ubuntu5.2) ...\n",
            "Setting up libsnappy-dev:amd64 (1.1.8-1build1) ...\n",
            "Setting up libprotobuf-dev:amd64 (3.6.1.3-2ubuntu5.2) ...\n",
            "Setting up lmdb-doc (0.9.24-1) ...\n",
            "Setting up opencl-clhpp-headers (2.1.0~~git51-gc5063c3-1) ...\n",
            "Setting up liblmdb-dev:amd64 (0.9.24-1) ...\n",
            "Setting up libviennacl-dev (1.7.1+dfsg1-5ubuntu1) ...\n",
            "Setting up libgflags2.2 (2.2.2-1build1) ...\n",
            "Setting up opencl-headers (2.2~2019.08.06-g0d5f18c-1) ...\n",
            "Setting up libgflags-dev (2.2.2-1build1) ...\n",
            "Setting up libgoogle-glog0v5 (0.4.0-1build1) ...\n",
            "Setting up libgoogle-glog-dev (0.4.0-1build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- GCC detected, adding compile flags\n",
            "-- GCC detected, adding compile flags\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Found CUDA: /usr/local/cuda (found version \"11.8\") \n",
            "-- Building with CUDA.\n",
            "-- CUDA detected: 11.8\n",
            "-- Added CUDA NVCC flags for: sm_75\n",
            "-- Found cuDNN: ver. 8.7.0 found (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
            "-- Found GFlags: /usr/include  \n",
            "-- Found gflags  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libgflags.so)\n",
            "-- Found Glog: /usr/include  \n",
            "-- Found glog    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)\n",
            "-- Found Protobuf: /usr/lib/x86_64-linux-gnu/libprotobuf.so;-lpthread (found version \"3.6.1\") \n",
            "-- Found OpenCV: /usr (found version \"4.2.0\") \n",
            "-- Caffe will be downloaded from source now. NOTE: This process might take several minutes depending\n",
            "        on your internet connection.\n",
            "Submodule '3rdparty/caffe' (https://github.com/CMU-Perceptual-Computing-Lab/caffe.git) registered for path '../3rdparty/caffe'\n",
            "Cloning into '/content/openpose/3rdparty/caffe'...\n",
            "Submodule path '../3rdparty/caffe': checked out 'b5ede488952e40861e84e51a9f9fd8fe2395cc8a'\n",
            "Previous HEAD position was b5ede488 Added TX2 JetPack3.3 support\n",
            "HEAD is now at 1807aada Added Ampere arch's (CUDA11)\n",
            "-- Caffe will be built from source now.\n",
            "-- Download the models.\n",
            "-- Downloading BODY_25 model...\n",
            "-- NOTE: This process might take several minutes depending on your internet connection.\n",
            "-- Not downloading body (COCO) model\n",
            "-- Not downloading body (MPI) model\n",
            "-- Downloading face model...\n",
            "-- NOTE: This process might take several minutes depending on your internet connection.\n",
            "-- Downloading hand model...\n",
            "-- NOTE: This process might take several minutes depending on your internet connection.\n",
            "-- Models Downloaded.\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/openpose/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_lib\u001b[0m\n",
            "[ 12%] \u001b[34m\u001b[1mCreating directories for 'openpose_lib'\u001b[0m\n",
            "[ 25%] \u001b[34m\u001b[1mNo download step for 'openpose_lib'\u001b[0m\n",
            "[ 37%] \u001b[34m\u001b[1mNo update step for 'openpose_lib'\u001b[0m\n",
            "[ 50%] \u001b[34m\u001b[1mNo patch step for 'openpose_lib'\u001b[0m\n",
            "[ 62%] \u001b[34m\u001b[1mPerforming configure step for 'openpose_lib'\u001b[0m\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Boost 1.54 found.\n",
            "-- Found Boost components:\n",
            "   system;thread;filesystem\n",
            "-- Found Threads: TRUE  \n",
            "-- Found GFlags: /usr/include  \n",
            "-- Found gflags  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libgflags.so)\n",
            "-- Found Glog: /usr/include  \n",
            "-- Found glog    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)\n",
            "-- Found Protobuf: /usr/lib/x86_64-linux-gnu/libprotobuf.so;-lpthread (found version \"3.6.1\") \n",
            "-- Found PROTOBUF Compiler: /usr/bin/protoc\n",
            "-- HDF5: Using hdf5 compiler wrapper to determine C configuration\n",
            "-- HDF5: Using hdf5 compiler wrapper to determine CXX configuration\n",
            "-- Found HDF5: /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_cpp.so;/usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5.so;/usr/lib/x86_64-linux-gnu/libpthread.so;/usr/lib/x86_64-linux-gnu/libsz.so;/usr/lib/x86_64-linux-gnu/libz.so;/usr/lib/x86_64-linux-gnu/libdl.so;/usr/lib/x86_64-linux-gnu/libm.so (found version \"1.10.4\") found components:  HL \n",
            "-- CUDA detected: 11.8\n",
            "-- Added CUDA NVCC flags for: sm_75\n",
            "-- Found Atlas: /usr/include/x86_64-linux-gnu  \n",
            "-- Found Atlas (include: /usr/include/x86_64-linux-gnu library: /usr/lib/x86_64-linux-gnu/libatlas.so lapack: /usr/lib/x86_64-linux-gnu/liblapack.so\n",
            "-- Python interface is disabled or not all required dependencies found. Building without it...\n",
            "-- Found Git: /usr/bin/git (found version \"2.25.1\") \n",
            "-- \n",
            "-- ******************* Caffe Configuration Summary *******************\n",
            "-- General:\n",
            "--   Version           :   1.0.0\n",
            "--   Git               :   1.0-149-g1807aada\n",
            "--   System            :   Linux\n",
            "--   C++ compiler      :   /usr/bin/c++\n",
            "--   Release CXX flags :   -O3 -DNDEBUG -fPIC -Wall -std=c++11 -Wno-sign-compare -Wno-uninitialized\n",
            "--   Debug CXX flags   :   -g -fPIC -Wall -std=c++11 -Wno-sign-compare -Wno-uninitialized\n",
            "--   Build type        :   Release\n",
            "-- \n",
            "--   BUILD_SHARED_LIBS :   ON\n",
            "--   BUILD_python      :   OFF\n",
            "--   BUILD_matlab      :   OFF\n",
            "--   BUILD_docs        :   OFF\n",
            "--   CPU_ONLY          :   OFF\n",
            "--   USE_OPENCV        :   OFF\n",
            "--   USE_LEVELDB       :   OFF\n",
            "--   USE_LMDB          :   OFF\n",
            "--   USE_NCCL          :   OFF\n",
            "--   ALLOW_LMDB_NOLOCK :   OFF\n",
            "--   USE_HDF5          :   ON\n",
            "-- \n",
            "-- Dependencies:\n",
            "--   BLAS              :   Yes (Atlas)\n",
            "--   Boost             :   Yes (ver. 1.71)\n",
            "--   glog              :   Yes\n",
            "--   gflags            :   Yes\n",
            "--   protobuf          :   Yes (ver. 3.6.1)\n",
            "--   CUDA              :   Yes (ver. 11.8)\n",
            "-- \n",
            "-- NVIDIA CUDA:\n",
            "--   Target GPU(s)     :   Auto\n",
            "--   GPU arch(s)       :   sm_75\n",
            "--   cuDNN             :   Disabled\n",
            "-- \n",
            "-- Install:\n",
            "--   Install path      :   /content/openpose/build/caffe\n",
            "-- \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "CMake Warning:\n",
            "  Manually-specified variables were not used by the project:\n",
            "\n",
            "    CUDA_ARCH_BIN\n",
            "\n",
            "\n",
            "-- Build files have been written to: /content/openpose/build/caffe/src/openpose_lib-build\n",
            "[ 75%] \u001b[34m\u001b[1mPerforming build step for 'openpose_lib'\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mRunning C++/Python protocol buffer compiler on /content/openpose/3rdparty/caffe/src/caffe/proto/caffe.proto\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target caffeproto\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffeproto.dir/__/__/include/caffe/proto/caffe.pb.cc.o\u001b[0m\n",
            "[  1%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libcaffeproto.a\u001b[0m\n",
            "[  1%] Built target caffeproto\n",
            "[  1%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/util/cuda_compile_1_generated_math_functions.cu.o\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_absval_layer.cu.o\u001b[0m\n",
            "In file included from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:1:\n",
            "/usr/local/cuda/include/math_functions.h:54:2: warning: #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\n",
            "   54 | #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "      |  ^~~~~~~\n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:25,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:2:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:131:13: warning: Thrust requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  131 |      THRUST_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                           \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36,\n",
            "                 from /usr/local/cuda/include/cub/detail/device_synchronize.cuh:20,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:36,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:35,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:277,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:104,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:721,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:57,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.h:45,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.h:234,\n",
            "                 from /usr/local/cuda/include/thrust/detail/vector_base.h:30,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:26,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:2:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:142:13: warning: CUB requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  142 |      CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                        \n",
            "In file included from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:1:\n",
            "/usr/local/cuda/include/math_functions.h:54:2: warning: #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\n",
            "   54 | #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "      |  ^~~~~~~\n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:25,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:2:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:131:13: warning: Thrust requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  131 |      THRUST_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                           \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36,\n",
            "                 from /usr/local/cuda/include/cub/detail/device_synchronize.cuh:20,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:36,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:35,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:277,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:104,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:721,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:57,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.h:45,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.h:234,\n",
            "                 from /usr/local/cuda/include/thrust/detail/vector_base.h:30,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:26,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:2:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:142:13: warning: CUB requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  142 |      CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                        \n",
            "In file included from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:1:\n",
            "/usr/local/cuda/include/math_functions.h:54:2: warning: #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\n",
            "   54 | #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "      |  ^~~~~~~\n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:25,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:2:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:131:13: warning: Thrust requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  131 |      THRUST_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                           \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36,\n",
            "                 from /usr/local/cuda/include/cub/detail/device_synchronize.cuh:20,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:36,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:35,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:277,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:104,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:721,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:57,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.h:45,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.h:234,\n",
            "                 from /usr/local/cuda/include/thrust/detail/vector_base.h:30,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:26,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:2:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:142:13: warning: CUB requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  142 |      CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                        \n",
            "In file included from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:1:\n",
            "/usr/local/cuda/include/math_functions.h:54:2: warning: #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\n",
            "   54 | #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "      |  ^~~~~~~\n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:25,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:2:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:131:13: warning: Thrust requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  131 |      THRUST_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                           \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36,\n",
            "                 from /usr/local/cuda/include/cub/detail/device_synchronize.cuh:20,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:36,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:35,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:277,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:104,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:721,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:57,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.h:45,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.h:234,\n",
            "                 from /usr/local/cuda/include/thrust/detail/vector_base.h:30,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:26,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:2:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:142:13: warning: CUB requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  142 |      CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                        \n",
            "[  1%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_accuracy_layer.cu.o\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_base_data_layer.cu.o\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_batch_norm_layer.cu.o\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_batch_reindex_layer.cu.o\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_bias_layer.cu.o\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_bnll_layer.cu.o\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_clip_layer.cu.o\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_concat_layer.cu.o\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_contrastive_loss_layer.cu.o\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_conv_layer.cu.o\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_crop_layer.cu.o\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_cudnn_conv_layer.cu.o\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_cudnn_deconv_layer.cu.o\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_cudnn_lcn_layer.cu.o\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_cudnn_lrn_layer.cu.o\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_cudnn_pooling_layer.cu.o\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_cudnn_relu_layer.cu.o\u001b[0m\n",
            "[ 10%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_cudnn_sigmoid_layer.cu.o\u001b[0m\n",
            "[ 10%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_cudnn_softmax_layer.cu.o\u001b[0m\n",
            "[ 10%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_cudnn_tanh_layer.cu.o\u001b[0m\n",
            "[ 12%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_deconv_layer.cu.o\u001b[0m\n",
            "[ 12%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_dropout_layer.cu.o\u001b[0m\n",
            "[ 13%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_eltwise_layer.cu.o\u001b[0m\n",
            "[ 13%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_elu_layer.cu.o\u001b[0m\n",
            "[ 13%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_embed_layer.cu.o\u001b[0m\n",
            "[ 15%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_euclidean_loss_layer.cu.o\u001b[0m\n",
            "[ 15%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_exp_layer.cu.o\u001b[0m\n",
            "[ 15%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_filter_layer.cu.o\u001b[0m\n",
            "[ 16%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_hdf5_data_layer.cu.o\u001b[0m\n",
            "[ 16%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_hdf5_output_layer.cu.o\u001b[0m\n",
            "[ 17%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_im2col_layer.cu.o\u001b[0m\n",
            "[ 17%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_inner_product_layer.cu.o\u001b[0m\n",
            "[ 17%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_log_layer.cu.o\u001b[0m\n",
            "[ 19%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_lrn_layer.cu.o\u001b[0m\n",
            "[ 19%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_lstm_unit_layer.cu.o\u001b[0m\n",
            "[ 19%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_mvn_layer.cu.o\u001b[0m\n",
            "[ 20%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_pooling_layer.cu.o\u001b[0m\n",
            "[ 20%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_power_layer.cu.o\u001b[0m\n",
            "[ 21%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_prelu_layer.cu.o\u001b[0m\n",
            "[ 21%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_recurrent_layer.cu.o\u001b[0m\n",
            "[ 21%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_reduction_layer.cu.o\u001b[0m\n",
            "[ 23%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_relu_layer.cu.o\u001b[0m\n",
            "[ 23%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_scale_layer.cu.o\u001b[0m\n",
            "[ 23%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_sigmoid_cross_entropy_loss_layer.cu.o\u001b[0m\n",
            "[ 24%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_sigmoid_layer.cu.o\u001b[0m\n",
            "[ 24%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_silence_layer.cu.o\u001b[0m\n",
            "[ 26%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_slice_layer.cu.o\u001b[0m\n",
            "[ 26%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_softmax_layer.cu.o\u001b[0m\n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:25,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/layers/softmax_layer.cu:5:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:131:13: warning: Thrust requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  131 |      THRUST_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                           \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36,\n",
            "                 from /usr/local/cuda/include/cub/detail/device_synchronize.cuh:20,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:36,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:35,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:277,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:104,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:721,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:57,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.h:45,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.h:234,\n",
            "                 from /usr/local/cuda/include/thrust/detail/vector_base.h:30,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:26,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/layers/softmax_layer.cu:5:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:142:13: warning: CUB requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  142 |      CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                        \n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:25,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/layers/softmax_layer.cu:5:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:131:13: warning: Thrust requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  131 |      THRUST_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                           \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36,\n",
            "                 from /usr/local/cuda/include/cub/detail/device_synchronize.cuh:20,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:36,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:35,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:277,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:104,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:721,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:57,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.h:45,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.h:234,\n",
            "                 from /usr/local/cuda/include/thrust/detail/vector_base.h:30,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:26,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/layers/softmax_layer.cu:5:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:142:13: warning: CUB requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  142 |      CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                        \n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:25,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/layers/softmax_layer.cu:5:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:131:13: warning: Thrust requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  131 |      THRUST_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                           \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36,\n",
            "                 from /usr/local/cuda/include/cub/detail/device_synchronize.cuh:20,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:36,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:35,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:277,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:104,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:721,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:57,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.h:45,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.h:234,\n",
            "                 from /usr/local/cuda/include/thrust/detail/vector_base.h:30,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:26,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/layers/softmax_layer.cu:5:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:142:13: warning: CUB requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  142 |      CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                        \n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:25,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/layers/softmax_layer.cu:5:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:131:13: warning: Thrust requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  131 |      THRUST_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                           \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36,\n",
            "                 from /usr/local/cuda/include/cub/detail/device_synchronize.cuh:20,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:36,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:35,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:277,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:104,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:721,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:57,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/allocator/copy_construct_range.h:45,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/detail/contiguous_storage.h:234,\n",
            "                 from /usr/local/cuda/include/thrust/detail/vector_base.h:30,\n",
            "                 from /usr/local/cuda/include/thrust/device_vector.h:26,\n",
            "                 from /content/openpose/3rdparty/caffe/src/caffe/layers/softmax_layer.cu:5:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:142:13: warning: CUB requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "  142 |      CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n",
            "      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                        \n",
            "[ 26%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_softmax_loss_layer.cu.o\u001b[0m\n",
            "[ 27%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_split_layer.cu.o\u001b[0m\n",
            "[ 27%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_swish_layer.cu.o\u001b[0m\n",
            "[ 28%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_tanh_layer.cu.o\u001b[0m\n",
            "[ 28%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_threshold_layer.cu.o\u001b[0m\n",
            "[ 28%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_tile_layer.cu.o\u001b[0m\n",
            "[ 30%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/solvers/cuda_compile_1_generated_adadelta_solver.cu.o\u001b[0m\n",
            "[ 30%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/solvers/cuda_compile_1_generated_adagrad_solver.cu.o\u001b[0m\n",
            "[ 30%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/solvers/cuda_compile_1_generated_adam_solver.cu.o\u001b[0m\n",
            "[ 31%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/solvers/cuda_compile_1_generated_nesterov_solver.cu.o\u001b[0m\n",
            "[ 31%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/solvers/cuda_compile_1_generated_rmsprop_solver.cu.o\u001b[0m\n",
            "[ 32%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/solvers/cuda_compile_1_generated_sgd_solver.cu.o\u001b[0m\n",
            "[ 32%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/util/cuda_compile_1_generated_im2col.cu.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target caffe\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/blob.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/common.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/data_transformer.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/internal_thread.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layer.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layer_factory.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/absval_layer.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/accuracy_layer.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/argmax_layer.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/base_conv_layer.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/base_data_layer.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/batch_norm_layer.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/batch_reindex_layer.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/bias_layer.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/bnll_layer.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/clip_layer.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/concat_layer.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/contrastive_loss_layer.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/conv_layer.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/crop_layer.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_conv_layer.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_deconv_layer.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_lcn_layer.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_lrn_layer.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_pooling_layer.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_relu_layer.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_sigmoid_layer.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_softmax_layer.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_tanh_layer.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/data_layer.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/deconv_layer.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/dropout_layer.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/dummy_data_layer.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/eltwise_layer.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/elu_layer.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/embed_layer.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/euclidean_loss_layer.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/exp_layer.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/filter_layer.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/flatten_layer.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/hdf5_data_layer.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/hdf5_output_layer.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/hinge_loss_layer.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/im2col_layer.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/image_data_layer.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/infogain_loss_layer.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/inner_product_layer.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/input_layer.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/log_layer.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/loss_layer.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/lrn_layer.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/lstm_layer.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/lstm_unit_layer.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/memory_data_layer.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/multinomial_logistic_loss_layer.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/mvn_layer.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/neuron_layer.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/parameter_layer.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/pooling_layer.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/power_layer.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/prelu_layer.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/recurrent_layer.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/reduction_layer.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/relu_layer.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/reshape_layer.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/rnn_layer.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/scale_layer.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/sigmoid_cross_entropy_loss_layer.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/sigmoid_layer.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/silence_layer.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/slice_layer.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/softmax_layer.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/softmax_loss_layer.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/split_layer.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/spp_layer.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/swish_layer.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/tanh_layer.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/threshold_layer.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/tile_layer.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/layers/window_data_layer.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/net.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/parallel.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/solver.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/solvers/adadelta_solver.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/solvers/adagrad_solver.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/solvers/adam_solver.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/solvers/nesterov_solver.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/solvers/rmsprop_solver.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/solvers/sgd_solver.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/syncedmem.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/benchmark.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/blocking_queue.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/cudnn.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/db.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/db_leveldb.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/db_lmdb.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/hdf5.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/im2col.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/insert_splits.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/io.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/math_functions.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/signal_handler.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffe.dir/util/upgrade_proto.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX shared library ../../lib/libcaffe.so\u001b[0m\n",
            "[ 87%] Built target caffe\n",
            "\u001b[35m\u001b[1mScanning dependencies of target compute_image_mean\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target upgrade_solver_proto_text\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/CMakeFiles/compute_image_mean.dir/compute_image_mean.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/CMakeFiles/upgrade_solver_proto_text.dir/upgrade_solver_proto_text.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable compute_image_mean\u001b[0m\n",
            "[ 89%] Built target compute_image_mean\n",
            "\u001b[35m\u001b[1mScanning dependencies of target caffe.bin\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/CMakeFiles/caffe.bin.dir/caffe.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable upgrade_solver_proto_text\u001b[0m\n",
            "[ 90%] Built target upgrade_solver_proto_text\n",
            "\u001b[35m\u001b[1mScanning dependencies of target upgrade_net_proto_binary\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/CMakeFiles/upgrade_net_proto_binary.dir/upgrade_net_proto_binary.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable upgrade_net_proto_binary\u001b[0m\n",
            "[ 91%] Built target upgrade_net_proto_binary\n",
            "\u001b[35m\u001b[1mScanning dependencies of target convert_imageset\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/CMakeFiles/convert_imageset.dir/convert_imageset.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable caffe\u001b[0m\n",
            "[ 91%] Built target caffe.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target extract_features\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/CMakeFiles/extract_features.dir/extract_features.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable convert_imageset\u001b[0m\n",
            "[ 91%] Built target convert_imageset\n",
            "\u001b[35m\u001b[1mScanning dependencies of target upgrade_net_proto_text\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/CMakeFiles/upgrade_net_proto_text.dir/upgrade_net_proto_text.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable extract_features\u001b[0m\n",
            "[ 93%] Built target extract_features\n",
            "\u001b[35m\u001b[1mScanning dependencies of target classification\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object examples/CMakeFiles/classification.dir/cpp_classification/classification.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable upgrade_net_proto_text\u001b[0m\n",
            "[ 95%] Built target upgrade_net_proto_text\n",
            "\u001b[35m\u001b[1mScanning dependencies of target convert_mnist_data\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object examples/CMakeFiles/convert_mnist_data.dir/mnist/convert_mnist_data.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable mnist/convert_mnist_data\u001b[0m\n",
            "[ 97%] Built target convert_mnist_data\n",
            "\u001b[35m\u001b[1mScanning dependencies of target convert_cifar_data\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object examples/CMakeFiles/convert_cifar_data.dir/cifar10/convert_cifar_data.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable cpp_classification/classification\u001b[0m\n",
            "[ 97%] Built target classification\n",
            "\u001b[35m\u001b[1mScanning dependencies of target convert_mnist_siamese_data\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object examples/CMakeFiles/convert_mnist_siamese_data.dir/siamese/convert_mnist_siamese_data.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable cifar10/convert_cifar_data\u001b[0m\n",
            "[100%] Built target convert_cifar_data\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable siamese/convert_mnist_siamese_data\u001b[0m\n",
            "[100%] Built target convert_mnist_siamese_data\n",
            "[ 87%] \u001b[34m\u001b[1mPerforming install step for 'openpose_lib'\u001b[0m\n",
            "-- Boost 1.54 found.\n",
            "-- Found Boost components:\n",
            "   system;thread;filesystem\n",
            "-- Found gflags  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libgflags.so)\n",
            "-- Found glog    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)\n",
            "-- Found PROTOBUF Compiler: /usr/bin/protoc\n",
            "-- HDF5: Using hdf5 compiler wrapper to determine C configuration\n",
            "-- HDF5: Using hdf5 compiler wrapper to determine CXX configuration\n",
            "-- CUDA detected: 11.8\n",
            "-- Added CUDA NVCC flags for: sm_75\n",
            "-- Found Atlas (include: /usr/include/x86_64-linux-gnu library: /usr/lib/x86_64-linux-gnu/libatlas.so lapack: /usr/lib/x86_64-linux-gnu/liblapack.so\n",
            "-- Python interface is disabled or not all required dependencies found. Building without it...\n",
            "-- \n",
            "-- ******************* Caffe Configuration Summary *******************\n",
            "-- General:\n",
            "--   Version           :   1.0.0\n",
            "--   Git               :   1.0-149-g1807aada\n",
            "--   System            :   Linux\n",
            "--   C++ compiler      :   /usr/bin/c++\n",
            "--   Release CXX flags :   -O3 -DNDEBUG -fPIC -Wall -std=c++11 -Wno-sign-compare -Wno-uninitialized\n",
            "--   Debug CXX flags   :   -g -fPIC -Wall -std=c++11 -Wno-sign-compare -Wno-uninitialized\n",
            "--   Build type        :   Release\n",
            "-- \n",
            "--   BUILD_SHARED_LIBS :   ON\n",
            "--   BUILD_python      :   OFF\n",
            "--   BUILD_matlab      :   OFF\n",
            "--   BUILD_docs        :   OFF\n",
            "--   CPU_ONLY          :   OFF\n",
            "--   USE_OPENCV        :   OFF\n",
            "--   USE_LEVELDB       :   OFF\n",
            "--   USE_LMDB          :   OFF\n",
            "--   USE_NCCL          :   OFF\n",
            "--   ALLOW_LMDB_NOLOCK :   OFF\n",
            "--   USE_HDF5          :   ON\n",
            "-- \n",
            "-- Dependencies:\n",
            "--   BLAS              :   Yes (Atlas)\n",
            "--   Boost             :   Yes (ver. 1.71)\n",
            "--   glog              :   Yes\n",
            "--   gflags            :   Yes\n",
            "--   protobuf          :   Yes (ver. 3.6.1)\n",
            "--   CUDA              :   Yes (ver. 11.8)\n",
            "-- \n",
            "-- NVIDIA CUDA:\n",
            "--   Target GPU(s)     :   Auto\n",
            "--   GPU arch(s)       :   sm_75\n",
            "--   cuDNN             :   Disabled\n",
            "-- \n",
            "-- Install:\n",
            "--   Install path      :   /content/openpose/build/caffe\n",
            "-- \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/openpose/build/caffe/src/openpose_lib-build\n",
            "[  2%] Built target caffeproto\n",
            "\u001b[35m\u001b[1mScanning dependencies of target caffe\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../lib/libcaffe.so\u001b[0m\n",
            "[ 89%] Built target caffe\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable compute_image_mean\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable upgrade_solver_proto_text\u001b[0m\n",
            "[ 91%] Built target compute_image_mean\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable caffe\u001b[0m\n",
            "[ 91%] Built target upgrade_solver_proto_text\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable upgrade_net_proto_binary\u001b[0m\n",
            "[ 91%] Built target caffe.bin\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable convert_imageset\u001b[0m\n",
            "[ 94%] Built target upgrade_net_proto_binary\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable extract_features\u001b[0m\n",
            "[ 95%] Built target convert_imageset\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable upgrade_net_proto_text\u001b[0m\n",
            "[ 97%] Built target extract_features\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable cpp_classification/classification\u001b[0m\n",
            "[ 97%] Built target upgrade_net_proto_text\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable mnist/convert_mnist_data\u001b[0m\n",
            "[ 97%] Built target classification\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable cifar10/convert_cifar_data\u001b[0m\n",
            "[ 98%] Built target convert_mnist_data\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable siamese/convert_mnist_siamese_data\u001b[0m\n",
            "[100%] Built target convert_mnist_siamese_data\n",
            "[100%] Built target convert_cifar_data\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /content/openpose/build/caffe/share/Caffe/CaffeConfig.cmake\n",
            "-- Installing: /content/openpose/build/caffe/share/Caffe/CaffeTargets.cmake\n",
            "-- Installing: /content/openpose/build/caffe/share/Caffe/CaffeTargets-release.cmake\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/solver_factory.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/solver.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/net.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/common.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/syncedmem.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layer_factory.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/caffe.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/data_transformer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/internal_thread.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/gpu_util.cuh\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/format.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/nccl.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/db_leveldb.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/db.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/im2col.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/upgrade_proto.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/rng.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/mkl_alternate.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/db_lmdb.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/hdf5.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/signal_handler.h\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/benchmark.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/insert_splits.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/io.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/blocking_queue.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/math_functions.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/cudnn.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/util/device_alternate.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/dummy_data_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/recurrent_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/cudnn_deconv_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/lstm_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/deconv_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/bias_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/elu_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/parameter_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/batch_norm_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/concat_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/mvn_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/multinomial_logistic_loss_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/power_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/cudnn_lrn_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/hdf5_data_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/pooling_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/accuracy_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/embed_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/softmax_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/inner_product_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/reshape_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/image_data_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/lrn_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/flatten_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/prelu_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/softmax_loss_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/neuron_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/batch_reindex_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/filter_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/contrastive_loss_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/cudnn_lcn_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/infogain_loss_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/cudnn_tanh_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/clip_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/python_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/silence_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/reduction_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/exp_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/eltwise_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/conv_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/slice_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/hdf5_output_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/input_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/threshold_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/sigmoid_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/spp_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/tanh_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/absval_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/sigmoid_cross_entropy_loss_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/base_conv_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/split_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/relu_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/window_data_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/log_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/bnll_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/scale_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/memory_data_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/rnn_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/dropout_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/base_data_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/loss_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/hinge_loss_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/im2col_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/argmax_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/cudnn_relu_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/cudnn_sigmoid_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/cudnn_pooling_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/cudnn_conv_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/data_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/tile_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/swish_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/euclidean_loss_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/crop_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/layers/cudnn_softmax_layer.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/filler.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/sgd_solvers.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/test\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/test/test_caffe_main.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/test/test_gradient_check_util.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/parallel.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/blob.hpp\n",
            "-- Installing: /content/openpose/build/caffe/include/caffe/proto/caffe.pb.h\n",
            "-- Installing: /content/openpose/build/caffe/lib/libcaffe.so.1.0.0\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/lib/libcaffe.so.1.0.0\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/lib/libcaffe.so\n",
            "-- Installing: /content/openpose/build/caffe/lib/libcaffeproto.a\n",
            "-- Installing: /content/openpose/build/caffe/python/caffe/proto/caffe_pb2.py\n",
            "-- Installing: /content/openpose/build/caffe/python/caffe/proto/__init__.py\n",
            "-- Installing: /content/openpose/build/caffe/bin/caffe\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/caffe\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/bin/compute_image_mean\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/compute_image_mean\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/bin/convert_imageset\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/convert_imageset\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/bin/extract_features\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/extract_features\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/bin/upgrade_net_proto_binary\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/upgrade_net_proto_binary\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/bin/upgrade_net_proto_text\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/upgrade_net_proto_text\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/bin/upgrade_solver_proto_text\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/upgrade_solver_proto_text\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/bin/convert_cifar_data\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/convert_cifar_data\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/bin/classification\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/classification\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/bin/convert_mnist_data\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/convert_mnist_data\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "-- Installing: /content/openpose/build/caffe/bin/convert_mnist_siamese_data\n",
            "-- Set runtime path of \"/content/openpose/build/caffe/bin/convert_mnist_siamese_data\" to \"/content/openpose/build/caffe/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64\"\n",
            "[100%] \u001b[34m\u001b[1mCompleted 'openpose_lib'\u001b[0m\n",
            "\u001b[34m\u001b[1mRerunning cmake after building Caffe submodule\u001b[0m\n",
            "-- GCC detected, adding compile flags\n",
            "-- GCC detected, adding compile flags\n",
            "-- Building with CUDA.\n",
            "-- CUDA detected: 11.8\n",
            "-- Added CUDA NVCC flags for: sm_75\n",
            "-- Found cuDNN: ver. 8.7.0 found (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
            "-- Found gflags  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libgflags.so)\n",
            "-- Found glog    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)\n",
            "-- Caffe will be downloaded from source now. NOTE: This process might take several minutes depending\n",
            "        on your internet connection.\n",
            "-- Caffe has already been downloaded.\n",
            "HEAD is now at 1807aada Added Ampere arch's (CUDA11)\n",
            "-- Caffe will be built from source now.\n",
            "-- Adding Example calibration.bin\n",
            "-- Adding Example tutorial_add_module_custom_post_processing.bin\n",
            "-- Adding Example tutorial_api_thread_1_user_processing_function.bin\n",
            "-- Adding Example tutorial_api_thread_2_user_input_processing_output_and_datum.bin\n",
            "-- Adding Example openpose.bin\n",
            "-- Adding Example 01_body_from_image_default.bin\n",
            "-- Adding Example 02_whole_body_from_image_default.bin\n",
            "-- Adding Example 03_keypoints_from_image.bin\n",
            "-- Adding Example 04_keypoints_from_images.bin\n",
            "-- Adding Example 05_keypoints_from_images_multi_gpu.bin\n",
            "-- Adding Example 06_face_from_image.bin\n",
            "-- Adding Example 07_hand_from_image.bin\n",
            "-- Adding Example 08_heatmaps_from_image.bin\n",
            "-- Adding Example 09_keypoints_from_heatmaps.bin\n",
            "-- Adding Example 10_asynchronous_custom_input.bin\n",
            "-- Adding Example 11_asynchronous_custom_input_multi_camera.bin\n",
            "-- Adding Example 12_asynchronous_custom_output.bin\n",
            "-- Adding Example 13_asynchronous_custom_input_output_and_datum.bin\n",
            "-- Adding Example 14_synchronous_custom_input.bin\n",
            "-- Adding Example 15_synchronous_custom_preprocessing.bin\n",
            "-- Adding Example 16_synchronous_custom_postprocessing.bin\n",
            "-- Adding Example 17_synchronous_custom_output.bin\n",
            "-- Adding Example 18_synchronous_custom_all_and_datum.bin\n",
            "-- Adding Example handFromJsonTest.bin\n",
            "-- Adding Example resizeTest.bin\n",
            "-- Download the models.\n",
            "-- Downloading BODY_25 model...\n",
            "-- Model already exists.\n",
            "-- Not downloading body (COCO) model\n",
            "-- Not downloading body (MPI) model\n",
            "-- Downloading face model...\n",
            "-- Model already exists.\n",
            "-- Downloading hand model...\n",
            "-- Model already exists.\n",
            "-- Models Downloaded.\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/openpose/build\n",
            "[  0%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/CMakeFiles/openpose.dir/tracking/openpose_generated_pyramidalLK.cu.o\u001b[0m\n",
            "[  2%] Built target openpose_lib\n",
            "[  2%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/CMakeFiles/openpose.dir/face/openpose_generated_renderFace.cu.o\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/CMakeFiles/openpose.dir/gpu/openpose_generated_cuda.cu.o\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/CMakeFiles/openpose.dir/hand/openpose_generated_renderHand.cu.o\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/CMakeFiles/openpose.dir/net/openpose_generated_bodyPartConnectorBase.cu.o\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/CMakeFiles/openpose.dir/net/openpose_generated_maximumBase.cu.o\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/CMakeFiles/openpose.dir/net/openpose_generated_nmsBase.cu.o\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/CMakeFiles/openpose.dir/net/openpose_generated_resizeAndMergeBase.cu.o\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/CMakeFiles/openpose.dir/pose/openpose_generated_renderPose.cu.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/3d/defineTemplates.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/3d/cameraParameterReader.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/3d/jointAngleEstimation.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/3d/poseTriangulation.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/3d/poseTriangulationPrivate.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/calibration/cameraParameterEstimation.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/calibration/gridPatternFunctions.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/array.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/arrayCpuGpu.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/cvMatToOpInput.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/cvMatToOpOutput.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/datum.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/defineTemplates.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/gpuRenderer.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/keepTopNPeople.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/keypointScaler.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/matrix.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/opOutputToCvMat.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/point.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/rectangle.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/renderer.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/scaleAndSizeExtractor.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/string.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/core/verbosePrinter.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/face/defineTemplates.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/face/faceDetector.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/face/faceDetectorOpenCV.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/face/faceExtractorCaffe.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/face/faceExtractorNet.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/face/faceCpuRenderer.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/face/faceGpuRenderer.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/face/faceRenderer.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/face/renderFace.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/bvhSaver.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/cocoJsonSaver.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/defineTemplates.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/fileSaver.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/fileStream.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/heatMapSaver.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/imageSaver.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/jsonOfstream.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/keypointSaver.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/peopleJsonSaver.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/udpSender.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/filestream/videoSaver.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/gpu/cuda.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/gpu/gpu.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/gpu/opencl.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/gui/defineTemplates.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/gui/frameDisplayer.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/gui/gui.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/gui/guiAdam.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/gui/gui3D.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/gui/guiInfoAdder.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/hand/defineTemplates.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/hand/handDetector.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/hand/handDetectorFromTxt.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/hand/handExtractorCaffe.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/hand/handExtractorNet.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/hand/handCpuRenderer.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/hand/handGpuRenderer.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/hand/handRenderer.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/hand/renderHand.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/bodyPartConnectorBase.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/bodyPartConnectorBaseCL.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/bodyPartConnectorCaffe.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/maximumBase.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/maximumCaffe.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/netCaffe.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/netOpenCv.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/nmsBase.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/nmsBaseCL.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/nmsCaffe.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/resizeAndMergeBase.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/resizeAndMergeBaseCL.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/net/resizeAndMergeCaffe.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/pose/defineTemplates.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/pose/poseCpuRenderer.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/pose/poseExtractor.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/pose/poseExtractorCaffe.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/pose/poseExtractorNet.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/pose/poseGpuRenderer.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/pose/poseParameters.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/pose/poseParametersRender.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/pose/poseRenderer.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/pose/renderPose.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/producer/datumProducer.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/producer/defineTemplates.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/producer/flirReader.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/producer/imageDirectoryReader.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/producer/ipCameraReader.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/producer/producer.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/producer/spinnakerWrapper.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/producer/videoCaptureReader.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/producer/videoReader.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/producer/webcamReader.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/thread/defineTemplates.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/tracking/defineTemplates.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/tracking/personIdExtractor.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/tracking/personTracker.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/tracking/pyramidalLK.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/unity/unityBinding.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/utilities/errorAndLog.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/utilities/fileSystem.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/utilities/flagsToOpenPose.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/utilities/keypoint.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/utilities/openCv.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/utilities/openCvPrivate.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/utilities/profiler.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/utilities/string.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/wrapper/defineTemplates.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/wrapper/wrapperAuxiliary.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/wrapper/wrapperStructExtra.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/wrapper/wrapperStructFace.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/wrapper/wrapperStructGui.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/wrapper/wrapperStructHand.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/wrapper/wrapperStructInput.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/wrapper/wrapperStructOutput.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/openpose/CMakeFiles/openpose.dir/wrapper/wrapperStructPose.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose.so\u001b[0m\n",
            "[ 41%] Built target openpose\n",
            "\u001b[35m\u001b[1mScanning dependencies of target calibration.bin\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_core\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object examples/calibration/CMakeFiles/calibration.bin.dir/calibration.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/array.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable calibration.bin\u001b[0m\n",
            "[ 42%] Built target calibration.bin\n",
            "[ 43%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/arrayCpuGpu.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/cvMatToOpInput.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/cvMatToOpOutput.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/datum.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/defineTemplates.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/gpuRenderer.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/keepTopNPeople.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/keypointScaler.cpp.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target tutorial_api_thread_1_user_processing_function.bin\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object examples/deprecated/CMakeFiles/tutorial_api_thread_1_user_processing_function.bin.dir/tutorial_api_thread_1_user_processing_function.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/matrix.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/opOutputToCvMat.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/point.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable tutorial_api_thread_1_user_processing_function.bin\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/rectangle.cpp.o\u001b[0m\n",
            "[ 47%] Built target tutorial_api_thread_1_user_processing_function.bin\n",
            "[ 47%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/renderer.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/scaleAndSizeExtractor.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/string.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/openpose/core/CMakeFiles/openpose_core.dir/verbosePrinter.cpp.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target tutorial_add_module_custom_post_processing.bin\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object examples/deprecated/CMakeFiles/tutorial_add_module_custom_post_processing.bin.dir/tutorial_add_module_custom_post_processing.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_core.so\u001b[0m\n",
            "[ 49%] Built target openpose_core\n",
            "\u001b[35m\u001b[1mScanning dependencies of target tutorial_api_thread_2_user_input_processing_output_and_datum.bin\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object examples/deprecated/CMakeFiles/tutorial_api_thread_2_user_input_processing_output_and_datum.bin.dir/tutorial_api_thread_2_user_input_processing_output_and_datum.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable tutorial_api_thread_2_user_input_processing_output_and_datum.bin\u001b[0m\n",
            "[ 50%] Built target tutorial_api_thread_2_user_input_processing_output_and_datum.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose.bin\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object examples/openpose/CMakeFiles/openpose.bin.dir/openpose.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable openpose.bin\u001b[0m\n",
            "[ 50%] Built target openpose.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 18_synchronous_custom_all_and_datum.bin\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/18_synchronous_custom_all_and_datum.bin.dir/18_synchronous_custom_all_and_datum.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable tutorial_add_module_custom_post_processing.bin\u001b[0m\n",
            "[ 50%] Built target tutorial_add_module_custom_post_processing.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 17_synchronous_custom_output.bin\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/17_synchronous_custom_output.bin.dir/17_synchronous_custom_output.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable 17_synchronous_custom_output.bin\u001b[0m\n",
            "[ 51%] Built target 17_synchronous_custom_output.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 05_keypoints_from_images_multi_gpu.bin\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/05_keypoints_from_images_multi_gpu.bin.dir/05_keypoints_from_images_multi_gpu.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable 18_synchronous_custom_all_and_datum.bin\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable 05_keypoints_from_images_multi_gpu.bin\u001b[0m\n",
            "[ 52%] Built target 18_synchronous_custom_all_and_datum.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 03_keypoints_from_image.bin\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/03_keypoints_from_image.bin.dir/03_keypoints_from_image.cpp.o\u001b[0m\n",
            "[ 52%] Built target 05_keypoints_from_images_multi_gpu.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 06_face_from_image.bin\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/06_face_from_image.bin.dir/06_face_from_image.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable 03_keypoints_from_image.bin\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable 06_face_from_image.bin\u001b[0m\n",
            "[ 52%] Built target 03_keypoints_from_image.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 07_hand_from_image.bin\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/07_hand_from_image.bin.dir/07_hand_from_image.cpp.o\u001b[0m\n",
            "[ 52%] Built target 06_face_from_image.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 04_keypoints_from_images.bin\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/04_keypoints_from_images.bin.dir/04_keypoints_from_images.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable 07_hand_from_image.bin\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable 04_keypoints_from_images.bin\u001b[0m\n",
            "[ 54%] Built target 07_hand_from_image.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 01_body_from_image_default.bin\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/01_body_from_image_default.bin.dir/01_body_from_image_default.cpp.o\u001b[0m\n",
            "[ 54%] Built target 04_keypoints_from_images.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 09_keypoints_from_heatmaps.bin\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/09_keypoints_from_heatmaps.bin.dir/09_keypoints_from_heatmaps.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable 01_body_from_image_default.bin\u001b[0m\n",
            "[ 55%] Built target 01_body_from_image_default.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 02_whole_body_from_image_default.bin\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/02_whole_body_from_image_default.bin.dir/02_whole_body_from_image_default.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable 09_keypoints_from_heatmaps.bin\u001b[0m\n",
            "[ 55%] Built target 09_keypoints_from_heatmaps.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 13_asynchronous_custom_input_output_and_datum.bin\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/13_asynchronous_custom_input_output_and_datum.bin.dir/13_asynchronous_custom_input_output_and_datum.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable 02_whole_body_from_image_default.bin\u001b[0m\n",
            "[ 56%] Built target 02_whole_body_from_image_default.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 08_heatmaps_from_image.bin\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/08_heatmaps_from_image.bin.dir/08_heatmaps_from_image.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable 08_heatmaps_from_image.bin\u001b[0m\n",
            "[ 56%] Built target 08_heatmaps_from_image.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 16_synchronous_custom_postprocessing.bin\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/16_synchronous_custom_postprocessing.bin.dir/16_synchronous_custom_postprocessing.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable 16_synchronous_custom_postprocessing.bin\u001b[0m\n",
            "[ 56%] Built target 16_synchronous_custom_postprocessing.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 11_asynchronous_custom_input_multi_camera.bin\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/11_asynchronous_custom_input_multi_camera.bin.dir/11_asynchronous_custom_input_multi_camera.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable 11_asynchronous_custom_input_multi_camera.bin\u001b[0m\n",
            "[ 56%] Built target 11_asynchronous_custom_input_multi_camera.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 14_synchronous_custom_input.bin\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/14_synchronous_custom_input.bin.dir/14_synchronous_custom_input.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable 13_asynchronous_custom_input_output_and_datum.bin\u001b[0m\n",
            "[ 57%] Built target 13_asynchronous_custom_input_output_and_datum.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 15_synchronous_custom_preprocessing.bin\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/15_synchronous_custom_preprocessing.bin.dir/15_synchronous_custom_preprocessing.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable 14_synchronous_custom_input.bin\u001b[0m\n",
            "[ 57%] Built target 14_synchronous_custom_input.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 10_asynchronous_custom_input.bin\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable 15_synchronous_custom_preprocessing.bin\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/10_asynchronous_custom_input.bin.dir/10_asynchronous_custom_input.cpp.o\u001b[0m\n",
            "[ 58%] Built target 15_synchronous_custom_preprocessing.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target 12_asynchronous_custom_output.bin\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object examples/tutorial_api_cpp/CMakeFiles/12_asynchronous_custom_output.bin.dir/12_asynchronous_custom_output.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable 10_asynchronous_custom_input.bin\u001b[0m\n",
            "[ 59%] Built target 10_asynchronous_custom_input.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target resizeTest.bin\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/tests/CMakeFiles/resizeTest.bin.dir/resizeTest.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable 12_asynchronous_custom_output.bin\u001b[0m\n",
            "[ 60%] Built target 12_asynchronous_custom_output.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target handFromJsonTest.bin\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/tests/CMakeFiles/handFromJsonTest.bin.dir/handFromJsonTest.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable resizeTest.bin\u001b[0m\n",
            "[ 61%] Built target resizeTest.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_3d\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object src/openpose/3d/CMakeFiles/openpose_3d.dir/cameraParameterReader.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable handFromJsonTest.bin\u001b[0m\n",
            "[ 63%] Built target handFromJsonTest.bin\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_calibration\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object src/openpose/calibration/CMakeFiles/openpose_calibration.dir/cameraParameterEstimation.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object src/openpose/3d/CMakeFiles/openpose_3d.dir/defineTemplates.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object src/openpose/3d/CMakeFiles/openpose_3d.dir/jointAngleEstimation.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object src/openpose/3d/CMakeFiles/openpose_3d.dir/poseTriangulation.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object src/openpose/3d/CMakeFiles/openpose_3d.dir/poseTriangulationPrivate.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/openpose/calibration/CMakeFiles/openpose_calibration.dir/gridPatternFunctions.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_3d.so\u001b[0m\n",
            "[ 65%] Built target openpose_3d\n",
            "[ 65%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/face/CMakeFiles/openpose_face.dir/openpose_face_generated_renderFace.cu.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_calibration.so\u001b[0m\n",
            "[ 65%] Built target openpose_calibration\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_filestream\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/bvhSaver.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/cocoJsonSaver.cpp.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_face\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/defineTemplates.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object src/openpose/face/CMakeFiles/openpose_face.dir/defineTemplates.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object src/openpose/face/CMakeFiles/openpose_face.dir/faceDetector.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object src/openpose/face/CMakeFiles/openpose_face.dir/faceDetectorOpenCV.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/fileSaver.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/fileStream.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object src/openpose/face/CMakeFiles/openpose_face.dir/faceExtractorCaffe.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/heatMapSaver.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/imageSaver.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/jsonOfstream.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/keypointSaver.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/peopleJsonSaver.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/udpSender.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/openpose/face/CMakeFiles/openpose_face.dir/faceExtractorNet.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/openpose/filestream/CMakeFiles/openpose_filestream.dir/videoSaver.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object src/openpose/face/CMakeFiles/openpose_face.dir/faceCpuRenderer.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object src/openpose/face/CMakeFiles/openpose_face.dir/faceGpuRenderer.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_filestream.so\u001b[0m\n",
            "[ 71%] Built target openpose_filestream\n",
            "[ 71%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/gpu/CMakeFiles/openpose_gpu.dir/openpose_gpu_generated_cuda.cu.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object src/openpose/face/CMakeFiles/openpose_face.dir/faceRenderer.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object src/openpose/face/CMakeFiles/openpose_face.dir/renderFace.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_face.so\u001b[0m\n",
            "[ 72%] Built target openpose_face\n",
            "[ 72%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/pose/CMakeFiles/openpose_pose.dir/openpose_pose_generated_renderPose.cu.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_gpu\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object src/openpose/gpu/CMakeFiles/openpose_gpu.dir/cuda.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/openpose/gpu/CMakeFiles/openpose_gpu.dir/gpu.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/openpose/gpu/CMakeFiles/openpose_gpu.dir/opencl.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_gpu.so\u001b[0m\n",
            "[ 73%] Built target openpose_gpu\n",
            "[ 74%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/hand/CMakeFiles/openpose_hand.dir/openpose_hand_generated_renderHand.cu.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_pose\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object src/openpose/pose/CMakeFiles/openpose_pose.dir/defineTemplates.cpp.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_hand\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object src/openpose/pose/CMakeFiles/openpose_pose.dir/poseCpuRenderer.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object src/openpose/hand/CMakeFiles/openpose_hand.dir/defineTemplates.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object src/openpose/pose/CMakeFiles/openpose_pose.dir/poseExtractor.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object src/openpose/hand/CMakeFiles/openpose_hand.dir/handDetector.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object src/openpose/pose/CMakeFiles/openpose_pose.dir/poseExtractorCaffe.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/openpose/hand/CMakeFiles/openpose_hand.dir/handDetectorFromTxt.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/openpose/hand/CMakeFiles/openpose_hand.dir/handExtractorCaffe.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/openpose/pose/CMakeFiles/openpose_pose.dir/poseExtractorNet.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object src/openpose/pose/CMakeFiles/openpose_pose.dir/poseGpuRenderer.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object src/openpose/pose/CMakeFiles/openpose_pose.dir/poseParameters.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object src/openpose/hand/CMakeFiles/openpose_hand.dir/handExtractorNet.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object src/openpose/pose/CMakeFiles/openpose_pose.dir/poseParametersRender.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object src/openpose/hand/CMakeFiles/openpose_hand.dir/handCpuRenderer.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object src/openpose/pose/CMakeFiles/openpose_pose.dir/poseRenderer.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object src/openpose/hand/CMakeFiles/openpose_hand.dir/handGpuRenderer.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object src/openpose/pose/CMakeFiles/openpose_pose.dir/renderPose.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object src/openpose/hand/CMakeFiles/openpose_hand.dir/handRenderer.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_pose.so\u001b[0m\n",
            "[ 79%] Built target openpose_pose\n",
            "[ 79%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/net/CMakeFiles/openpose_net.dir/openpose_net_generated_resizeAndMergeBase.cu.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object src/openpose/hand/CMakeFiles/openpose_hand.dir/renderHand.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_hand.so\u001b[0m\n",
            "[ 80%] Built target openpose_hand\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_thread\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object src/openpose/thread/CMakeFiles/openpose_thread.dir/defineTemplates.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/net/CMakeFiles/openpose_net.dir/openpose_net_generated_bodyPartConnectorBase.cu.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_thread.so\u001b[0m\n",
            "[ 80%] Built target openpose_thread\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_tracking\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object src/openpose/tracking/CMakeFiles/openpose_tracking.dir/defineTemplates.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object src/openpose/tracking/CMakeFiles/openpose_tracking.dir/personIdExtractor.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/net/CMakeFiles/openpose_net.dir/openpose_net_generated_maximumBase.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object src/openpose/tracking/CMakeFiles/openpose_tracking.dir/personTracker.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object src/openpose/tracking/CMakeFiles/openpose_tracking.dir/pyramidalLK.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_tracking.so\u001b[0m\n",
            "[ 83%] Built target openpose_tracking\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_unity\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object src/openpose/unity/CMakeFiles/openpose_unity.dir/unityBinding.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_unity.so\u001b[0m\n",
            "[ 83%] Built target openpose_unity\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_gui\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object src/openpose/gui/CMakeFiles/openpose_gui.dir/defineTemplates.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object src/openpose/gui/CMakeFiles/openpose_gui.dir/frameDisplayer.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object src/openpose/gui/CMakeFiles/openpose_gui.dir/gui.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object src/openpose/gui/CMakeFiles/openpose_gui.dir/guiAdam.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object src/openpose/gui/CMakeFiles/openpose_gui.dir/gui3D.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/openpose/net/CMakeFiles/openpose_net.dir/openpose_net_generated_nmsBase.cu.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object src/openpose/gui/CMakeFiles/openpose_gui.dir/guiInfoAdder.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_gui.so\u001b[0m\n",
            "[ 85%] Built target openpose_gui\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_producer\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object src/openpose/producer/CMakeFiles/openpose_producer.dir/datumProducer.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object src/openpose/producer/CMakeFiles/openpose_producer.dir/defineTemplates.cpp.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_net\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/bodyPartConnectorBase.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object src/openpose/producer/CMakeFiles/openpose_producer.dir/flirReader.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object src/openpose/producer/CMakeFiles/openpose_producer.dir/imageDirectoryReader.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/bodyPartConnectorBaseCL.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/bodyPartConnectorCaffe.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/maximumBase.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object src/openpose/producer/CMakeFiles/openpose_producer.dir/ipCameraReader.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object src/openpose/producer/CMakeFiles/openpose_producer.dir/producer.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/maximumCaffe.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/netCaffe.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object src/openpose/producer/CMakeFiles/openpose_producer.dir/spinnakerWrapper.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/netOpenCv.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object src/openpose/producer/CMakeFiles/openpose_producer.dir/videoCaptureReader.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/nmsBase.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object src/openpose/producer/CMakeFiles/openpose_producer.dir/videoReader.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/nmsBaseCL.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object src/openpose/producer/CMakeFiles/openpose_producer.dir/webcamReader.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/nmsCaffe.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/resizeAndMergeBase.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_producer.so\u001b[0m\n",
            "[ 93%] Built target openpose_producer\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_utilities\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object src/openpose/utilities/CMakeFiles/openpose_utilities.dir/errorAndLog.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object src/openpose/utilities/CMakeFiles/openpose_utilities.dir/fileSystem.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/resizeAndMergeBaseCL.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object src/openpose/utilities/CMakeFiles/openpose_utilities.dir/flagsToOpenPose.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object src/openpose/net/CMakeFiles/openpose_net.dir/resizeAndMergeCaffe.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object src/openpose/utilities/CMakeFiles/openpose_utilities.dir/keypoint.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_net.so\u001b[0m\n",
            "[ 96%] Built target openpose_net\n",
            "[ 96%] \u001b[32mBuilding CXX object src/openpose/utilities/CMakeFiles/openpose_utilities.dir/openCv.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object src/openpose/utilities/CMakeFiles/openpose_utilities.dir/openCvPrivate.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object src/openpose/utilities/CMakeFiles/openpose_utilities.dir/profiler.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object src/openpose/utilities/CMakeFiles/openpose_utilities.dir/string.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_utilities.so\u001b[0m\n",
            "[ 97%] Built target openpose_utilities\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_wrapper\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object src/openpose/wrapper/CMakeFiles/openpose_wrapper.dir/defineTemplates.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object src/openpose/wrapper/CMakeFiles/openpose_wrapper.dir/wrapperAuxiliary.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object src/openpose/wrapper/CMakeFiles/openpose_wrapper.dir/wrapperStructExtra.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object src/openpose/wrapper/CMakeFiles/openpose_wrapper.dir/wrapperStructFace.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object src/openpose/wrapper/CMakeFiles/openpose_wrapper.dir/wrapperStructGui.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object src/openpose/wrapper/CMakeFiles/openpose_wrapper.dir/wrapperStructHand.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object src/openpose/wrapper/CMakeFiles/openpose_wrapper.dir/wrapperStructInput.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object src/openpose/wrapper/CMakeFiles/openpose_wrapper.dir/wrapperStructOutput.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object src/openpose/wrapper/CMakeFiles/openpose_wrapper.dir/wrapperStructPose.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX shared library libopenpose_wrapper.so\u001b[0m\n",
            "[100%] Built target openpose_wrapper\n",
            "Built target openpose_lib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human parse"
      ],
      "metadata": {
        "id": "veoqdndStlW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/Engineering-Course/CIHP_PGN.git\n",
        "%cd /content/CIHP_PGN\n",
        "!gdown --id 1Mqpse5Gen4V4403wFEpv3w3JAsWw2uhk\n",
        "!unzip CIHP_pgn.zip\n",
        "!pip install --upgrade tf_slim"
      ],
      "metadata": {
        "id": "Btyy67-9t20l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516598ab-a09c-4184-f59f-8418b77246c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'CIHP_PGN'...\n",
            "remote: Enumerating objects: 4402, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 4402 (delta 14), reused 21 (delta 6), pack-reused 4364\u001b[K\n",
            "Receiving objects: 100% (4402/4402), 852.01 KiB | 12.72 MiB/s, done.\n",
            "Resolving deltas: 100% (720/720), done.\n",
            "/content/CIHP_PGN\n",
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Mqpse5Gen4V4403wFEpv3w3JAsWw2uhk\n",
            "To: /content/CIHP_PGN/CIHP_pgn.zip\n",
            "100% 1.23G/1.23G [00:25<00:00, 48.5MB/s]\n",
            "Archive:  CIHP_pgn.zip\n",
            "   creating: CIHP_pgn/\n",
            "  inflating: CIHP_pgn/checkpoint     \n",
            "  inflating: CIHP_pgn/model.ckpt-593292.data-00000-of-00001  \n",
            "  inflating: CIHP_pgn/model.ckpt-593292.index  \n",
            "  inflating: CIHP_pgn/model.ckpt-593292.meta  \n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf_slim in /usr/local/lib/python3.9/dist-packages (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from tf_slim) (1.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overwriting network.py"
      ],
      "metadata": {
        "id": "0KOGV_IiuBaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/CIHP_PGN/kaffe/tensorflow/network.py\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tf_slim as slim\n",
        "slim = slim\n",
        "\n",
        "DEFAULT_PADDING = 'SAME'\n",
        "\n",
        "\n",
        "def layer(op):\n",
        "    '''Decorator for composable network layers.'''\n",
        "\n",
        "    def layer_decorated(self, *args, **kwargs):\n",
        "        # Automatically set a name if not provided.\n",
        "        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))\n",
        "        # Figure out the layer inputs.\n",
        "        if len(self.terminals) == 0:\n",
        "            raise RuntimeError('No input variables found for layer %s.' % name)\n",
        "        elif len(self.terminals) == 1:\n",
        "            layer_input = self.terminals[0]\n",
        "        else:\n",
        "            layer_input = list(self.terminals)\n",
        "        # Perform the operation and get the output.\n",
        "        layer_output = op(self, layer_input, *args, **kwargs)\n",
        "        # Add to layer LUT.\n",
        "        self.layers[name] = layer_output\n",
        "        # This output is now the input for the next layer.\n",
        "        self.feed(layer_output)\n",
        "        # Return self for chained calls.\n",
        "        return self\n",
        "\n",
        "    return layer_decorated\n",
        "\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "    def __init__(self, inputs, trainable=True, is_training=False, n_classes=20, keep_prob=1):\n",
        "        # The input nodes for this network\n",
        "        self.inputs = inputs\n",
        "        # The current list of terminal nodes\n",
        "        self.terminals = []\n",
        "        # Mapping from layer names to layers\n",
        "        self.layers = dict(inputs)\n",
        "        # If true, the resulting variables are set as trainable\n",
        "        self.trainable = trainable\n",
        "        # Switch variable for dropout\n",
        "        self.use_dropout = tf.placeholder_with_default(tf.constant(1.0),\n",
        "                                                       shape=[],\n",
        "                                                       name='use_dropout')\n",
        "        self.setup(is_training, n_classes, keep_prob)\n",
        "\n",
        "    def setup(self, is_training, n_classes, keep_prob):\n",
        "        '''Construct the network. '''\n",
        "        raise NotImplementedError('Must be implemented by the subclass.')\n",
        "\n",
        "    def load(self, data_path, session, ignore_missing=False):\n",
        "        '''Load network weights.\n",
        "        data_path: The path to the numpy-serialized network weights\n",
        "        session: The current TensorFlow session\n",
        "        ignore_missing: If true, serialized weights for missing layers are ignored.\n",
        "        '''\n",
        "        data_dict = np.load(data_path).item()\n",
        "        for op_name in data_dict:\n",
        "            with tf.variable_scope(op_name, reuse=True):\n",
        "                for param_name, data in data_dict[op_name].iteritems():\n",
        "                    try:\n",
        "                        var = tf.get_variable(param_name)\n",
        "                        session.run(var.assign(data))\n",
        "                    except ValueError:\n",
        "                        if not ignore_missing:\n",
        "                            raise\n",
        "\n",
        "    def feed(self, *args):\n",
        "        '''Set the input(s) for the next operation by replacing the terminal nodes.\n",
        "        The arguments can be either layer names or the actual layers.\n",
        "        '''\n",
        "        assert len(args) != 0\n",
        "        self.terminals = []\n",
        "        for fed_layer in args:\n",
        "            if isinstance(fed_layer, str):\n",
        "                try:\n",
        "                    fed_layer = self.layers[fed_layer]\n",
        "                except KeyError:\n",
        "                    raise KeyError('Unknown layer name fed: %s' % fed_layer)\n",
        "            self.terminals.append(fed_layer)\n",
        "        return self\n",
        "\n",
        "    def get_output(self):\n",
        "        '''Returns the current network output.'''\n",
        "        return self.terminals[-1]\n",
        "\n",
        "    def get_unique_name(self, prefix):\n",
        "        '''Returns an index-suffixed unique name for the given prefix.\n",
        "        This is used for auto-generating layer names based on the type-prefix.\n",
        "        '''\n",
        "        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n",
        "        return '%s_%d' % (prefix, ident)\n",
        "\n",
        "    def make_var(self, name, shape):\n",
        "        '''Creates a new TensorFlow variable.'''\n",
        "        return tf.get_variable(name, shape, trainable=self.trainable)\n",
        "\n",
        "    def make_w_var(self, name, shape):\n",
        "        '''Creates a new TensorFlow variable.'''\n",
        "        stddev=0.01\n",
        "        return tf.get_variable(name, shape, initializer=tf.truncated_normal_initializer(stddev=stddev), trainable=self.trainable)\n",
        "\n",
        "    def make_b_var(self, name, shape):\n",
        "        return tf.get_variable(name, shape, initializer=tf.constant_initializer(0.0), trainable=self.trainable)\n",
        "\n",
        "    def validate_padding(self, padding):\n",
        "        '''Verifies that the padding is one of the supported ones.'''\n",
        "        assert padding in ('SAME', 'VALID')\n",
        "\n",
        "    @layer\n",
        "    def conv(self,\n",
        "             input,\n",
        "             k_h,\n",
        "             k_w,\n",
        "             c_o,\n",
        "             s_h,\n",
        "             s_w,\n",
        "             name,\n",
        "             relu=True,\n",
        "             padding=DEFAULT_PADDING,\n",
        "             group=1,\n",
        "             biased=True):\n",
        "        # Verify that the padding is acceptable\n",
        "        self.validate_padding(padding)\n",
        "        # Get the number of channels in the input\n",
        "        c_i = input.get_shape()[-1]\n",
        "        # Verify that the grouping parameter is valid\n",
        "        assert c_i % group == 0\n",
        "        assert c_o % group == 0\n",
        "        # Convolution for a given input and kernel\n",
        "        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            kernel = self.make_w_var('weights', shape=[k_h, k_w, c_i // group, c_o])\n",
        "            if group == 1:\n",
        "                # This is the common-case. Convolve the input without any further complications.\n",
        "                output = convolve(input, kernel)\n",
        "            else:\n",
        "                # Split the input into groups and then convolve each of them independently\n",
        "                input_groups = tf.split(3, group, input)\n",
        "                kernel_groups = tf.split(3, group, kernel)\n",
        "                output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
        "                # Concatenate the groups\n",
        "                output = tf.concat(3, output_groups)\n",
        "            # Add the biases\n",
        "            if biased:\n",
        "                biases = self.make_b_var('biases', [c_o])\n",
        "                output = tf.nn.bias_add(output, biases)\n",
        "            if relu:\n",
        "                # ReLU non-linearity\n",
        "                output = tf.nn.relu(output, name=scope.name)\n",
        "            return output\n",
        "\n",
        "    @layer\n",
        "    def atrous_conv(self,\n",
        "                    input,\n",
        "                    k_h,\n",
        "                    k_w,\n",
        "                    c_o,\n",
        "                    dilation,\n",
        "                    name,\n",
        "                    relu=True,\n",
        "                    padding=DEFAULT_PADDING,\n",
        "                    group=1,\n",
        "                    biased=True):\n",
        "        # Verify that the padding is acceptable\n",
        "        self.validate_padding(padding)\n",
        "        # Get the number of channels in the input\n",
        "        c_i = input.get_shape()[-1]\n",
        "        # Verify that the grouping parameter is valid\n",
        "        assert c_i % group == 0\n",
        "        assert c_o % group == 0\n",
        "        # Convolution for a given input and kernel\n",
        "        convolve = lambda i, k: tf.nn.atrous_conv2d(i, k, dilation, padding=padding)\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            kernel = self.make_w_var('weights', shape=[k_h, k_w, c_i // group, c_o])\n",
        "            if group == 1:\n",
        "                # This is the common-case. Convolve the input without any further complications.\n",
        "                output = convolve(input, kernel)\n",
        "            else:\n",
        "                # Split the input into groups and then convolve each of them independently\n",
        "                input_groups = tf.split(3, group, input)\n",
        "                kernel_groups = tf.split(3, group, kernel)\n",
        "                output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
        "                # Concatenate the groups\n",
        "                output = tf.concat(3, output_groups)\n",
        "            # Add the biases\n",
        "            if biased:\n",
        "                biases = self.make_b_var('biases', [c_o])\n",
        "                output = tf.nn.bias_add(output, biases)\n",
        "            if relu:\n",
        "                # ReLU non-linearity\n",
        "                output = tf.nn.relu(output, name=scope.name)\n",
        "            return output\n",
        "        \n",
        "    @layer\n",
        "    def relu(self, input, name):\n",
        "        return tf.nn.relu(input, name=name)\n",
        "\n",
        "    @layer\n",
        "    def max_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n",
        "        self.validate_padding(padding)\n",
        "        return tf.nn.max_pool(input,\n",
        "                              ksize=[1, k_h, k_w, 1],\n",
        "                              strides=[1, s_h, s_w, 1],\n",
        "                              padding=padding,\n",
        "                              name=name)\n",
        "\n",
        "    @layer\n",
        "    def avg_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n",
        "        self.validate_padding(padding)\n",
        "        return tf.nn.avg_pool(input,\n",
        "                              ksize=[1, k_h, k_w, 1],\n",
        "                              strides=[1, s_h, s_w, 1],\n",
        "                              padding=padding,\n",
        "                              name=name)\n",
        "\n",
        "    @layer\n",
        "    def lrn(self, input, radius, alpha, beta, name, bias=1.0):\n",
        "        return tf.nn.local_response_normalization(input,\n",
        "                                                  depth_radius=radius,\n",
        "                                                  alpha=alpha,\n",
        "                                                  beta=beta,\n",
        "                                                  bias=bias,\n",
        "                                                  name=name)\n",
        "\n",
        "    @layer\n",
        "    def concat(self, inputs, axis, name):\n",
        "        return tf.concat(values=inputs, axis=axis, name=name)\n",
        "\n",
        "    @layer\n",
        "    def add(self, inputs, name):\n",
        "        return tf.add_n(inputs, name=name)\n",
        "\n",
        "    @layer\n",
        "    def fc(self, input, num_out, name, relu=True):\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            input_shape = input.get_shape()\n",
        "            if input_shape.ndims == 4:\n",
        "                # The input is spatial. Vectorize it first.\n",
        "                dim = 1\n",
        "                for d in input_shape[1:].as_list():\n",
        "                    dim *= d\n",
        "                feed_in = tf.reshape(input, [-1, dim])\n",
        "            else:\n",
        "                feed_in, dim = (input, input_shape[-1].value)\n",
        "            weights = self.make_var('weights', shape=[dim, num_out])\n",
        "            biases = self.make_var('biases', [num_out])\n",
        "            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n",
        "            fc = op(feed_in, weights, biases, name=scope.name)\n",
        "            return fc\n",
        "\n",
        "    @layer\n",
        "    def softmax(self, input, name):\n",
        "        input_shape = map(lambda v: v.value, input.get_shape())\n",
        "        if len(input_shape) > 2:\n",
        "            # For certain models (like NiN), the singleton spatial dimensions\n",
        "            # need to be explicitly squeezed, since they're not broadcast-able\n",
        "            # in TensorFlow's NHWC ordering (unlike Caffe's NCHW).\n",
        "            if input_shape[1] == 1 and input_shape[2] == 1:\n",
        "                input = tf.squeeze(input, squeeze_dims=[1, 2])\n",
        "            else:\n",
        "                raise ValueError('Rank 2 tensor input expected for softmax!')\n",
        "        return tf.nn.softmax(input, name)\n",
        "        \n",
        "    @layer\n",
        "    def batch_normalization(self, input, name, is_training, activation_fn=None, scale=True):\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            output = slim.batch_norm(\n",
        "                input,\n",
        "                activation_fn=activation_fn,\n",
        "                is_training=is_training,\n",
        "                updates_collections=None,\n",
        "                scale=scale,\n",
        "                scope=scope)\n",
        "            return output\n",
        "\n",
        "    @layer\n",
        "    def dropout(self, input, keep_prob, name):\n",
        "        keep = 1 - self.use_dropout + (self.use_dropout * keep_prob)\n",
        "        return tf.nn.dropout(input, keep, name=name)\n",
        "\n",
        "    @layer\n",
        "    def upsample(self, input, size_h, size_w, name):\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            return tf.image.resize_images(input, size=[size_h, size_w])\n",
        "\n",
        "    @layer\n",
        "    def pyramid_pooling(self, input, o_c, pool_size, name):\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            dims = tf.shape(input)\n",
        "            out_height, out_width = dims[1], dims[2]\n",
        "            pool_ly = tf.nn.avg_pool(input, ksize=[1, pool_size, pool_size, 1], strides=[1, pool_size, pool_size, 1],\n",
        "                                     padding=DEFAULT_PADDING, name='pool_ly')\n",
        "            weight = self.make_w_var('weights', shape=[3, 3, pool_ly.get_shape()[-1], o_c])\n",
        "            biases = self.make_var('biases', o_c)\n",
        "            conv_ly = tf.nn.conv2d(pool_ly, weight, strides=[1, 1, 1, 1], padding='SAME', name='conv_ly')\n",
        "            conv_ly = tf.nn.bias_add(conv_ly, biases)\n",
        "            conv_ly = tf.nn.relu(conv_ly, name='relu_ly')\n",
        "            output = tf.image.resize_bilinear(conv_ly, [out_height, out_width])\n",
        "            return output\n"
      ],
      "metadata": {
        "id": "ucg6e8i5uDwV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ade17b-dfbd-4ac8-8bfb-decf8de9a99a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/CIHP_PGN/kaffe/tensorflow/network.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overwriting image_reade_inf.py"
      ],
      "metadata": {
        "id": "XccDQmIuuGRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/CIHP_PGN/utils/image_reade_inf.py\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "IGNORE_LABEL = 255\n",
        "IMG_MEAN = np.array((125.0, 114.4, 107.9), dtype=np.float32)\n",
        "\n",
        "\n",
        "def image_scaling(img):\n",
        "    \"\"\"\n",
        "    Randomly scales the images between 0.5 to 1.5 times the original size.\n",
        "    Args:\n",
        "      img: Training image to scale.\n",
        "      label: Segmentation mask to scale.\n",
        "    \"\"\"\n",
        "\n",
        "    scale = tf.random_uniform([1], minval=0.5, maxval=2.0, dtype=tf.float32,\n",
        "                              seed=None)\n",
        "    h_new = tf.to_int32(tf.multiply(tf.to_float(tf.shape(img)[0]), scale))\n",
        "    w_new = tf.to_int32(tf.multiply(tf.to_float(tf.shape(img)[1]), scale))\n",
        "    new_shape = tf.squeeze(tf.stack([h_new, w_new]), squeeze_dims=[1])\n",
        "    img = tf.image.resize_images(img, new_shape)\n",
        "    # label = tf.image.resize_nearest_neighbor(tf.expand_dims(label, 0),\n",
        "    #                                          new_shape)\n",
        "    # label = tf.squeeze(label, squeeze_dims=[0])\n",
        "    # edge = tf.image.resize_nearest_neighbor(tf.expand_dims(edge, 0), new_shape)\n",
        "    # edge = tf.squeeze(edge, squeeze_dims=[0])\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def image_mirroring(img):\n",
        "    \"\"\"\n",
        "    Randomly mirrors the images.\n",
        "    Args:\n",
        "      img: Training image to mirror.\n",
        "      label: Segmentation mask to mirror.\n",
        "    \"\"\"\n",
        "\n",
        "    distort_left_right_random = \\\n",
        "    tf.random_uniform([1], 0, 1.0, dtype=tf.float32)[0]\n",
        "    mirror = tf.less(tf.stack([1.0, distort_left_right_random, 1.0]), 0.5)\n",
        "    mirror = tf.boolean_mask([0, 1, 2], mirror)\n",
        "    img = tf.reverse(img, mirror)\n",
        "    # label = tf.reverse(label, mirror)\n",
        "    # edge = tf.reverse(edge, mirror)\n",
        "    return img\n",
        "\n",
        "\n",
        "def random_resize_img_labels(image, label, resized_h, resized_w):\n",
        "    scale = tf.random_uniform([1], minval=0.75, maxval=1.25, dtype=tf.float32,\n",
        "                              seed=None)\n",
        "    h_new = tf.to_int32(tf.multiply(tf.to_float(resized_h), scale))\n",
        "    w_new = tf.to_int32(tf.multiply(tf.to_float(resized_w), scale))\n",
        "\n",
        "    new_shape = tf.squeeze(tf.stack([h_new, w_new]), squeeze_dims=[1])\n",
        "    img = tf.image.resize_images(image, new_shape)\n",
        "    label = tf.image.resize_nearest_neighbor(tf.expand_dims(label, 0),\n",
        "                                             new_shape)\n",
        "    label = tf.squeeze(label, squeeze_dims=[0])\n",
        "    return img, label\n",
        "\n",
        "\n",
        "def resize_img_labels(image, label, resized_h, resized_w):\n",
        "    new_shape = tf.stack([tf.to_int32(resized_h), tf.to_int32(resized_w)])\n",
        "    img = tf.image.resize_images(image, new_shape)\n",
        "    label = tf.image.resize_nearest_neighbor(tf.expand_dims(label, 0),\n",
        "                                             new_shape)\n",
        "    label = tf.squeeze(label, squeeze_dims=[0])\n",
        "    return img, label\n",
        "\n",
        "\n",
        "def random_crop_and_pad_image_and_labels(image, crop_h, crop_w,\n",
        "                                         ignore_label=255):\n",
        "    \"\"\"\n",
        "    Randomly crop and pads the input images.\n",
        "    Args:\n",
        "      image: Training image to crop/ pad.\n",
        "      label: Segmentation mask to crop/ pad.\n",
        "      crop_h: Height of cropped segment.\n",
        "      crop_w: Width of cropped segment.\n",
        "      ignore_label: Label to ignore during the training.\n",
        "    \"\"\"\n",
        "\n",
        "    # label = tf.cast(label, dtype=tf.float32)\n",
        "    # label = label - ignore_label  # Needs to be subtracted and later added due to 0 padding.\n",
        "    # edge = tf.cast(edge, dtype=tf.float32)\n",
        "    # edge = edge - 0\n",
        "\n",
        "    combined = tf.concat([image, label, edge], 2)\n",
        "    image_shape = tf.shape(image)\n",
        "    combined_pad = tf.image.pad_to_bounding_box(combined, 0, 0,\n",
        "                                                tf.maximum(crop_h,\n",
        "                                                           image_shape[0]),\n",
        "                                                tf.maximum(crop_w,\n",
        "                                                           image_shape[1]))\n",
        "\n",
        "    last_image_dim = tf.shape(image)[-1]\n",
        "    last_label_dim = tf.shape(label)[-1]\n",
        "    combined_crop = tf.random_crop(combined_pad, [crop_h, crop_w, 4 + 1])\n",
        "    img_crop = combined_crop[:, :, :last_image_dim]\n",
        "    label_crop = combined_crop[:, :,\n",
        "                 last_image_dim:last_image_dim + last_label_dim]\n",
        "    edge_crop = combined_crop[:, :, last_image_dim + last_label_dim:]\n",
        "    label_crop = label_crop + ignore_label\n",
        "    label_crop = tf.cast(label_crop, dtype=tf.uint8)\n",
        "    edge_crop = edge_crop + 0\n",
        "    edge_crop = tf.cast(edge_crop, dtype=tf.uint8)\n",
        "\n",
        "    # Set static shape so that tensorflow knows shape at compile time. \n",
        "    img_crop.set_shape((crop_h, crop_w, 3))\n",
        "    label_crop.set_shape((crop_h, crop_w, 1))\n",
        "    edge_crop.set_shape((crop_h, crop_w, 1))\n",
        "    return img_crop, label_crop, edge_crop\n",
        "\n",
        "\n",
        "def read_labeled_image_reverse_list(data_dir, data_list):\n",
        "    \"\"\"Reads txt file containing paths to images and ground truth masks.\n",
        "    \n",
        "    Args:\n",
        "      data_dir: path to the directory with images and masks.\n",
        "      data_list: path to the file with lines of the form '/path/to/image /path/to/mask'.\n",
        "       \n",
        "    Returns:\n",
        "      Two lists with all file names for images and masks, respectively.\n",
        "    \"\"\"\n",
        "    f = open(data_list, 'r')\n",
        "    images = []\n",
        "    masks = []\n",
        "    masks_rev = []\n",
        "    for line in f:\n",
        "        try:\n",
        "            image, mask, mask_rev = line.strip(\"\\n\").split(' ')\n",
        "        except ValueError:  # Adhoc for test.\n",
        "            image = mask = mask_rev = line.strip(\"\\n\")\n",
        "        images.append(data_dir + image)\n",
        "        masks.append(data_dir + mask)\n",
        "        masks_rev.append(data_dir + mask_rev)\n",
        "    return images, masks, masks_rev\n",
        "\n",
        "\n",
        "def read_labeled_image_list(data_dir, data_list):\n",
        "    \"\"\"Reads txt file containing paths to images and ground truth masks.\n",
        "    \n",
        "    Args:\n",
        "      data_dir: path to the directory with images and masks.\n",
        "      data_list: path to the file with lines of the form '/path/to/image /path/to/mask'.\n",
        "       \n",
        "    Returns:\n",
        "      Two lists with all file names for images and masks, respectively.\n",
        "    \"\"\"\n",
        "    f = open(data_list, 'r')\n",
        "    images = []\n",
        "    masks = []\n",
        "    for line in f:\n",
        "        try:\n",
        "            image, mask = line.strip(\"\\n\").split(' ')\n",
        "        except ValueError:  # Adhoc for test.\n",
        "            image = mask = line.strip(\"\\n\")\n",
        "        images.append(data_dir + image)\n",
        "        masks.append(data_dir + mask)\n",
        "    return images, masks\n",
        "\n",
        "\n",
        "def read_edge_list(data_dir, data_id_list):\n",
        "    f = open(data_id_list, 'r')\n",
        "    edges = []\n",
        "    for line in f:\n",
        "        edge = line.strip(\"\\n\")\n",
        "        edges.append(data_dir + '/edges/' + edge + '.png')\n",
        "    return edges\n",
        "\n",
        "\n",
        "def read_images_from_disk(input_queue, input_size, random_scale,\n",
        "                          random_mirror=False):  # optional pre-processing arguments\n",
        "    \"\"\"Read one image and its corresponding mask with optional pre-processing.\n",
        "    \n",
        "    Args:\n",
        "      input_queue: tf queue with paths to the image and its mask.\n",
        "      input_size: a tuple with (height, width) values.\n",
        "                  If not given, return images of original size.\n",
        "      random_scale: whether to randomly scale the images prior\n",
        "                    to random crop.\n",
        "      random_mirror: whether to randomly mirror the images prior\n",
        "                    to random crop.\n",
        "      \n",
        "    Returns:\n",
        "      Two tensors: the decoded image and its mask.\n",
        "    \"\"\"\n",
        "    img_contents = tf.io.read_file(input_queue[0])\n",
        "    # label_contents = tf.read_file(input_queue[1])\n",
        "    # edge_contents = tf.read_file(input_queue[2])\n",
        "\n",
        "    img = tf.image.decode_jpeg(img_contents, channels=3)\n",
        "    img_r, img_g, img_b = tf.split(value=img, num_or_size_splits=3, axis=2)\n",
        "    img = tf.cast(tf.concat([img_b, img_g, img_r], 2), dtype=tf.float32)\n",
        "    # Extract mean.\n",
        "    img -= IMG_MEAN\n",
        "    # label = tf.image.decode_png(label_contents, channels=1)\n",
        "    # edge = tf.image.decode_png(edge_contents, channels=1)\n",
        "\n",
        "    # if input_size is not None:\n",
        "    #     h, w = input_size\n",
        "    #\n",
        "    #     # Randomly scale the images and labels.\n",
        "    #     if random_scale:\n",
        "    #         img, label, edge = image_scaling(img)\n",
        "    #\n",
        "    #     # Randomly mirror the images and labels.\n",
        "    #     if random_mirror:\n",
        "    #         img, label, edge = image_mirroring(img)\n",
        "    #\n",
        "    #     # Randomly crops the images and labels.\n",
        "    #     img, label, edge = random_crop_and_pad_image_and_labels(img, h, w,\n",
        "    #                                                             IGNORE_LABEL)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "class ImageReader(object):\n",
        "    '''Generic ImageReader which reads images and corresponding segmentation\n",
        "       masks from the disk, and enqueues them into a TensorFlow queue.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, data_dir, input_size,\n",
        "                 random_scale,\n",
        "                 random_mirror, shuffle, coord):\n",
        "        '''Initialise an ImageReader.\n",
        "        \n",
        "        Args:\n",
        "          data_dir: path to the directory with images and masks.\n",
        "          data_list: path to the file with lines of the form '/path/to/image /path/to/mask'.\n",
        "          data_id_list: path to the file of image id.\n",
        "          input_size: a tuple with (height, width) values, to which all the images will be resized.\n",
        "          random_scale: whether to randomly scale the images prior to random crop.\n",
        "          random_mirror: whether to randomly mirror the images prior to random crop.\n",
        "          coord: TensorFlow queue coordinator.\n",
        "        '''\n",
        "        self.image_list = data_dir\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.coord = coord\n",
        "\n",
        "        # self.image_list, self.label_list = read_labeled_image_list(\n",
        "        #     self.data_dir, self.data_list)\n",
        "        # self.edge_list = read_edge_list(self.data_dir, self.data_id_list)\n",
        "        self.images = tf.convert_to_tensor(self.image_list, dtype=tf.string)\n",
        "        # self.labels = tf.convert_to_tensor(self.label_list, dtype=tf.string)\n",
        "        # self.edges = tf.convert_to_tensor(self.edge_list, dtype=tf.string)\n",
        "        self.queue = data_dir\n",
        "        # self.queue = tf.data.Dataset.from_tensor_slices([self.images])\n",
        "        print(self.queue)\n",
        "        self.image = read_images_from_disk(self.queue, self.input_size,\n",
        "                                                                  random_scale,\n",
        "                                                                  random_mirror)\n",
        "\n",
        "    def dequeue(self, num_elements):\n",
        "        '''Pack images and labels into a batch.\n",
        "        \n",
        "        Args:\n",
        "          num_elements: the batch size.\n",
        "          \n",
        "        Returns:\n",
        "          Two tensors of size (batch_size, h, w, {3, 1}) for images and masks.'''\n",
        "        batch_list = [self.image, self.label, self.edge]\n",
        "        image_batch, label_batch, edge_batch = tf.train.batch(\n",
        "            [self.image, self.label, self.edge], num_elements)\n",
        "        return image_batch, label_batch, edge_batch\n"
      ],
      "metadata": {
        "id": "psxeGZnKuJdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a331a1-8bea-4642-81f8-51943ef482b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/CIHP_PGN/utils/image_reade_inf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overwriting inf_pgn.py"
      ],
      "metadata": {
        "id": "NLKWVI44uca1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/CIHP_PGN/inf_pgn.py\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import scipy.misc\n",
        "import scipy.io as sio\n",
        "import cv2\n",
        "import argparse\n",
        "from glob import glob\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from utils.image_reade_inf import *\n",
        "from utils.ops import  *\n",
        "from utils.utils import *\n",
        "from utils.model_pgn import *\n",
        "\n",
        "argp = argparse.ArgumentParser(description=\"Inference pipeline\")\n",
        "argp.add_argument('-i',\n",
        "                  '--directory',\n",
        "                  type=str, help='Path of the input dir',\n",
        "                  default='./datasets/images')\n",
        "argp.add_argument('-o',\n",
        "                  '--output',\n",
        "                  type=str, help='Path of the input dir',\n",
        "                  default='./datasets/output')\n",
        "\n",
        "args = argp.parse_args()\n",
        "\n",
        "image_list_inp = []\n",
        "for i in glob(os.path.join(args.directory, '**'), recursive=True):\n",
        "    if os.path.isfile(i):\n",
        "        image_list_inp.append(i)\n",
        "# print(image_list)\n",
        "image_list_inp = image_list_inp[:5]\n",
        "# sys.exit(2)\n",
        "N_CLASSES = 20\n",
        "NUM_STEPS = len(image_list_inp)\n",
        "RESTORE_FROM = '/content/CIHP_PGN/CIHP_pgn/'\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Create the model and start the evaluation process.\"\"\"\n",
        "    # Create queue coordinator.\n",
        "    coord = tf.train.Coordinator()\n",
        "    # Load reader.\n",
        "    with tf.name_scope(\"create_inputs\"):\n",
        "        reader = ImageReader(image_list_inp, None, False,\n",
        "                             False, False, coord)\n",
        "        image = reader.image\n",
        "        image_rev = tf.reverse(image, tf.stack([1]))\n",
        "        image_list = reader.image_list\n",
        "\n",
        "    image_batch = tf.stack([image, image_rev])\n",
        "    h_orig, w_orig = tf.cast(tf.shape(image_batch)[1], tf.float32), tf.cast(tf.shape(image_batch)[2], tf.float32)\n",
        "    image_batch050 = tf.image.resize(image_batch, tf.stack([tf.cast(tf.multiply(h_orig, 0.50), tf.int32), tf.cast(tf.multiply(w_orig, 0.50), tf.int32)]))\n",
        "    image_batch075 = tf.image.resize(image_batch, tf.stack([tf.cast(tf.multiply(h_orig, 0.75), tf.int32), tf.cast(tf.multiply(w_orig, 0.75), tf.int32)]))\n",
        "    image_batch125 = tf.image.resize(image_batch, tf.stack([tf.cast(tf.multiply(h_orig, 1.25), tf.int32), tf.cast(tf.multiply(w_orig, 1.25), tf.int32)]))\n",
        "    image_batch150 = tf.image.resize(image_batch, tf.stack([tf.cast(tf.multiply(h_orig, 1.50), tf.int32), tf.cast(tf.multiply(w_orig, 1.50), tf.int32)]))\n",
        "    image_batch175 = tf.image.resize(image_batch, tf.stack([tf.cast(tf.multiply(h_orig, 1.75), tf.int32), tf.cast(tf.multiply(w_orig, 1.75), tf.int32)]))\n",
        "         \n",
        "    # Create network.\n",
        "    with tf.compat.v1.variable_scope('', reuse=False):\n",
        "        net_100 = PGNModel({'data': image_batch}, is_training=False, n_classes=N_CLASSES)\n",
        "    with tf.compat.v1.variable_scope('', reuse=True):\n",
        "        net_050 = PGNModel({'data': image_batch050}, is_training=False, n_classes=N_CLASSES)\n",
        "    with tf.compat.v1.variable_scope('', reuse=True):\n",
        "        net_075 = PGNModel({'data': image_batch075}, is_training=False, n_classes=N_CLASSES)\n",
        "    with tf.compat.v1.variable_scope('', reuse=True):\n",
        "        net_125 = PGNModel({'data': image_batch125}, is_training=False, n_classes=N_CLASSES)\n",
        "    with tf.compat.v1.variable_scope('', reuse=True):\n",
        "        net_150 = PGNModel({'data': image_batch150}, is_training=False, n_classes=N_CLASSES)\n",
        "    with tf.compat.v1.variable_scope('', reuse=True):\n",
        "        net_175 = PGNModel({'data': image_batch175}, is_training=False, n_classes=N_CLASSES)\n",
        "    # parsing net\n",
        "\n",
        "    parsing_out1_050 = net_050.layers['parsing_fc']\n",
        "    parsing_out1_075 = net_075.layers['parsing_fc']\n",
        "    parsing_out1_100 = net_100.layers['parsing_fc']\n",
        "    parsing_out1_125 = net_125.layers['parsing_fc']\n",
        "    parsing_out1_150 = net_150.layers['parsing_fc']\n",
        "    parsing_out1_175 = net_175.layers['parsing_fc']\n",
        "\n",
        "    parsing_out2_050 = net_050.layers['parsing_rf_fc']\n",
        "    parsing_out2_075 = net_075.layers['parsing_rf_fc']\n",
        "    parsing_out2_100 = net_100.layers['parsing_rf_fc']\n",
        "    parsing_out2_125 = net_125.layers['parsing_rf_fc']\n",
        "    parsing_out2_150 = net_150.layers['parsing_rf_fc']\n",
        "    parsing_out2_175 = net_175.layers['parsing_rf_fc']\n",
        "\n",
        "    # edge net\n",
        "    edge_out2_100 = net_100.layers['edge_rf_fc']\n",
        "    edge_out2_125 = net_125.layers['edge_rf_fc']\n",
        "    edge_out2_150 = net_150.layers['edge_rf_fc']\n",
        "    edge_out2_175 = net_175.layers['edge_rf_fc']\n",
        "\n",
        "\n",
        "    # combine resize\n",
        "    parsing_out1 = tf.reduce_mean(tf.stack([tf.image.resize(parsing_out1_050, tf.shape(image_batch)[1:3,]),\n",
        "                                            tf.image.resize(parsing_out1_075, tf.shape(image_batch)[1:3,]),\n",
        "                                            tf.image.resize(parsing_out1_100, tf.shape(image_batch)[1:3,]),\n",
        "                                            tf.image.resize(parsing_out1_125, tf.shape(image_batch)[1:3,]),\n",
        "                                            tf.image.resize(parsing_out1_150, tf.shape(image_batch)[1:3,]),\n",
        "                                            tf.image.resize(parsing_out1_175, tf.shape(image_batch)[1:3,])]), axis=0)\n",
        "\n",
        "    parsing_out2 = tf.reduce_mean(tf.stack([tf.image.resize(parsing_out2_050, tf.shape(image_batch)[1:3,]),\n",
        "                                            tf.image.resize(parsing_out2_075, tf.shape(image_batch)[1:3,]),\n",
        "                                            tf.image.resize(parsing_out2_100, tf.shape(image_batch)[1:3,]),\n",
        "                                            tf.image.resize(parsing_out2_125, tf.shape(image_batch)[1:3,]),\n",
        "                                            tf.image.resize(parsing_out2_150, tf.shape(image_batch)[1:3,]),\n",
        "                                            tf.image.resize(parsing_out2_175, tf.shape(image_batch)[1:3,])]), axis=0)\n",
        "\n",
        "\n",
        "    edge_out2_100 = tf.image.resize(edge_out2_100, tf.shape(image_batch)[1:3,])\n",
        "    edge_out2_125 = tf.image.resize(edge_out2_125, tf.shape(image_batch)[1:3,])\n",
        "    edge_out2_150 = tf.image.resize(edge_out2_150, tf.shape(image_batch)[1:3,])\n",
        "    edge_out2_175 = tf.image.resize(edge_out2_175, tf.shape(image_batch)[1:3,])\n",
        "    edge_out2 = tf.reduce_mean(tf.stack([edge_out2_100, edge_out2_125, edge_out2_150, edge_out2_175]), axis=0)\n",
        "                                           \n",
        "    raw_output = tf.reduce_mean(tf.stack([parsing_out1, parsing_out2]), axis=0)\n",
        "    head_output, tail_output = tf.unstack(raw_output, num=2, axis=0)\n",
        "    tail_list = tf.unstack(tail_output, num=20, axis=2)\n",
        "    tail_list_rev = [None] * 20\n",
        "    for xx in range(14):\n",
        "        tail_list_rev[xx] = tail_list[xx]\n",
        "    tail_list_rev[14] = tail_list[15]\n",
        "    tail_list_rev[15] = tail_list[14]\n",
        "    tail_list_rev[16] = tail_list[17]\n",
        "    tail_list_rev[17] = tail_list[16]\n",
        "    tail_list_rev[18] = tail_list[19]\n",
        "    tail_list_rev[19] = tail_list[18]\n",
        "    tail_output_rev = tf.stack(tail_list_rev, axis=2)\n",
        "    tail_output_rev = tf.reverse(tail_output_rev, tf.stack([1]))\n",
        "    \n",
        "    raw_output_all = tf.reduce_mean(tf.stack([head_output, tail_output_rev]), axis=0)\n",
        "    raw_output_all = tf.expand_dims(raw_output_all, axis=0)\n",
        "    pred_scores = tf.reduce_max(raw_output_all, axis=3)\n",
        "    raw_output_all = tf.argmax(raw_output_all, axis=3)\n",
        "    pred_all = tf.expand_dims(raw_output_all, axis=3) # Create 4-d tensor.\n",
        "\n",
        "\n",
        "    raw_edge = tf.reduce_mean(tf.stack([edge_out2]), axis=0)\n",
        "    head_output, tail_output = tf.unstack(raw_edge, num=2, axis=0)\n",
        "    tail_output_rev = tf.reverse(tail_output, tf.stack([1]))\n",
        "    raw_edge_all = tf.reduce_mean(tf.stack([head_output, tail_output_rev]), axis=0)\n",
        "    raw_edge_all = tf.expand_dims(raw_edge_all, axis=0)\n",
        "    pred_edge = tf.sigmoid(raw_edge_all)\n",
        "    res_edge = tf.cast(tf.greater(pred_edge, 0.5), tf.int32)\n",
        "\n",
        "    # prepare ground truth \n",
        "    # preds = tf.reshape(pred_all, [-1,])\n",
        "    # weights = tf.cast(tf.less_equal(gt, N_CLASSES - 1), tf.int32) # Ignoring all labels greater than or equal to n_classes.\n",
        "    # mIoU, update_op_iou = tf.contrib.metrics.streaming_mean_iou(preds, gt, num_classes=N_CLASSES, weights=weights)\n",
        "    # macc, update_op_acc = tf.contrib.metrics.streaming_accuracy(preds, gt, weights=weights)\n",
        "    #\n",
        "    # # precision and recall\n",
        "    # recall, update_op_recall = tf.contrib.metrics.streaming_recall(res_edge, edge_gt_batch)\n",
        "    # precision, update_op_precision = tf.contrib.metrics.streaming_precision(res_edge, edge_gt_batch)\n",
        "\n",
        "    # update_op = tf.group(update_op_iou, update_op_acc, update_op_recall, update_op_precision)\n",
        "\n",
        "    # Which variables to load.\n",
        "    restore_var = tf.compat.v1.global_variables()\n",
        "    # Set up tf session and initialize variables. \n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    sess = tf.compat.v1.Session(config=config)\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "    sess.run(init)\n",
        "    sess.run(tf.compat.v1.local_variables_initializer())\n",
        "    \n",
        "    # Load weights.\n",
        "\n",
        "    # loader = tf.compat.v1.train.Saver(var_list=restore_var)\n",
        "    # if RESTORE_FROM is not None:\n",
        "    #     if load(loader, sess, RESTORE_FROM):\n",
        "    #         print(\" [*] Load SUCCESS\")\n",
        "    #     else:\n",
        "    #         print(\" [!] Load failed...\")\n",
        "    \n",
        "    loader = tf.compat.v1.train.Saver(var_list=restore_var)\n",
        "    if RESTORE_FROM is not None:\n",
        "        if load(loader, sess, RESTORE_FROM):\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "    # Start queue threads.\n",
        "    threads = tf.compat.v1.train.start_queue_runners(coord=coord, sess=sess)\n",
        "\n",
        "    # evaluate prosessing\n",
        "    parsing_dir = os.path.join(args.output, 'cihp_parsing_maps')\n",
        "    if not os.path.exists(parsing_dir):\n",
        "        os.makedirs(parsing_dir)\n",
        "    edge_dir = os.path.join(args.output, 'cihp_edge_maps')\n",
        "    if not os.path.exists(edge_dir):\n",
        "        os.makedirs(edge_dir)\n",
        "    # Iterate over training steps.\n",
        "    for step in range(NUM_STEPS):\n",
        "        # if step > 100:\n",
        "        #     break\n",
        "        print(step)\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "        parsing_, scores, edge_ = sess.run([pred_all, pred_scores, pred_edge])\n",
        "        if step % 1 == 0:\n",
        "            print('step {:d}'.format(step))\n",
        "            print (image_list[step])\n",
        "        img_split = image_list[step].split('/')\n",
        "        img_id = img_split[-1][:-4]\n",
        "        \n",
        "        msk = decode_labels(parsing_, num_classes=N_CLASSES)\n",
        "\n",
        "        parsing_im = Image.fromarray(msk[0])\n",
        "        # print(\"here\")\n",
        "        parsing_im.save('{}/{}_vis.png'.format(parsing_dir, img_id))\n",
        "        cv2.imwrite('{}/{}.png'.format(parsing_dir, img_id), parsing_[0,:,:,0])\n",
        "        # sio.savemat('{}/{}.mat'.format(parsing_dir, img_id), {'data': scores[0,:,:]})\n",
        "        \n",
        "        # cv2.imwrite('{}/{}.png'.format(edge_dir, img_id), edge_[0,:,:,0] * 255)\n",
        "\n",
        "        # Agnostic\n",
        "        msk = decode_labels(parsing_, num_classes=N_CLASSES, get_agn=True)\n",
        "\n",
        "        parsing_im = Image.fromarray(msk[0])\n",
        "        # print(\"here\")\n",
        "        parsing_im.save('{}/{}_agn_vis.png'.format(parsing_dir, img_id))\n",
        "        cv2.imwrite('{}/{}_agn.png'.format(parsing_dir, img_id), parsing_[0,:,:,0])\n",
        "        # sio.savemat('{}/{}.mat'.format(parsing_dir, img_id), {'data': scores[0,:,:]})\n",
        "        \n",
        "        # cv2.imwrite('{}/{}.png'.format(edge_dir, img_id), edge_[0,:,:,0] * 255)\n",
        "        print(\"here\")\n",
        "\n",
        "\n",
        "\n",
        "    # res_mIou = mIoU.eval(session=sess)\n",
        "    # res_macc = macc.eval(session=sess)\n",
        "    # res_recall = recall.eval(session=sess)\n",
        "    # res_precision = precision.eval(session=sess)\n",
        "    # f1 = 2 * res_precision * res_recall / (res_precision + res_recall)\n",
        "    # print('Mean IoU: {:.4f}, Mean Acc: {:.4f}'.format(res_mIou, res_macc))\n",
        "    # print('Recall: {:.4f}, Precision: {:.4f}, F1 score: {:.4f}'.format(res_recall, res_precision, f1))\n",
        "\n",
        "    coord.request_stop()\n",
        "    coord.join(threads)\n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "##############################################################333"
      ],
      "metadata": {
        "id": "FL3yaE6gufo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a932d986-c11e-4883-fd47-558db4107b00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/CIHP_PGN/inf_pgn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overwriting Utils"
      ],
      "metadata": {
        "id": "IDMD29aCuK11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/CIHP_PGN/utils/utils.py\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import scipy.misc\n",
        "from scipy.stats import multivariate_normal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_classes = 20\n",
        "# colour map\n",
        "label_colours = [(0,0,0)\n",
        "                , (128,0,0), (255,0,0), (0,85,0), (170,0,51), (255,85,0), (0,0,85), (0,119,221), (85,85,0), (0,85,85), (85,51,0), (52,86,128), (0,128,0)\n",
        "                , (0,0,255), (51,170,221), (0,255,255), (85,255,170), (170,255,85), (255,255,0), (255,170,0)]\n",
        "agnostics = [5, 6, 7, 10]\n",
        "# label_colours = [(0,0,0)\n",
        "#                 # 0=background\n",
        "#                 ,(128,0,0), (0,128,0), (128,128,0), (0,0,128), (128,0,128), (0,128,128)]\n",
        "#                 # 1=head, 2=torso, 3=upper arm, 4=lower arm, 5=upper leg, # 6=lower leg\n",
        "# image mean\n",
        "IMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\n",
        "    \n",
        "def decode_labels(mask, num_images=1, num_classes=21, get_agn=False):\n",
        "    \"\"\"Decode batch of segmentation masks.\n",
        "    \n",
        "    Args:\n",
        "      mask: result of inference after taking argmax.\n",
        "      num_images: number of images to decode from the batch.\n",
        "      num_classes: number of classes to predict (including background).\n",
        "    \n",
        "    Returns:\n",
        "      A batch with num_images RGB images of the same size as the input. \n",
        "    \"\"\"\n",
        "    n, h, w, c = mask.shape\n",
        "    assert(n >= num_images), 'Batch size %d should be greater or equal than number of images to save %d.' % (n, num_images)\n",
        "    outputs = np.zeros((num_images, h, w, 3), dtype=np.uint8)\n",
        "    for i in range(num_images):\n",
        "      img = Image.new('RGB', (len(mask[i, 0]), len(mask[i])))\n",
        "      pixels = img.load()\n",
        "      for j_, j in enumerate(mask[i, :, :, 0]):\n",
        "          for k_, k in enumerate(j):\n",
        "              if get_agn and k in agnostics:\n",
        "                  pixels[k_,j_] = label_colours[0]\n",
        "                  continue\n",
        "              if k < num_classes:\n",
        "                  pixels[k_,j_] = label_colours[k]\n",
        "      outputs[i] = np.array(img)\n",
        "    return outputs\n",
        "\n",
        "def prepare_label(input_batch, new_size, one_hot=True):\n",
        "    \"\"\"Resize masks and perform one-hot encoding.\n",
        "\n",
        "    Args:\n",
        "      input_batch: input tensor of shape [batch_size H W 1].\n",
        "      new_size: a tensor with new height and width.\n",
        "\n",
        "    Returns:\n",
        "      Outputs a tensor of shape [batch_size h w 21]\n",
        "      with last dimension comprised of 0's and 1's only.\n",
        "    \"\"\"\n",
        "    with tf.name_scope('label_encode'):\n",
        "        input_batch = tf.image.resize_nearest_neighbor(input_batch, new_size) # as labels are integer numbers, need to use NN interp.\n",
        "        input_batch = tf.squeeze(input_batch, squeeze_dims=[3]) # reducing the channel dimension.\n",
        "        if one_hot:\n",
        "          input_batch = tf.one_hot(input_batch, depth=n_classes)\n",
        "    return input_batch\n",
        "\n",
        "def inv_preprocess(imgs, num_images):\n",
        "  \"\"\"Inverse preprocessing of the batch of images.\n",
        "     Add the mean vector and convert from BGR to RGB.\n",
        "   \n",
        "  Args:\n",
        "    imgs: batch of input images.\n",
        "    num_images: number of images to apply the inverse transformations on.\n",
        "  \n",
        "  Returns:\n",
        "    The batch of the size num_images with the same spatial dimensions as the input.\n",
        "  \"\"\"\n",
        "  n, h, w, c = imgs.shape\n",
        "  assert(n >= num_images), 'Batch size %d should be greater or equal than number of images to save %d.' % (n, num_images)\n",
        "  outputs = np.zeros((num_images, h, w, c), dtype=np.uint8)\n",
        "  for i in range(num_images):\n",
        "    outputs[i] = (imgs[i] + IMG_MEAN)[:, :, ::-1].astype(np.uint8)\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def save(saver, sess, logdir, step):\n",
        "    '''Save weights.   \n",
        "    Args:\n",
        "     saver: TensorFlow Saver object.\n",
        "     sess: TensorFlow session.\n",
        "     logdir: path to the snapshots directory.\n",
        "     step: current training step.\n",
        "    '''\n",
        "    if not os.path.exists(logdir):\n",
        "        os.makedirs(logdir)   \n",
        "    model_name = 'model.ckpt'\n",
        "    checkpoint_path = os.path.join(logdir, model_name)\n",
        "      \n",
        "    if not os.path.exists(logdir):\n",
        "      os.makedirs(logdir)\n",
        "    saver.save(sess, checkpoint_path, global_step=step)\n",
        "    print('The checkpoint has been created.')\n",
        "\n",
        "def load(saver, sess, ckpt_path):\n",
        "    '''Load trained weights.\n",
        "    \n",
        "    Args:\n",
        "      saver: TensorFlow saver object.\n",
        "      sess: TensorFlow session.\n",
        "      ckpt_path: path to checkpoint file with parameters.\n",
        "    ''' \n",
        "    ckpt = tf.train.get_checkpoint_state(ckpt_path)\n",
        "    if ckpt and ckpt.model_checkpoint_path:\n",
        "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "        saver.restore(sess, os.path.join(ckpt_path, ckpt_name))\n",
        "        print(\"Restored model parameters from {}\".format(ckpt_name))\n",
        "        return True\n",
        "    else:\n",
        "        return False  \n"
      ],
      "metadata": {
        "id": "8g8pZWsYuOOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7966ebc-db01-4152-baa1-3c9264eb9b32"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/CIHP_PGN/utils/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Densepose"
      ],
      "metadata": {
        "id": "BXBl_KAWtrP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install \"git+https://github.com/facebookresearch/detectron2@main#subdirectory=projects/DensePose\""
      ],
      "metadata": {
        "id": "deqPr_oBujsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d2d4eb-93ab-41f6-bb8f-c5e1aea7e474"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp39-cp39-linux_x86_64.whl size=44089 sha256=a11ca6b06a599c96e42e6f0a974e1994f96d039bf5baa48500e837d8dc24b45a\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/be/8f/b6c454cd264e0b349b47f8ee00755511f277618af9e5dae20d\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flax 0.6.8 requires PyYAML>=5.4.1, but you have pyyaml 5.1 which is incompatible.\n",
            "dask 2022.12.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyyaml-5.1\n",
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 14960, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 14960 (delta 19), reused 24 (delta 10), pack-reused 14918\u001b[K\n",
            "Receiving objects: 100% (14960/14960), 6.08 MiB | 19.45 MiB/s, done.\n",
            "Resolving deltas: 100% (10836/10836), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.9/dist-packages (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.9/dist-packages (2.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.9/dist-packages (2.2.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (0.8.10)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.9/dist-packages (4.65.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (2.12.1)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black\n",
            "  Downloading black-23.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (23.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from yacs>=0.1.8) (5.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (67.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (2.2.3)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.53.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (2.17.1)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (0.40.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (0.7.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pathspec>=0.9.0\n",
            "  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.9/dist-packages (from black) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from black) (4.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from black) (8.1.3)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard) (6.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61429 sha256=274ff1873e44531a45dab6a1e1e2b3a813620f4223f77fe295466174bd92d636\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/42/02/66178d16e5c44dc26d309931834956baeda371956e86fbd876\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=97316035fcf6d8b08d7ae6ea4c92e58c04f32cb3f7eab3dcd12f549ac53647fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-23.3.0 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.11.1 portalocker-2.7.0 yacs-0.1.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/detectron2@main#subdirectory=projects/DensePose\n",
            "  Cloning https://github.com/facebookresearch/detectron2 (to revision main) to /tmp/pip-req-build-71_o3gub\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2 /tmp/pip-req-build-71_o3gub\n",
            "  Resolved https://github.com/facebookresearch/detectron2 to commit 1bc3a33a71991142c2c67bc99e1559d6101fb009\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting detectron2@ git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-install-azttt0yw/detectron2_aa24dd3676904f2b8ef2488dca64ff76\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-install-azttt0yw/detectron2_aa24dd3676904f2b8ef2488dca64ff76\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 1bc3a33a71991142c2c67bc99e1559d6101fb009\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av>=8.0.3\n",
            "  Downloading av-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless>=4.5.3.56 in /usr/local/lib/python3.9/dist-packages (from detectron2-densepose==0.6) (4.7.0.72)\n",
            "Requirement already satisfied: scipy>=1.5.4 in /usr/local/lib/python3.9/dist-packages (from detectron2-densepose==0.6) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.9/dist-packages (from opencv-python-headless>=4.5.3.56->detectron2-densepose==0.6) (1.22.4)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.2.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (0.8.10)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (4.65.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.12.1)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (23.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (5.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.9/dist-packages (from hydra-core>=1.1->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.9/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.7.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (4.39.3)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (5.12.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.9/dist-packages (from black->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.0.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from black->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (8.1.3)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from black->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (4.5.0)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.9/dist-packages (from black->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (3.2.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from black->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (0.11.1)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from black->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.27.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.17.1)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.53.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (0.7.0)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (67.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.3.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (3.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (6.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git->detectron2-densepose==0.6) (3.2.2)\n",
            "Building wheels for collected packages: detectron2-densepose, detectron2\n",
            "  Building wheel for detectron2-densepose (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2-densepose: filename=detectron2_densepose-0.6-py3-none-any.whl size=176112 sha256=7a2327844b8b302c795a03731a2552be1e150bc3c02b072d4d2a4a42d4f18b09\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kl44vll1/wheels/42/29/0b/a78e1ba5aaf3869a3097fd056ebf8cf6644b4308752fec9350\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp39-cp39-linux_x86_64.whl size=7802195 sha256=5fab95b833d483f1bfcb75627f09f31d4108b2c90104d4405e51eb8ad0dec89a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kl44vll1/wheels/59/b4/83/84bfca751fa4dcc59998468be8688eb50e97408a83af171d42\n",
            "Successfully built detectron2-densepose detectron2\n",
            "Installing collected packages: av, detectron2, detectron2-densepose\n",
            "Successfully installed av-10.0.0 detectron2-0.6 detectron2-densepose-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "metadata": {
        "id": "K2b2yjkWupxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4959ab-1eff-4813-f157-534043370da4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "torch:  2.0 ; cuda:  cu118\n",
            "detectron2: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overwriting base.py"
      ],
      "metadata": {
        "id": "aZzCkKo-utwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/detectron2/projects/DensePose/densepose/vis/base.py\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "import logging\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "Image = np.ndarray\n",
        "Boxes = torch.Tensor\n",
        "\n",
        "\n",
        "class MatrixVisualizer(object):\n",
        "    \"\"\"\n",
        "    Base visualizer for matrix data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplace=True,\n",
        "        cmap=cv2.COLORMAP_PARULA,\n",
        "        val_scale=1.0,\n",
        "        alpha=0.7,\n",
        "        interp_method_matrix=cv2.INTER_LINEAR,\n",
        "        interp_method_mask=cv2.INTER_NEAREST,\n",
        "    ):\n",
        "        self.inplace = inplace\n",
        "        self.cmap = cmap\n",
        "        self.val_scale = val_scale\n",
        "        self.alpha = alpha\n",
        "        self.interp_method_matrix = interp_method_matrix\n",
        "        self.interp_method_mask = interp_method_mask\n",
        "\n",
        "    def visualize(self, image_bgr, mask, matrix, bbox_xywh):\n",
        "        self._check_image(image_bgr)\n",
        "        self._check_mask_matrix(mask, matrix)\n",
        "        if self.inplace:\n",
        "            image_target_bgr = image_bgr\n",
        "        else:\n",
        "            image_target_bgr = image_bgr * 0\n",
        "        x, y, w, h = [int(v) for v in bbox_xywh]\n",
        "        if w <= 0 or h <= 0:\n",
        "            return image_bgr\n",
        "        mask, matrix = self._resize(mask, matrix, w, h)\n",
        "        mask_bg = np.tile((mask == 0)[:, :, np.newaxis], [1, 1, 3])\n",
        "        matrix_scaled = matrix.astype(np.float32) * self.val_scale\n",
        "        _EPSILON = 1e-6\n",
        "        if np.any(matrix_scaled > 255 + _EPSILON):\n",
        "            logger = logging.getLogger(__name__)\n",
        "            logger.warning(\n",
        "                f\"Matrix has values > {255 + _EPSILON} after \" f\"scaling, clipping to [0..255]\"\n",
        "            )\n",
        "        matrix_scaled_8u = matrix_scaled.clip(0, 255).astype(np.uint8)\n",
        "        matrix_vis = cv2.applyColorMap(matrix_scaled_8u, self.cmap)\n",
        "        black = np.zeros(matrix_vis.shape)\n",
        "        black_bgr = np.zeros(image_target_bgr.shape)\n",
        "        matrix_vis[mask_bg] = black[mask_bg]#image_target_bgr[y : y + h, x : x + w, :][mask_bg]\n",
        "        cv2.imwrite(\"densepose_result.jpg\",matrix_vis)\n",
        "        image_target_bgr[y : y + h, x : x + w, :] = (\n",
        "            matrix_vis\n",
        "        )\n",
        "        image_target_bgr[:y, :, :] = black_bgr[:y, : , :]\n",
        "        image_target_bgr[y+h:, :, :] = black_bgr[y+h, : , :]\n",
        "        image_target_bgr[:, :x, :] = black_bgr[:, :x , :]\n",
        "        image_target_bgr[:, x+w:, :] = black_bgr[:, x+w:, :]\n",
        "        cv2.imwrite(\"densepose_result.jpg\",image_target_bgr)\n",
        "        return image_target_bgr.astype(np.uint8)\n",
        "\n",
        "    def _resize(self, mask, matrix, w, h):\n",
        "        if (w != mask.shape[1]) or (h != mask.shape[0]):\n",
        "            mask = cv2.resize(mask, (w, h), self.interp_method_mask)\n",
        "        if (w != matrix.shape[1]) or (h != matrix.shape[0]):\n",
        "            matrix = cv2.resize(matrix, (w, h), self.interp_method_matrix)\n",
        "        return mask, matrix\n",
        "\n",
        "    def _check_image(self, image_rgb):\n",
        "        assert len(image_rgb.shape) == 3\n",
        "        assert image_rgb.shape[2] == 3\n",
        "        assert image_rgb.dtype == np.uint8\n",
        "\n",
        "    def _check_mask_matrix(self, mask, matrix):\n",
        "        assert len(matrix.shape) == 2\n",
        "        assert len(mask.shape) == 2\n",
        "        assert mask.dtype == np.uint8\n",
        "\n",
        "\n",
        "class RectangleVisualizer(object):\n",
        "\n",
        "    _COLOR_GREEN = (18, 127, 15)\n",
        "\n",
        "    def __init__(self, color=_COLOR_GREEN, thickness=1):\n",
        "        self.color = color\n",
        "        self.thickness = thickness\n",
        "\n",
        "    def visualize(self, image_bgr, bbox_xywh, color=None, thickness=None):\n",
        "        x, y, w, h = bbox_xywh\n",
        "        color = color or self.color\n",
        "        thickness = thickness or self.thickness\n",
        "        cv2.rectangle(image_bgr, (int(x), int(y)), (int(x + w), int(y + h)), color, thickness)\n",
        "        return image_bgr\n",
        "\n",
        "\n",
        "class PointsVisualizer(object):\n",
        "\n",
        "    _COLOR_GREEN = (18, 127, 15)\n",
        "\n",
        "    def __init__(self, color_bgr=_COLOR_GREEN, r=5):\n",
        "        self.color_bgr = color_bgr\n",
        "        self.r = r\n",
        "\n",
        "    def visualize(self, image_bgr, pts_xy, colors_bgr=None, rs=None):\n",
        "        for j, pt_xy in enumerate(pts_xy):\n",
        "            x, y = pt_xy\n",
        "            color_bgr = colors_bgr[j] if colors_bgr is not None else self.color_bgr\n",
        "            r = rs[j] if rs is not None else self.r\n",
        "            cv2.circle(image_bgr, (x, y), r, color_bgr, -1)\n",
        "        return image_bgr\n",
        "\n",
        "\n",
        "class TextVisualizer(object):\n",
        "\n",
        "    _COLOR_GRAY = (218, 227, 218)\n",
        "    _COLOR_WHITE = (255, 255, 255)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        font_face=cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        font_color_bgr=_COLOR_GRAY,\n",
        "        font_scale=0.35,\n",
        "        font_line_type=cv2.LINE_AA,\n",
        "        font_line_thickness=1,\n",
        "        fill_color_bgr=_COLOR_WHITE,\n",
        "        fill_color_transparency=1.0,\n",
        "        frame_color_bgr=_COLOR_WHITE,\n",
        "        frame_color_transparency=1.0,\n",
        "        frame_thickness=1,\n",
        "    ):\n",
        "        self.font_face = font_face\n",
        "        self.font_color_bgr = font_color_bgr\n",
        "        self.font_scale = font_scale\n",
        "        self.font_line_type = font_line_type\n",
        "        self.font_line_thickness = font_line_thickness\n",
        "        self.fill_color_bgr = fill_color_bgr\n",
        "        self.fill_color_transparency = fill_color_transparency\n",
        "        self.frame_color_bgr = frame_color_bgr\n",
        "        self.frame_color_transparency = frame_color_transparency\n",
        "        self.frame_thickness = frame_thickness\n",
        "\n",
        "    def visualize(self, image_bgr, txt, topleft_xy):\n",
        "        txt_w, txt_h = self.get_text_size_wh(txt)\n",
        "        topleft_xy = tuple(map(int, topleft_xy))\n",
        "        x, y = topleft_xy\n",
        "        if self.frame_color_transparency < 1.0:\n",
        "            t = self.frame_thickness\n",
        "            image_bgr[y - t : y + txt_h + t, x - t : x + txt_w + t, :] = (\n",
        "                image_bgr[y - t : y + txt_h + t, x - t : x + txt_w + t, :]\n",
        "                * self.frame_color_transparency\n",
        "                + np.array(self.frame_color_bgr) * (1.0 - self.frame_color_transparency)\n",
        "            ).astype(np.float)\n",
        "        if self.fill_color_transparency < 1.0:\n",
        "            image_bgr[y : y + txt_h, x : x + txt_w, :] = (\n",
        "                image_bgr[y : y + txt_h, x : x + txt_w, :] * self.fill_color_transparency\n",
        "                + np.array(self.fill_color_bgr) * (1.0 - self.fill_color_transparency)\n",
        "            ).astype(np.float)\n",
        "        cv2.putText(\n",
        "            image_bgr,\n",
        "            txt,\n",
        "            topleft_xy,\n",
        "            self.font_face,\n",
        "            self.font_scale,\n",
        "            self.font_color_bgr,\n",
        "            self.font_line_thickness,\n",
        "            self.font_line_type,\n",
        "        )\n",
        "        return image_bgr\n",
        "\n",
        "    def get_text_size_wh(self, txt):\n",
        "        ((txt_w, txt_h), _) = cv2.getTextSize(\n",
        "            txt, self.font_face, self.font_scale, self.font_line_thickness\n",
        "        )\n",
        "        return txt_w, txt_h\n",
        "\n",
        "\n",
        "class CompoundVisualizer(object):\n",
        "    def __init__(self, visualizers):\n",
        "        self.visualizers = visualizers\n",
        "\n",
        "    def visualize(self, image_bgr, data):\n",
        "        assert len(data) == len(\n",
        "            self.visualizers\n",
        "        ), \"The number of datas {} should match the number of visualizers\" \" {}\".format(\n",
        "            len(data), len(self.visualizers)\n",
        "        )\n",
        "        image = image_bgr\n",
        "        for i, visualizer in enumerate(self.visualizers):\n",
        "            image = visualizer.visualize(image, data[i])\n",
        "        return image\n",
        "\n",
        "    def __str__(self):\n",
        "        visualizer_str = \", \".join([str(v) for v in self.visualizers])\n",
        "        return \"Compound Visualizer [{}]\".format(visualizer_str)\n"
      ],
      "metadata": {
        "id": "uQdBXr6autHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1546da-4ae0-423f-fbc2-204839ccf0bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/detectron2/projects/DensePose/densepose/vis/base.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overwriting densepose_result.py"
      ],
      "metadata": {
        "id": "dAlXSAURuyCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/detectron2/projects/DensePose/densepose/vis/densepose_results.py\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "import logging\n",
        "import numpy as np\n",
        "from typing import List, Optional, Tuple\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "from densepose.structures import DensePoseDataRelative\n",
        "\n",
        "from ..structures import DensePoseChartResult\n",
        "from .base import Boxes, Image, MatrixVisualizer\n",
        "\n",
        "\n",
        "class DensePoseResultsVisualizer(object):\n",
        "    def visualize(\n",
        "        self,\n",
        "        image_bgr: Image,\n",
        "        results_and_boxes_xywh: Tuple[Optional[List[DensePoseChartResult]], Optional[Boxes]],\n",
        "    ) -> Image:\n",
        "        densepose_result, boxes_xywh = results_and_boxes_xywh\n",
        "        #print(densepose_result)\n",
        "        #cv2.imwrite(\"result.jpg\", densepose_result)\n",
        "        if densepose_result is None or boxes_xywh is None:\n",
        "            return image_bgr\n",
        "        boxes_xywh = boxes_xywh.cpu().numpy()\n",
        "        context = self.create_visualization_context(image_bgr)\n",
        "        for i, result in enumerate(densepose_result):\n",
        "            iuv_array = torch.cat(\n",
        "                (result.labels[None].type(torch.float32), result.uv * 255.0)\n",
        "            ).type(torch.uint8)\n",
        "            self.visualize_iuv_arr(context, iuv_array.cpu().numpy(), boxes_xywh[i])\n",
        "        image_bgr = self.context_to_image_bgr(context)\n",
        "        return image_bgr\n",
        "\n",
        "    def create_visualization_context(self, image_bgr: Image):\n",
        "        return image_bgr\n",
        "\n",
        "    def visualize_iuv_arr(self, context, iuv_arr: np.ndarray, bbox_xywh) -> None:\n",
        "        pass\n",
        "\n",
        "    def context_to_image_bgr(self, context):\n",
        "        return context\n",
        "\n",
        "    def get_image_bgr_from_context(self, context):\n",
        "        return context\n",
        "\n",
        "\n",
        "class DensePoseMaskedColormapResultsVisualizer(DensePoseResultsVisualizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_extractor,\n",
        "        segm_extractor,\n",
        "        inplace=False,\n",
        "        cmap=cv2.COLORMAP_PARULA,\n",
        "        alpha=0.7,\n",
        "        val_scale=1.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.mask_visualizer = MatrixVisualizer(\n",
        "            inplace=inplace, cmap=cmap, val_scale=val_scale, alpha=alpha\n",
        "        )\n",
        "        self.data_extractor = data_extractor\n",
        "        self.segm_extractor = segm_extractor\n",
        "\n",
        "    def context_to_image_bgr(self, context):\n",
        "        return context\n",
        "\n",
        "    def visualize_iuv_arr(self, context, iuv_arr: np.ndarray, bbox_xywh) -> None:\n",
        "        image_bgr = self.get_image_bgr_from_context(context)\n",
        "        matrix = self.data_extractor(iuv_arr)\n",
        "        segm = self.segm_extractor(iuv_arr)\n",
        "        mask = np.zeros(matrix.shape, dtype=np.uint8)\n",
        "        mask[segm > 0] = 1\n",
        "        image_bgr = self.mask_visualizer.visualize(image_bgr, mask, matrix, bbox_xywh)\n",
        "\n",
        "\n",
        "def _extract_i_from_iuvarr(iuv_arr):\n",
        "    return iuv_arr[0, :, :]\n",
        "\n",
        "\n",
        "def _extract_u_from_iuvarr(iuv_arr):\n",
        "    return iuv_arr[1, :, :]\n",
        "\n",
        "\n",
        "def _extract_v_from_iuvarr(iuv_arr):\n",
        "    return iuv_arr[2, :, :]\n",
        "\n",
        "\n",
        "class DensePoseResultsMplContourVisualizer(DensePoseResultsVisualizer):\n",
        "    def __init__(self, levels=10, **kwargs):\n",
        "        self.levels = levels\n",
        "        self.plot_args = kwargs\n",
        "\n",
        "    def create_visualization_context(self, image_bgr: Image):\n",
        "        import matplotlib.pyplot as plt\n",
        "        from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "        context = {}\n",
        "        context[\"image_bgr\"] = image_bgr\n",
        "        dpi = 100\n",
        "        height_inches = float(image_bgr.shape[0]) / dpi\n",
        "        width_inches = float(image_bgr.shape[1]) / dpi\n",
        "        fig = plt.figure(figsize=(width_inches, height_inches), dpi=dpi)\n",
        "        plt.axes([0, 0, 1, 1])\n",
        "        plt.axis(\"off\")\n",
        "        context[\"fig\"] = fig\n",
        "        canvas = FigureCanvas(fig)\n",
        "        context[\"canvas\"] = canvas\n",
        "        extent = (0, image_bgr.shape[1], image_bgr.shape[0], 0)\n",
        "        plt.imshow(image_bgr[:, :, ::-1], extent=extent)\n",
        "        return context\n",
        "\n",
        "    def context_to_image_bgr(self, context):\n",
        "        fig = context[\"fig\"]\n",
        "        w, h = map(int, fig.get_size_inches() * fig.get_dpi())\n",
        "        canvas = context[\"canvas\"]\n",
        "        canvas.draw()\n",
        "        image_1d = np.fromstring(canvas.tostring_rgb(), dtype=\"uint8\")\n",
        "        image_rgb = image_1d.reshape(h, w, 3)\n",
        "        image_bgr = image_rgb[:, :, ::-1].copy()\n",
        "        return image_bgr\n",
        "\n",
        "    def visualize_iuv_arr(self, context, iuv_arr: np.ndarray, bbox_xywh: Boxes) -> None:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        u = _extract_u_from_iuvarr(iuv_arr).astype(float) / 255.0\n",
        "        v = _extract_v_from_iuvarr(iuv_arr).astype(float) / 255.0\n",
        "        extent = (\n",
        "            bbox_xywh[0],\n",
        "            bbox_xywh[0] + bbox_xywh[2],\n",
        "            bbox_xywh[1],\n",
        "            bbox_xywh[1] + bbox_xywh[3],\n",
        "        )\n",
        "        plt.contour(u, self.levels, extent=extent, **self.plot_args)\n",
        "        plt.contour(v, self.levels, extent=extent, **self.plot_args)\n",
        "\n",
        "\n",
        "class DensePoseResultsCustomContourVisualizer(DensePoseResultsVisualizer):\n",
        "    \"\"\"\n",
        "    Contour visualization using marching squares\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, levels=10, **kwargs):\n",
        "        # TODO: colormap is hardcoded\n",
        "        cmap = cv2.COLORMAP_PARULA\n",
        "        if isinstance(levels, int):\n",
        "            self.levels = np.linspace(0, 1, levels)\n",
        "        else:\n",
        "            self.levels = levels\n",
        "        if \"linewidths\" in kwargs:\n",
        "            self.linewidths = kwargs[\"linewidths\"]\n",
        "        else:\n",
        "            self.linewidths = [1] * len(self.levels)\n",
        "        self.plot_args = kwargs\n",
        "        img_colors_bgr = cv2.applyColorMap((self.levels * 255).astype(np.uint8), cmap)\n",
        "        self.level_colors_bgr = [\n",
        "            [int(v) for v in img_color_bgr.ravel()] for img_color_bgr in img_colors_bgr\n",
        "        ]\n",
        "\n",
        "    def visualize_iuv_arr(self, context, iuv_arr: np.ndarray, bbox_xywh: Boxes) -> None:\n",
        "        image_bgr = self.get_image_bgr_from_context(context)\n",
        "        segm = _extract_i_from_iuvarr(iuv_arr)\n",
        "        u = _extract_u_from_iuvarr(iuv_arr).astype(float) / 255.0\n",
        "        v = _extract_v_from_iuvarr(iuv_arr).astype(float) / 255.0\n",
        "        self._contours(image_bgr, u, segm, bbox_xywh)\n",
        "        self._contours(image_bgr, v, segm, bbox_xywh)\n",
        "\n",
        "    def _contours(self, image_bgr, arr, segm, bbox_xywh):\n",
        "        for part_idx in range(1, DensePoseDataRelative.N_PART_LABELS + 1):\n",
        "            mask = segm == part_idx\n",
        "            if not np.any(mask):\n",
        "                continue\n",
        "            arr_min = np.amin(arr[mask])\n",
        "            arr_max = np.amax(arr[mask])\n",
        "            I, J = np.nonzero(mask)\n",
        "            i0 = np.amin(I)\n",
        "            i1 = np.amax(I) + 1\n",
        "            j0 = np.amin(J)\n",
        "            j1 = np.amax(J) + 1\n",
        "            if (j1 == j0 + 1) or (i1 == i0 + 1):\n",
        "                continue\n",
        "            Nw = arr.shape[1] - 1\n",
        "            Nh = arr.shape[0] - 1\n",
        "            for level_idx, level in enumerate(self.levels):\n",
        "                if (level < arr_min) or (level > arr_max):\n",
        "                    continue\n",
        "                vp = arr[i0:i1, j0:j1] >= level\n",
        "                bin_codes = vp[:-1, :-1] + vp[1:, :-1] * 2 + vp[1:, 1:] * 4 + vp[:-1, 1:] * 8\n",
        "                mp = mask[i0:i1, j0:j1]\n",
        "                bin_mask_codes = mp[:-1, :-1] + mp[1:, :-1] * 2 + mp[1:, 1:] * 4 + mp[:-1, 1:] * 8\n",
        "                it = np.nditer(bin_codes, flags=[\"multi_index\"])\n",
        "                color_bgr = self.level_colors_bgr[level_idx]\n",
        "                linewidth = self.linewidths[level_idx]\n",
        "                while not it.finished:\n",
        "                    if (it[0] != 0) and (it[0] != 15):\n",
        "                        i, j = it.multi_index\n",
        "                        if bin_mask_codes[i, j] != 0:\n",
        "                            self._draw_line(\n",
        "                                image_bgr,\n",
        "                                arr,\n",
        "                                mask,\n",
        "                                level,\n",
        "                                color_bgr,\n",
        "                                linewidth,\n",
        "                                it[0],\n",
        "                                it.multi_index,\n",
        "                                bbox_xywh,\n",
        "                                Nw,\n",
        "                                Nh,\n",
        "                                (i0, j0),\n",
        "                            )\n",
        "                    it.iternext()\n",
        "\n",
        "    def _draw_line(\n",
        "        self,\n",
        "        image_bgr,\n",
        "        arr,\n",
        "        mask,\n",
        "        v,\n",
        "        color_bgr,\n",
        "        linewidth,\n",
        "        bin_code,\n",
        "        multi_idx,\n",
        "        bbox_xywh,\n",
        "        Nw,\n",
        "        Nh,\n",
        "        offset,\n",
        "    ):\n",
        "        lines = self._bin_code_2_lines(arr, v, bin_code, multi_idx, Nw, Nh, offset)\n",
        "        x0, y0, w, h = bbox_xywh\n",
        "        x1 = x0 + w\n",
        "        y1 = y0 + h\n",
        "        for line in lines:\n",
        "            x0r, y0r = line[0]\n",
        "            x1r, y1r = line[1]\n",
        "            pt0 = (int(x0 + x0r * (x1 - x0)), int(y0 + y0r * (y1 - y0)))\n",
        "            pt1 = (int(x0 + x1r * (x1 - x0)), int(y0 + y1r * (y1 - y0)))\n",
        "            cv2.line(image_bgr, pt0, pt1, color_bgr, linewidth)\n",
        "\n",
        "    def _bin_code_2_lines(self, arr, v, bin_code, multi_idx, Nw, Nh, offset):\n",
        "        i0, j0 = offset\n",
        "        i, j = multi_idx\n",
        "        i += i0\n",
        "        j += j0\n",
        "        v0, v1, v2, v3 = arr[i, j], arr[i + 1, j], arr[i + 1, j + 1], arr[i, j + 1]\n",
        "        x0i = float(j) / Nw\n",
        "        y0j = float(i) / Nh\n",
        "        He = 1.0 / Nh\n",
        "        We = 1.0 / Nw\n",
        "        if (bin_code == 1) or (bin_code == 14):\n",
        "            a = (v - v0) / (v1 - v0)\n",
        "            b = (v - v0) / (v3 - v0)\n",
        "            pt1 = (x0i, y0j + a * He)\n",
        "            pt2 = (x0i + b * We, y0j)\n",
        "            return [(pt1, pt2)]\n",
        "        elif (bin_code == 2) or (bin_code == 13):\n",
        "            a = (v - v0) / (v1 - v0)\n",
        "            b = (v - v1) / (v2 - v1)\n",
        "            pt1 = (x0i, y0j + a * He)\n",
        "            pt2 = (x0i + b * We, y0j + He)\n",
        "            return [(pt1, pt2)]\n",
        "        elif (bin_code == 3) or (bin_code == 12):\n",
        "            a = (v - v0) / (v3 - v0)\n",
        "            b = (v - v1) / (v2 - v1)\n",
        "            pt1 = (x0i + a * We, y0j)\n",
        "            pt2 = (x0i + b * We, y0j + He)\n",
        "            return [(pt1, pt2)]\n",
        "        elif (bin_code == 4) or (bin_code == 11):\n",
        "            a = (v - v1) / (v2 - v1)\n",
        "            b = (v - v3) / (v2 - v3)\n",
        "            pt1 = (x0i + a * We, y0j + He)\n",
        "            pt2 = (x0i + We, y0j + b * He)\n",
        "            return [(pt1, pt2)]\n",
        "        elif (bin_code == 6) or (bin_code == 9):\n",
        "            a = (v - v0) / (v1 - v0)\n",
        "            b = (v - v3) / (v2 - v3)\n",
        "            pt1 = (x0i, y0j + a * He)\n",
        "            pt2 = (x0i + We, y0j + b * He)\n",
        "            return [(pt1, pt2)]\n",
        "        elif (bin_code == 7) or (bin_code == 8):\n",
        "            a = (v - v0) / (v3 - v0)\n",
        "            b = (v - v3) / (v2 - v3)\n",
        "            pt1 = (x0i + a * We, y0j)\n",
        "            pt2 = (x0i + We, y0j + b * He)\n",
        "            return [(pt1, pt2)]\n",
        "        elif bin_code == 5:\n",
        "            a1 = (v - v0) / (v1 - v0)\n",
        "            b1 = (v - v1) / (v2 - v1)\n",
        "            pt11 = (x0i, y0j + a1 * He)\n",
        "            pt12 = (x0i + b1 * We, y0j + He)\n",
        "            a2 = (v - v0) / (v3 - v0)\n",
        "            b2 = (v - v3) / (v2 - v3)\n",
        "            pt21 = (x0i + a2 * We, y0j)\n",
        "            pt22 = (x0i + We, y0j + b2 * He)\n",
        "            return [(pt11, pt12), (pt21, pt22)]\n",
        "        elif bin_code == 10:\n",
        "            a1 = (v - v0) / (v3 - v0)\n",
        "            b1 = (v - v0) / (v1 - v0)\n",
        "            pt11 = (x0i + a1 * We, y0j)\n",
        "            pt12 = (x0i, y0j + b1 * He)\n",
        "            a2 = (v - v1) / (v2 - v1)\n",
        "            b2 = (v - v3) / (v2 - v3)\n",
        "            pt21 = (x0i + a2 * We, y0j + He)\n",
        "            pt22 = (x0i + We, y0j + b2 * He)\n",
        "            return [(pt11, pt12), (pt21, pt22)]\n",
        "        return []\n",
        "\n",
        "\n",
        "try:\n",
        "    import matplotlib\n",
        "\n",
        "    matplotlib.use(\"Agg\")\n",
        "    DensePoseResultsContourVisualizer = DensePoseResultsMplContourVisualizer\n",
        "except ModuleNotFoundError:\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.warning(\"Could not import matplotlib, using custom contour visualizer\")\n",
        "    DensePoseResultsContourVisualizer = DensePoseResultsCustomContourVisualizer\n",
        "\n",
        "\n",
        "class DensePoseResultsFineSegmentationVisualizer(DensePoseMaskedColormapResultsVisualizer):\n",
        "    def __init__(self, inplace=True, cmap=cv2.COLORMAP_PARULA, alpha=0.7, **kwargs):\n",
        "        super(DensePoseResultsFineSegmentationVisualizer, self).__init__(\n",
        "            _extract_i_from_iuvarr,\n",
        "            _extract_i_from_iuvarr,\n",
        "            inplace,\n",
        "            cmap,\n",
        "            alpha,\n",
        "            val_scale=255.0 / DensePoseDataRelative.N_PART_LABELS,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "\n",
        "class DensePoseResultsUVisualizer(DensePoseMaskedColormapResultsVisualizer):\n",
        "    def __init__(self, inplace=True, cmap=cv2.COLORMAP_PARULA, alpha=0.7, **kwargs):\n",
        "        super(DensePoseResultsUVisualizer, self).__init__(\n",
        "            _extract_u_from_iuvarr,\n",
        "            _extract_i_from_iuvarr,\n",
        "            inplace,\n",
        "            cmap,\n",
        "            alpha,\n",
        "            val_scale=1.0,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "\n",
        "class DensePoseResultsVVisualizer(DensePoseMaskedColormapResultsVisualizer):\n",
        "    def __init__(self, inplace=True, cmap=cv2.COLORMAP_PARULA, alpha=0.7, **kwargs):\n",
        "        super(DensePoseResultsVVisualizer, self).__init__(\n",
        "            _extract_v_from_iuvarr,\n",
        "            _extract_i_from_iuvarr,\n",
        "            inplace,\n",
        "            cmap,\n",
        "            alpha,\n",
        "            val_scale=1.0,\n",
        "            **kwargs,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "BFpId_FEu1b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e7f9ff-ff06-464f-a51a-874ee14a45cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/detectron2/projects/DensePose/densepose/vis/densepose_results.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloth mask"
      ],
      "metadata": {
        "id": "lUwA8fMyvAmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "!pip install rembg"
      ],
      "metadata": {
        "id": "jvuHj4bzvCNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d13b955-0fbd-4c96-ab3b-1c323fc19509"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rembg\n",
            "  Downloading rembg-2.0.32-py3-none-any.whl (14 kB)\n",
            "Collecting filetype>=1.2.0\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.9/dist-packages (from rembg) (8.1.3)\n",
            "Requirement already satisfied: opencv-python-headless>=4.6.0.66 in /usr/local/lib/python3.9/dist-packages (from rembg) (4.7.0.72)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from rembg) (4.65.0)\n",
            "Collecting uvicorn>=0.20.0\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m200.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asyncer>=0.0.2\n",
            "  Downloading asyncer-0.0.2-py3-none-any.whl (8.3 kB)\n",
            "Collecting pillow>=9.3.0\n",
            "  Downloading Pillow-9.5.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymatting>=1.1.8\n",
            "  Downloading PyMatting-1.1.8-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pooch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from rembg) (1.6.0)\n",
            "Collecting onnxruntime>=1.13.1\n",
            "  Downloading onnxruntime-1.14.1-cp39-cp39-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.9/dist-packages (from rembg) (0.19.3)\n",
            "Collecting numpy>=1.23.5\n",
            "  Downloading numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.9/dist-packages (from rembg) (1.10.1)\n",
            "Collecting watchdog>=2.1.9\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart>=0.0.5\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imagehash>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from rembg) (4.3.1)\n",
            "Collecting aiohttp>=3.8.1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.92.0\n",
            "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp>=3.8.1->rembg) (22.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp>=3.8.1->rembg) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from asyncer>=0.0.2->rembg) (3.6.2)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.9/dist-packages (from fastapi>=0.92.0->rembg) (1.10.7)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyWavelets in /usr/local/lib/python3.9/dist-packages (from imagehash>=4.3.1->rembg) (1.4.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.13.1->rembg) (1.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.13.1->rembg) (23.0)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.13.1->rembg) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.13.1->rembg) (23.3.3)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.6.0->rembg) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.6.0->rembg) (2.27.1)\n",
            "Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.9/dist-packages (from pymatting>=1.1.8->rembg) (0.56.4)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.19.3->rembg) (2023.3.21)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.19.3->rembg) (2.25.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.19.3->rembg) (3.0)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<4.0.0,>=3.4.0->asyncer>=0.0.2->rembg) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/dist-packages (from anyio<4.0.0,>=3.4.0->asyncer>=0.0.2->rembg) (3.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba!=0.49.0->pymatting>=1.1.8->rembg) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba!=0.49.0->pymatting>=1.1.8->rembg) (67.6.1)\n",
            "Collecting numpy>=1.23.5\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi>=0.92.0->rembg) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.6.0->rembg) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.6.0->rembg) (1.26.15)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->onnxruntime>=1.13.1->rembg) (1.3.0)\n",
            "Installing collected packages: filetype, watchdog, python-multipart, pillow, numpy, multidict, humanfriendly, h11, frozenlist, async-timeout, yarl, uvicorn, starlette, coloredlogs, asyncer, aiosignal, pymatting, onnxruntime, fastapi, aiohttp, rembg\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flax 0.6.8 requires PyYAML>=5.4.1, but you have pyyaml 5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 asyncer-0.0.2 coloredlogs-15.0.1 fastapi-0.95.0 filetype-1.2.0 frozenlist-1.3.3 h11-0.14.0 humanfriendly-10.0 multidict-6.0.4 numpy-1.23.5 onnxruntime-1.14.1 pillow-9.5.0 pymatting-1.1.8 python-multipart-0.0.6 rembg-2.0.32 starlette-0.26.1 uvicorn-0.21.1 watchdog-3.0.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run model"
      ],
      "metadata": {
        "id": "ocI7kcxGtUfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/sangyun884/HR-VITON\n",
        "!pip install opencv-python torchgeometry Pillow tqdm tensorboardX scikit-image scipy\n",
        "!gdown --id 1XJTCdRBOPVgVTmqzhVGFAgMm2NLkw5uQ\n",
        "!gdown --id 1BkSA8UJo-6eOkKcXTFOHK80Esc4vBmVC"
      ],
      "metadata": {
        "id": "TmZZ9Z7cWhj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d598cc-53ac-42af-be21-0d8f23ec18ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'HR-VITON'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 129 (delta 31), reused 18 (delta 12), pack-reused 74\u001b[K\n",
            "Receiving objects: 100% (129/129), 16.06 MiB | 18.60 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (4.7.0.72)\n",
            "Collecting torchgeometry\n",
            "  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (9.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (0.19.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from torchgeometry) (2.0.0+cu118)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (3.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (23.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (2023.3.21)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (2.25.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->torchgeometry) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->torchgeometry) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->torchgeometry) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->torchgeometry) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->torchgeometry) (3.10.7)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.0.0->torchgeometry) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.0.0->torchgeometry) (16.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\n",
            "Installing collected packages: tensorboardX, torchgeometry\n",
            "Successfully installed tensorboardX-2.6 torchgeometry-0.1.2\n",
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XJTCdRBOPVgVTmqzhVGFAgMm2NLkw5uQ\n",
            "To: /content/mtviton.pth\n",
            "100% 190M/190M [00:00<00:00, 202MB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BkSA8UJo-6eOkKcXTFOHK80Esc4vBmVC\n",
            "To: /content/gen.pth\n",
            "100% 402M/402M [00:05<00:00, 79.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/HR-VITON/get_parse_agnostic.py\n",
        "import json\n",
        "from os import path as osp\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import argparse\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def get_im_parse_agnostic(im_parse, pose_data, w=768, h=1024):\n",
        "    parse_array = np.array(im_parse)\n",
        "    parse_upper = ((parse_array == 5).astype(np.float32) +\n",
        "                    (parse_array == 6).astype(np.float32) +\n",
        "                    (parse_array == 7).astype(np.float32))\n",
        "    parse_neck = (parse_array == 10).astype(np.float32)\n",
        "\n",
        "    r = 10\n",
        "    agnostic = im_parse.copy()\n",
        "\n",
        "    # mask arms\n",
        "    for parse_id, pose_ids in [(14, [2, 5, 6, 7]), (15, [5, 2, 3, 4])]:\n",
        "        mask_arm = Image.new('L', (w, h), 'black')\n",
        "        mask_arm_draw = ImageDraw.Draw(mask_arm)\n",
        "        i_prev = pose_ids[0]\n",
        "        for i in pose_ids[1:]:\n",
        "            if (pose_data[i_prev, 0] == 0.0 and pose_data[i_prev, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n",
        "                continue\n",
        "            mask_arm_draw.line([tuple(pose_data[j]) for j in [i_prev, i]], 'white', width=r*10)\n",
        "            pointx, pointy = pose_data[i]\n",
        "            radius = r*4 if i == pose_ids[-1] else r*15\n",
        "            mask_arm_draw.ellipse((pointx-radius, pointy-radius, pointx+radius, pointy+radius), 'white', 'white')\n",
        "            i_prev = i\n",
        "        parse_arm = (np.array(mask_arm) / 255) * (parse_array == parse_id).astype(np.float32)\n",
        "        agnostic.paste(0, None, Image.fromarray(np.uint8(parse_arm * 255), 'L'))\n",
        "\n",
        "    # mask torso & neck\n",
        "    agnostic.paste(0, None, Image.fromarray(np.uint8(parse_upper * 255), 'L'))\n",
        "    agnostic.paste(0, None, Image.fromarray(np.uint8(parse_neck * 255), 'L'))\n",
        "\n",
        "    return agnostic\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data_path', type=str, help=\"dataset dir\")\n",
        "    parser.add_argument('--output_path', type=str, help=\"output dir\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    data_path = args.data_path\n",
        "    output_path = args.output_path\n",
        "    \n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    \n",
        "    for im_name in tqdm(os.listdir(osp.join(data_path, 'image-parse-v3'))):\n",
        "        # load pose image\n",
        "        pose_name = im_name.replace('.png', '_keypoints.json')\n",
        "        if pose_name in '.ipynb_checkpoints':\n",
        "          continue\n",
        "        try:\n",
        "            with open(osp.join(data_path, 'openpose_json', pose_name), 'r') as f:\n",
        "                pose_label = json.load(f)\n",
        "                pose_data = pose_label['people'][0]['pose_keypoints_2d']\n",
        "                pose_data = np.array(pose_data)\n",
        "                pose_data = pose_data.reshape((-1, 3))[:, :2]\n",
        "        except IndexError:\n",
        "            print(pose_name)\n",
        "            continue\n",
        "\n",
        "        # load parsing image\n",
        "        parse_name = im_name.replace('.jpg', '.png')\n",
        "        im_parse = Image.open(osp.join(data_path, 'image-parse-v3', parse_name))\n",
        "\n",
        "        agnostic = get_im_parse_agnostic(im_parse, pose_data)\n",
        "        \n",
        "        agnostic.save(osp.join(output_path, parse_name))"
      ],
      "metadata": {
        "id": "RlNGWtLSJH_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de30bab3-bced-46cf-9b3e-d6d28f47ea60"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/HR-VITON/get_parse_agnostic.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overwriting file test_generator.py"
      ],
      "metadata": {
        "id": "Q4H-D-1CvtKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/HR-VITON/test_generator.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.utils import make_grid as make_image_grid\n",
        "from torchvision.utils import save_image\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "from cp_dataset_test import CPDatasetTest, CPDataLoader\n",
        "\n",
        "from networks import ConditionGenerator, load_checkpoint, make_grid\n",
        "from network_generator import SPADEGenerator\n",
        "from tensorboardX import SummaryWriter\n",
        "from utils import *\n",
        "\n",
        "import torchgeometry as tgm\n",
        "from collections import OrderedDict\n",
        "\n",
        "def remove_overlap(seg_out, warped_cm):\n",
        "    \n",
        "    assert len(warped_cm.shape) == 4\n",
        "    \n",
        "    warped_cm = warped_cm - (torch.cat([seg_out[:, 1:3, :, :], seg_out[:, 5:, :, :]], dim=1)).sum(dim=1, keepdim=True) * warped_cm\n",
        "    return warped_cm\n",
        "def get_opt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--gpu_ids\", default=\"\")\n",
        "    parser.add_argument('-j', '--workers', type=int, default=4)\n",
        "    parser.add_argument('-b', '--batch-size', type=int, default=1)\n",
        "    parser.add_argument('--fp16', action='store_true', help='use amp')\n",
        "    # Cuda availability\n",
        "    parser.add_argument('--cuda',default=False, help='cuda or cpu')\n",
        "\n",
        "    parser.add_argument('--test_name', type=str, default='test', help='test name')\n",
        "    parser.add_argument(\"--dataroot\", default=\"/content/our_data_folder\")\n",
        "    parser.add_argument(\"--datamode\", default=\"test\")\n",
        "    parser.add_argument(\"--data_list\", default=\"/content/our_data_folder/test_pairs.txt\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./Output\")\n",
        "    parser.add_argument(\"--datasetting\", default=\"unpaired\")\n",
        "    parser.add_argument(\"--fine_width\", type=int, default=768)\n",
        "    parser.add_argument(\"--fine_height\", type=int, default=1024)\n",
        "\n",
        "    parser.add_argument('--tensorboard_dir', type=str, default='./data/zalando-hd-resize/tensorboard', help='save tensorboard infos')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='save checkpoint infos')\n",
        "    parser.add_argument('--tocg_checkpoint', type=str, default='/content/mtviton.pth', help='tocg checkpoint')\n",
        "    parser.add_argument('--gen_checkpoint', type=str, default='/content/gen.pth', help='G checkpoint')\n",
        "\n",
        "    parser.add_argument(\"--tensorboard_count\", type=int, default=100)\n",
        "    parser.add_argument(\"--shuffle\", action='store_true', help='shuffle input data')\n",
        "    parser.add_argument(\"--semantic_nc\", type=int, default=13)\n",
        "    parser.add_argument(\"--output_nc\", type=int, default=13)\n",
        "    parser.add_argument('--gen_semantic_nc', type=int, default=7, help='# of input label classes without unknown class')\n",
        "    \n",
        "    # network\n",
        "    parser.add_argument(\"--warp_feature\", choices=['encoder', 'T1'], default=\"T1\")\n",
        "    parser.add_argument(\"--out_layer\", choices=['relu', 'conv'], default=\"relu\")\n",
        "    \n",
        "    # training\n",
        "    parser.add_argument(\"--clothmask_composition\", type=str, choices=['no_composition', 'detach', 'warp_grad'], default='warp_grad')\n",
        "        \n",
        "    # Hyper-parameters\n",
        "    parser.add_argument('--upsample', type=str, default='bilinear', choices=['nearest', 'bilinear'])\n",
        "    parser.add_argument('--occlusion', action='store_true', help=\"Occlusion handling\")\n",
        "\n",
        "    # generator\n",
        "    parser.add_argument('--norm_G', type=str, default='spectralaliasinstance', help='instance normalization or batch normalization')\n",
        "    parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in first conv layer')\n",
        "    parser.add_argument('--init_type', type=str, default='xavier', help='network initialization [normal|xavier|kaiming|orthogonal]')\n",
        "    parser.add_argument('--init_variance', type=float, default=0.02, help='variance of the initialization distribution')\n",
        "    parser.add_argument('--num_upsampling_layers', choices=('normal', 'more', 'most'), default='most', # normal: 256, more: 512\n",
        "                        help=\"If 'more', adds upsampling layer between the two middle resnet blocks. If 'most', also add one more upsampling + resnet layer at the end of the generator\")\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "    return opt\n",
        "\n",
        "def load_checkpoint_G(model, checkpoint_path,opt):\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(\"Invalid path!\")\n",
        "        return\n",
        "    state_dict = torch.load(checkpoint_path)\n",
        "    new_state_dict = OrderedDict([(k.replace('ace', 'alias').replace('.Spade', ''), v) for (k, v) in state_dict.items()])\n",
        "    new_state_dict._metadata = OrderedDict([(k.replace('ace', 'alias').replace('.Spade', ''), v) for (k, v) in state_dict._metadata.items()])\n",
        "    model.load_state_dict(new_state_dict, strict=True)\n",
        "    if opt.cuda :\n",
        "        model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "def test(opt, test_loader, tocg, generator):\n",
        "    gauss = tgm.image.GaussianBlur((15, 15), (3, 3))\n",
        "    if opt.cuda:\n",
        "        gauss = gauss.cuda()\n",
        "    \n",
        "    # Model\n",
        "    if opt.cuda :\n",
        "        tocg.cuda()\n",
        "    tocg.eval()\n",
        "    generator.eval()\n",
        "    \n",
        "    if opt.output_dir is not None:\n",
        "        output_dir = opt.output_dir\n",
        "    else:\n",
        "        output_dir = os.path.join('./output', opt.test_name,\n",
        "                            opt.datamode, opt.datasetting, 'generator', 'output')\n",
        "    grid_dir = os.path.join('./output', opt.test_name,\n",
        "                             opt.datamode, opt.datasetting, 'generator', 'grid')\n",
        "    \n",
        "    os.makedirs(grid_dir, exist_ok=True)\n",
        "    \n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    num = 0\n",
        "    iter_start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for inputs in test_loader.data_loader:\n",
        "\n",
        "            if opt.cuda :\n",
        "                pose_map = inputs['pose'].cuda()\n",
        "                pre_clothes_mask = inputs['cloth_mask'][opt.datasetting].cuda()\n",
        "                label = inputs['parse']\n",
        "                parse_agnostic = inputs['parse_agnostic']\n",
        "                agnostic = inputs['agnostic'].cuda()\n",
        "                clothes = inputs['cloth'][opt.datasetting].cuda() # target cloth\n",
        "                densepose = inputs['densepose'].cuda()\n",
        "                im = inputs['image']\n",
        "                input_label, input_parse_agnostic = label.cuda(), parse_agnostic.cuda()\n",
        "                pre_clothes_mask = torch.FloatTensor((pre_clothes_mask.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
        "            else :\n",
        "                pose_map = inputs['pose']\n",
        "                pre_clothes_mask = inputs['cloth_mask'][opt.datasetting]\n",
        "                label = inputs['parse']\n",
        "                parse_agnostic = inputs['parse_agnostic']\n",
        "                agnostic = inputs['agnostic']\n",
        "                clothes = inputs['cloth'][opt.datasetting] # target cloth\n",
        "                densepose = inputs['densepose']\n",
        "                im = inputs['image']\n",
        "                input_label, input_parse_agnostic = label, parse_agnostic\n",
        "                pre_clothes_mask = torch.FloatTensor((pre_clothes_mask.detach().cpu().numpy() > 0.5).astype(np.float))\n",
        "\n",
        "\n",
        "\n",
        "            # down\n",
        "            pose_map_down = F.interpolate(pose_map, size=(256, 192), mode='bilinear')\n",
        "            pre_clothes_mask_down = F.interpolate(pre_clothes_mask, size=(256, 192), mode='nearest')\n",
        "            input_label_down = F.interpolate(input_label, size=(256, 192), mode='bilinear')\n",
        "            input_parse_agnostic_down = F.interpolate(input_parse_agnostic, size=(256, 192), mode='nearest')\n",
        "            agnostic_down = F.interpolate(agnostic, size=(256, 192), mode='nearest')\n",
        "            clothes_down = F.interpolate(clothes, size=(256, 192), mode='bilinear')\n",
        "            densepose_down = F.interpolate(densepose, size=(256, 192), mode='bilinear')\n",
        "\n",
        "            shape = pre_clothes_mask.shape\n",
        "            \n",
        "            # multi-task inputs\n",
        "            input1 = torch.cat([clothes_down, pre_clothes_mask_down], 1)\n",
        "            input2 = torch.cat([input_parse_agnostic_down, densepose_down], 1)\n",
        "\n",
        "            # forward\n",
        "            flow_list, fake_segmap, warped_cloth_paired, warped_clothmask_paired = tocg(opt,input1, input2)\n",
        "            \n",
        "            # warped cloth mask one hot\n",
        "            if opt.cuda :\n",
        "                warped_cm_onehot = torch.FloatTensor((warped_clothmask_paired.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
        "            else :\n",
        "                warped_cm_onehot = torch.FloatTensor((warped_clothmask_paired.detach().cpu().numpy() > 0.5).astype(np.float))\n",
        "\n",
        "            if opt.clothmask_composition != 'no_composition':\n",
        "                if opt.clothmask_composition == 'detach':\n",
        "                    cloth_mask = torch.ones_like(fake_segmap)\n",
        "                    cloth_mask[:,3:4, :, :] = warped_cm_onehot\n",
        "                    fake_segmap = fake_segmap * cloth_mask\n",
        "                    \n",
        "                if opt.clothmask_composition == 'warp_grad':\n",
        "                    cloth_mask = torch.ones_like(fake_segmap)\n",
        "                    cloth_mask[:,3:4, :, :] = warped_clothmask_paired\n",
        "                    fake_segmap = fake_segmap * cloth_mask\n",
        "                    \n",
        "            # make generator input parse map\n",
        "            fake_parse_gauss = gauss(F.interpolate(fake_segmap, size=(opt.fine_height, opt.fine_width), mode='bilinear'))\n",
        "            fake_parse = fake_parse_gauss.argmax(dim=1)[:, None]\n",
        "\n",
        "            if opt.cuda :\n",
        "                old_parse = torch.FloatTensor(fake_parse.size(0), 13, opt.fine_height, opt.fine_width).zero_().cuda()\n",
        "            else:\n",
        "                old_parse = torch.FloatTensor(fake_parse.size(0), 13, opt.fine_height, opt.fine_width).zero_()\n",
        "            old_parse.scatter_(1, fake_parse, 1.0)\n",
        "\n",
        "            labels = {\n",
        "                0:  ['background',  [0]],\n",
        "                1:  ['paste',       [2, 4, 7, 8, 9, 10, 11]],\n",
        "                2:  ['upper',       [3]],\n",
        "                3:  ['hair',        [1]],\n",
        "                4:  ['left_arm',    [5]],\n",
        "                5:  ['right_arm',   [6]],\n",
        "                6:  ['noise',       [12]]\n",
        "            }\n",
        "            if opt.cuda :\n",
        "                parse = torch.FloatTensor(fake_parse.size(0), 7, opt.fine_height, opt.fine_width).zero_().cuda()\n",
        "            else:\n",
        "                parse = torch.FloatTensor(fake_parse.size(0), 7, opt.fine_height, opt.fine_width).zero_()\n",
        "            for i in range(len(labels)):\n",
        "                for label in labels[i][1]:\n",
        "                    parse[:, i] += old_parse[:, label]\n",
        "                    \n",
        "            # warped cloth\n",
        "            N, _, iH, iW = clothes.shape\n",
        "            flow = F.interpolate(flow_list[-1].permute(0, 3, 1, 2), size=(iH, iW), mode='bilinear').permute(0, 2, 3, 1)\n",
        "            flow_norm = torch.cat([flow[:, :, :, 0:1] / ((96 - 1.0) / 2.0), flow[:, :, :, 1:2] / ((128 - 1.0) / 2.0)], 3)\n",
        "            \n",
        "            grid = make_grid(N, iH, iW,opt)\n",
        "            warped_grid = grid + flow_norm\n",
        "            warped_cloth = F.grid_sample(clothes, warped_grid, padding_mode='border')\n",
        "            warped_clothmask = F.grid_sample(pre_clothes_mask, warped_grid, padding_mode='border')\n",
        "            if opt.occlusion:\n",
        "                warped_clothmask = remove_overlap(F.softmax(fake_parse_gauss, dim=1), warped_clothmask)\n",
        "                warped_cloth = warped_cloth * warped_clothmask + torch.ones_like(warped_cloth) * (1-warped_clothmask)\n",
        "            \n",
        "\n",
        "            output = generator(torch.cat((agnostic, densepose, warped_cloth), dim=1), parse)\n",
        "            # visualize\n",
        "            unpaired_names = []\n",
        "            for i in range(shape[0]):\n",
        "                grid = make_image_grid([(clothes[i].cpu() / 2 + 0.5), (pre_clothes_mask[i].cpu()).expand(3, -1, -1), visualize_segmap(parse_agnostic.cpu(), batch=i), ((densepose.cpu()[i]+1)/2),\n",
        "                                        (warped_cloth[i].cpu().detach() / 2 + 0.5), (warped_clothmask[i].cpu().detach()).expand(3, -1, -1), visualize_segmap(fake_parse_gauss.cpu(), batch=i),\n",
        "                                        (pose_map[i].cpu()/2 +0.5), (warped_cloth[i].cpu()/2 + 0.5), (agnostic[i].cpu()/2 + 0.5),\n",
        "                                        (im[i]/2 +0.5), (output[i].cpu()/2 +0.5)],\n",
        "                                        nrow=4)\n",
        "                unpaired_name = (inputs['c_name']['paired'][i].split('.')[0] + '_' + inputs['c_name'][opt.datasetting][i].split('.')[0] + '.png')\n",
        "                save_image(grid, os.path.join(grid_dir, unpaired_name))\n",
        "                unpaired_names.append(unpaired_name)\n",
        "                \n",
        "            # save output\n",
        "            save_images(output, unpaired_names, output_dir)\n",
        "                \n",
        "            num += shape[0]\n",
        "            print(num)\n",
        "\n",
        "    print(f\"Test time {time.time() - iter_start_time}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    opt = get_opt()\n",
        "    print(opt)\n",
        "    print(\"Start to test %s!\")\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.gpu_ids\n",
        "    \n",
        "    # create test dataset & loader\n",
        "    test_dataset = CPDatasetTest(opt)\n",
        "    test_loader = CPDataLoader(opt, test_dataset)\n",
        "    \n",
        "    # visualization\n",
        "    # if not os.path.exists(opt.tensorboard_dir):\n",
        "    #     os.makedirs(opt.tensorboard_dir)\n",
        "    # board = SummaryWriter(log_dir=os.path.join(opt.tensorboard_dir, opt.test_name, opt.datamode, opt.datasetting))\n",
        "\n",
        "    ## Model\n",
        "    # tocg\n",
        "    input1_nc = 4  # cloth + cloth-mask\n",
        "    input2_nc = opt.semantic_nc + 3  # parse_agnostic + densepose\n",
        "    tocg = ConditionGenerator(opt, input1_nc=input1_nc, input2_nc=input2_nc, output_nc=opt.output_nc, ngf=96, norm_layer=nn.BatchNorm2d)\n",
        "       \n",
        "    # generator\n",
        "    opt.semantic_nc = 7\n",
        "    generator = SPADEGenerator(opt, 3+3+3)\n",
        "    generator.print_network()\n",
        "       \n",
        "    # Load Checkpoint\n",
        "    load_checkpoint(tocg, opt.tocg_checkpoint,opt)\n",
        "    load_checkpoint_G(generator, opt.gen_checkpoint,opt)\n",
        "\n",
        "    # Train\n",
        "    test(opt, test_loader, tocg, generator)\n",
        "\n",
        "    print(\"Finished testing!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "QfaSHWAqvsJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b7d8ad-68fb-4010-c5a3-ce84d75a507d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/HR-VITON/test_generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/HR-VITON/get_parse_agnostic.py\n",
        "import json\n",
        "from os import path as osp\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import argparse\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def get_im_parse_agnostic(im_parse, pose_data, w=768, h=1024):\n",
        "    parse_array = np.array(im_parse)\n",
        "    parse_upper = ((parse_array == 5).astype(np.float32) +\n",
        "                    (parse_array == 6).astype(np.float32) +\n",
        "                    (parse_array == 7).astype(np.float32))\n",
        "    parse_neck = (parse_array == 10).astype(np.float32)\n",
        "\n",
        "    r = 10\n",
        "    agnostic = im_parse.copy()\n",
        "\n",
        "    # mask arms\n",
        "    for parse_id, pose_ids in [(14, [2, 5, 6, 7]), (15, [5, 2, 3, 4])]:\n",
        "        mask_arm = Image.new('L', (w, h), 'black')\n",
        "        mask_arm_draw = ImageDraw.Draw(mask_arm)\n",
        "        i_prev = pose_ids[0]\n",
        "        for i in pose_ids[1:]:\n",
        "            if (pose_data[i_prev, 0] == 0.0 and pose_data[i_prev, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n",
        "                continue\n",
        "            mask_arm_draw.line([tuple(pose_data[j]) for j in [i_prev, i]], 'white', width=r*10)\n",
        "            pointx, pointy = pose_data[i]\n",
        "            radius = r*4 if i == pose_ids[-1] else r*15\n",
        "            mask_arm_draw.ellipse((pointx-radius, pointy-radius, pointx+radius, pointy+radius), 'white', 'white')\n",
        "            i_prev = i\n",
        "        parse_arm = (np.array(mask_arm) / 255) * (parse_array == parse_id).astype(np.float32)\n",
        "        agnostic.paste(0, None, Image.fromarray(np.uint8(parse_arm * 255), 'L'))\n",
        "\n",
        "    # mask torso & neck\n",
        "    agnostic.paste(0, None, Image.fromarray(np.uint8(parse_upper * 255), 'L'))\n",
        "    agnostic.paste(0, None, Image.fromarray(np.uint8(parse_neck * 255), 'L'))\n",
        "\n",
        "    return agnostic\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data_path', type=str, help=\"dataset dir\")\n",
        "    parser.add_argument('--output_path', type=str, help=\"output dir\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    data_path = args.data_path\n",
        "    output_path = args.output_path\n",
        "    \n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    \n",
        "    for im_name in tqdm(os.listdir(osp.join(data_path, 'image-parse-v3'))):\n",
        "        # load pose image\n",
        "        pose_name = im_name.replace('.png', '_keypoints.json')\n",
        "        if pose_name in '.ipynb_checkpoints':\n",
        "          continue\n",
        "        try:\n",
        "            with open(osp.join(data_path, 'openpose_json', pose_name), 'r') as f:\n",
        "                pose_label = json.load(f)\n",
        "                pose_data = pose_label['people'][0]['pose_keypoints_2d']\n",
        "                pose_data = np.array(pose_data)\n",
        "                pose_data = pose_data.reshape((-1, 3))[:, :2]\n",
        "        except IndexError:\n",
        "            print(pose_name)\n",
        "            continue\n",
        "\n",
        "        # load parsing image\n",
        "        parse_name = im_name.replace('.jpg', '.png')\n",
        "        im_parse = Image.open(osp.join(data_path, 'image-parse-v3', parse_name))\n",
        "\n",
        "        agnostic = get_im_parse_agnostic(im_parse, pose_data)\n",
        "        \n",
        "        agnostic.save(osp.join(output_path, parse_name))"
      ],
      "metadata": {
        "id": "CWe1FvjNzDyW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32db5005-f3a3-47b1-c2c6-f7acfb660f7c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/HR-VITON/get_parse_agnostic.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Streamlit"
      ],
      "metadata": {
        "id": "pnB0vU1Us1qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "ZjbRbUoNA3xX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e8ee21-b99a-45db-8ac1-a9d535c00a26"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/9.7 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m8.8/9.7 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "58dhXJR0BTzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18511ff2-9262-4339-95b0-56a5d85e3075"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Openpose"
      ],
      "metadata": {
        "id": "NDzccVyHv4F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/openpose.py\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "os.chdir('/content/')\n",
        "path_folder_image = '/content/our_data_folder/test/image/'\n",
        "path_human_input = '/content/human_input/'\n",
        "path_shirt_input = '/content/shirt_input/'\n",
        "img = cv2.imread(os.path.join(path_human_input, os.listdir(path_human_input)[-1]))\n",
        "print(os.path.join(path_human_input, os.listdir(path_human_input)[-1]))\n",
        "img = cv2.resize(img, (768, 1024))\n",
        "tmp_path_image = os.path.join(path_folder_image, os.listdir(path_human_input)[-1])\n",
        "cv2.imwrite(tmp_path_image, img)\n",
        "\n",
        "print('Hihi')\n",
        "\n",
        "path_folder_cloth = '/content/our_data_folder/test/cloth/'\n",
        "img = cv2.imread(os.path.join(path_shirt_input, os.listdir(path_shirt_input)[-1]))\n",
        "img = cv2.resize(img, (768, 1024))\n",
        "tmp_path_cloth = os.path.join(path_folder_cloth, os.listdir(path_shirt_input)[-1])\n",
        "cv2.imwrite(tmp_path_cloth, img)\n",
        "\n",
        "os.chdir('/content/')\n",
        "shutil.copy(tmp_path_image, '/content/our_data_folder/test/cloth')\n",
        "shutil.copy(tmp_path_image, '/content/our_data_folder/test/cloth-mask')\n",
        "shutil.copy(tmp_path_cloth, path_folder_image)"
      ],
      "metadata": {
        "id": "SWIkBUyB89RG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4ee38f-9351-434c-f2e2-948bc029c21f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/openpose.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/openpose.sh\n",
        "cd openpose && ./build/examples/openpose/openpose.bin --image_dir /content/our_data_folder/test/image --hand --disable_blending --display 0 --write_json /content/our_data_folder/test/openpose_json --write_images /content/our_data_folder/test/openpose_img --num_gpu 1 --num_gpu_start 0"
      ],
      "metadata": {
        "id": "njAZsSe7FTh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ce94d6-2694-4e06-f31e-142d81ec4161"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/openpose.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human parse"
      ],
      "metadata": {
        "id": "fGhqbewovcHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/humanparse1.py\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "os.chdir('/content/CIHP_PGN')\n",
        "if not os.path.exists('/content/CIHP_PGN/image_resize'):\n",
        "  os.mkdir('/content/CIHP_PGN/image_resize')\n",
        "file_name = os.listdir('/content/human_input/')[-1]\n",
        "img = cv2.imread(\"/content/our_data_folder/test/image/\" + file_name)\n",
        "img_r = cv2.resize(img, (192, 256))\n",
        "cv2.imwrite(\"/content/CIHP_PGN/image_resize/\" + file_name, img_r)\n",
        "\n",
        "if not os.path.exists('output'):\n",
        "  os.mkdir('output')\n"
      ],
      "metadata": {
        "id": "XlQc2mnckY47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5171b24-5524-4abb-9089-5cff384451c0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/humanparse1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/humanparse1.sh\n",
        "python /content/CIHP_PGN/inf_pgn.py --directory /content/CIHP_PGN/image_resize --output /content/CIHP_PGN/output"
      ],
      "metadata": {
        "id": "d8FjJdr-IQV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05402697-f7a2-4712-a92e-b66d348d4c0e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/humanparse1.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/humanparse2.py\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageFilter\n",
        "import os\n",
        "os.chdir('/content/CIHP_PGN')\n",
        "file_name = os.listdir('/content/human_input/')[-1]\n",
        "\n",
        "img = Image.open(\"/content/CIHP_PGN/output/cihp_parsing_maps/\" + file_name[:-4] + \".png\")\n",
        "img_r = transforms.Resize(768, interpolation=0)(img)\n",
        "img_s = img_r.filter(ImageFilter.ModeFilter(size=7))\n",
        "img_s.save(\"/content/our_data_folder/test/image-parse-v3/\" + file_name[:-3] + 'png')\n",
        "\n",
        "img = Image.open(\"/content/CIHP_PGN/output/cihp_parsing_maps/\" + file_name[:-4] + \"_agn_vis.png\")\n",
        "img_r = transforms.Resize(768, interpolation=0)(img)\n",
        "img_s = img_r.filter(ImageFilter.ModeFilter(size=7))\n",
        "img_s.save(\"/content/our_data_folder/test/image-parse-agnostic-v3.2/\" + file_name[:-3] + 'png')\n",
        "\n",
        "# img = cv2.imread(\"/content/CIHP_PGN/output/cihp_parsing_maps/\" + file_name[:-4] + \"_vis.png\")\n",
        "# img_r = cv2.resize(img, (768, 1024))\n",
        "# cv2.imwrite((\"/content/our_data_folder/test/image-parse-v3/\" + file_name[:-3] + 'png'), img_r)\n",
        "\n",
        "# img = cv2.imread(\"/content/CIHP_PGN/output/cihp_parsing_maps/\" + file_name[:-4] + \"_agn.png\")\n",
        "# img_r = cv2.resize(img, (768, 1024))\n",
        "# cv2.imwrite((\"/content/our_data_folder/test/image-parse-agnostic-v3.2/\" + file_name[:-3] + 'png'), img_r)\n"
      ],
      "metadata": {
        "id": "VEEoi_bsIZ8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715df782-1eb3-4b6a-a029-a2cb2b25f312"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/humanparse2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/humanparse2.sh\n",
        "python3 /content/HR-VITON/get_parse_agnostic.py --data_path /content/our_data_folder/test/ --output_path /content/our_data_folder/test/image-parse-agnostic-v3.2"
      ],
      "metadata": {
        "id": "Ed1yxdoOIipG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5f2902-3e56-4d8c-9241-160a60fb45f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/humanparse2.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Densepose"
      ],
      "metadata": {
        "id": "kVlYQY4owxVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/densepose1.py\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "os.chdir('/content/detectron2')\n",
        "image_path = \"/content/our_data_folder/test/image/\"\n",
        "file_name = os.listdir(image_path)[-1]\n",
        "image_path = os.path.join(image_path, file_name)\n",
        "cmd_str = f'python /content/detectron2/projects/DensePose/apply_net.py show /content/detectron2/projects/DensePose/configs/densepose_rcnn_R_50_FPN_s1x.yaml https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl {image_path} dp_segm -v'\n",
        "subprocess.run(cmd_str, shell=True)"
      ],
      "metadata": {
        "id": "zydLQNOFhu6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e858ed39-6080-4a52-8ac5-80d3be975e5f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/densepose1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/densepose.sh\n",
        "#!/bin/bash\n",
        "\n",
        "# Run Python script\n",
        "python /content/detectron2/projects/DensePose/apply_net.py show /content/detectron2/projects/DensePose/configs/densepose_rcnn_R_50_FPN_s1x.yaml https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl image_path dp_segm -v\n"
      ],
      "metadata": {
        "id": "fFnLskduIwJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59260968-c825-418c-aed1-a6b43c273e85"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/densepose.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/densepose2.py\n",
        "import shutil\n",
        "import os\n",
        "image_path = \"/content/our_data_folder/test/image/\"\n",
        "file_name = os.listdir(\"/content/human_input\")[-1]\n",
        "image_path = os.path.join(image_path, file_name)\n",
        "shutil.copy('/content/detectron2/densepose_result.jpg', '/content/our_data_folder/test/image-densepose')\n",
        "\n",
        "os.rename('/content/our_data_folder/test/image-densepose/densepose_result.jpg', '/content/our_data_folder/test/image-densepose/'+file_name)"
      ],
      "metadata": {
        "id": "psrhL85YI2jk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1adaf13-526d-418a-911e-73d2e985200b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/densepose2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloth mask"
      ],
      "metadata": {
        "id": "sErdM5_ww33u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/clothmask.py\n",
        "from rembg import remove\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "\n",
        "file_name = os.listdir('/content/shirt_input')[-1]\n",
        "image_path = os.path.join('/content/our_data_folder/test/cloth/', file_name)\n",
        "\n",
        "img = cv2.imread(image_path)\n",
        "output = remove(img)\n",
        "img = output[:, :, :3]\n",
        "img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "masked = np.where(img == 0, 0, 255)\n",
        "cv2_imshow(masked)\n",
        "\n",
        "cv2.imwrite(os.path.join('/content/our_data_folder/test/cloth-mask/', file_name), masked)\n"
      ],
      "metadata": {
        "id": "nvCrhrWNw5cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0a1926-c1d3-4a8c-d2df-26a099631a31"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/clothmask.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VITON"
      ],
      "metadata": {
        "id": "bwWfBJFMxAAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/viton.py\n",
        "import os\n",
        "import cv2\n",
        "cloth_path = os.listdir('/content/our_data_folder/test/cloth')\n",
        "image_path = os.listdir('/content/our_data_folder/test/image')\n",
        "image_people = os.listdir('/content/human_input/')[-1]\n",
        "image_cloth = os.listdir('/content/shirt_input/')[-1]\n",
        "with open('/content/our_data_folder/test_pairs.txt', 'w+') as f:\n",
        "  f.write(image_people + ' ' + image_cloth)\n",
        "\n",
        "img_original = cv2.imread(os.path.join('/content/human_input/', image_people))\n",
        "shape_original = img_original.shape\n",
        "img_res = cv2.imread(\"/content/Output/aperson_ashirt.png\")\n",
        "img_r = cv2.resize(img_res, shape_original[:2][::-1])\n",
        "cv2.imwrite('/content/res.png', img_r)"
      ],
      "metadata": {
        "id": "3y2sDnsqxBXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9849268-0de6-42ff-fc7c-fe84fbfc38dd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/viton.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/viton.sh\n",
        "python /content/HR-VITON/test_generator.py"
      ],
      "metadata": {
        "id": "tnPYfB7sxCzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337d29ab-df50-4cef-bdd5-5f42db10d5ee"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/viton.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/our_data_folder/test/cloth\n",
        "# !mkdir /content/our_data_folder/test/cloth\n",
        "# !rm -rf /content/our_data_folder/test/cloth-mask\n",
        "# !mkdir /content/our_data_folder/test/cloth-mask\n",
        "# !rm -rf /content/our_data_folder/test/image\n",
        "# !mkdir /content/our_data_folder/test/image"
      ],
      "metadata": {
        "id": "zNKgb7flYntk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/HR-VITON/cp_dataset_test.py\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "class CPDatasetTest(data.Dataset):\n",
        "    \"\"\"\n",
        "        Test Dataset for CP-VTON.\n",
        "    \"\"\"\n",
        "    def __init__(self, opt):\n",
        "        super(CPDatasetTest, self).__init__()\n",
        "        # base setting\n",
        "        self.opt = opt\n",
        "        self.root = opt.dataroot\n",
        "        self.datamode = opt.datamode # train or test or self-defined\n",
        "        self.data_list = opt.data_list\n",
        "        self.fine_height = opt.fine_height\n",
        "        self.fine_width = opt.fine_width\n",
        "        self.semantic_nc = opt.semantic_nc\n",
        "        self.data_path = osp.join(opt.dataroot, opt.datamode)\n",
        "        self.transform = transforms.Compose([  \\\n",
        "                transforms.ToTensor(),   \\\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "        # load data list\n",
        "        im_names = []\n",
        "        c_names = []\n",
        "        with open(osp.join(opt.dataroot, opt.data_list), 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                im_name, c_name = line.strip().split()\n",
        "                im_names.append(im_name)\n",
        "                c_names.append(c_name)\n",
        "\n",
        "        self.im_names = im_names\n",
        "        self.c_names = dict()\n",
        "        self.c_names['paired'] = im_names\n",
        "        self.c_names['unpaired'] = c_names\n",
        "\n",
        "    def name(self):\n",
        "        return \"CPDataset\"\n",
        "    def get_agnostic(self, im, im_parse, pose_data):\n",
        "        parse_array = np.array(im_parse)\n",
        "        parse_head = ((parse_array == 4).astype(np.float32) +\n",
        "                      (parse_array == 13).astype(np.float32))\n",
        "        parse_lower = ((parse_array == 9).astype(np.float32) +\n",
        "                       (parse_array == 12).astype(np.float32) +\n",
        "                       (parse_array == 16).astype(np.float32) +\n",
        "                       (parse_array == 17).astype(np.float32) +\n",
        "                       (parse_array == 18).astype(np.float32) +\n",
        "                       (parse_array == 19).astype(np.float32))\n",
        "\n",
        "        agnostic = im.copy()\n",
        "        agnostic_draw = ImageDraw.Draw(agnostic)\n",
        "\n",
        "        length_a = np.linalg.norm(pose_data[5] - pose_data[2])\n",
        "        length_b = np.linalg.norm(pose_data[12] - pose_data[9])\n",
        "        point = (pose_data[9] + pose_data[12]) / 2\n",
        "        pose_data[9] = point + (pose_data[9] - point) / length_b * length_a\n",
        "        pose_data[12] = point + (pose_data[12] - point) / length_b * length_a\n",
        "\n",
        "        r = int(length_a / 16) + 1\n",
        "\n",
        "        # mask torso\n",
        "        for i in [9, 12]:\n",
        "            pointx, pointy = pose_data[i]\n",
        "            agnostic_draw.ellipse((pointx-r*2, pointy-r*5, pointx+r*2, pointy+r*5), 'gray', 'gray')\n",
        "        agnostic_draw.line([tuple(pose_data[i]) for i in [2, 9]], 'gray', width=r*5)\n",
        "        agnostic_draw.line([tuple(pose_data[i]) for i in [5, 12]], 'gray', width=r*5)\n",
        "        agnostic_draw.line([tuple(pose_data[i]) for i in [9, 12]], 'gray', width=r*11)\n",
        "        agnostic_draw.polygon([tuple(pose_data[i]) for i in [2, 5, 12, 9]], 'gray', 'gray')\n",
        "\n",
        "        # mask neck\n",
        "        pointx, pointy = pose_data[1]\n",
        "        agnostic_draw.rectangle((pointx-r*2, pointy-r*4, pointx+r*2, pointy), 'gray', 'gray')\n",
        "\n",
        "        # mask arms\n",
        "        agnostic_draw.line([tuple(pose_data[i]) for i in [2, 5]], 'gray', width=r*11)\n",
        "        for i in [2, 5]:\n",
        "            pointx, pointy = pose_data[i]\n",
        "            agnostic_draw.ellipse((pointx-r*4, pointy-r*5, pointx+r*4, pointy+r*5), 'gray', 'gray')\n",
        "        for i in [3, 4, 6, 7]:\n",
        "            if (pose_data[i-1, 0] == 0.0 and pose_data[i-1, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n",
        "                continue\n",
        "            agnostic_draw.line([tuple(pose_data[j]) for j in [i - 1, i]], 'gray', width=r*10)\n",
        "            pointx, pointy = pose_data[i]\n",
        "            agnostic_draw.ellipse((pointx-r*4, pointy-r*4, pointx+r*4, pointy+r*4), 'gray', 'gray')\n",
        "\n",
        "        for parse_id, pose_ids in [(14, [5, 6, 7]), (15, [2, 3, 4])]:\n",
        "            mask_arm = Image.new('L', (768, 1024), 'white')\n",
        "            mask_arm_draw = ImageDraw.Draw(mask_arm)\n",
        "            pointx, pointy = pose_data[pose_ids[0]]\n",
        "            mask_arm_draw.ellipse((pointx-r*4, pointy-r*5, pointx+r*4, pointy+r*5), 'black', 'black')\n",
        "            for i in pose_ids[1:]:\n",
        "                if (pose_data[i-1, 0] == 0.0 and pose_data[i-1, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n",
        "                    continue\n",
        "                mask_arm_draw.line([tuple(pose_data[j]) for j in [i - 1, i]], 'black', width=r*10)\n",
        "                pointx, pointy = pose_data[i]\n",
        "                if i != pose_ids[-1]:\n",
        "                    mask_arm_draw.ellipse((pointx-r*4, pointy-r*4, pointx+r*4, pointy+r*4), 'black', 'black')\n",
        "            mask_arm_draw.ellipse((pointx-r*3, pointy-r*3, pointx+r*3, pointy+r*3), 'black', 'black')\n",
        "\n",
        "            parse_arm = (np.array(mask_arm) / 255) * (parse_array == parse_id).astype(np.float32)\n",
        "            agnostic.paste(im, None, Image.fromarray(np.uint8(parse_arm * 255), 'L'))\n",
        "\n",
        "        agnostic.paste(im, None, Image.fromarray(np.uint8(parse_head * 255), 'L'))\n",
        "        agnostic.paste(im, None, Image.fromarray(np.uint8(parse_lower * 255), 'L'))\n",
        "        return agnostic\n",
        "    def __getitem__(self, index):\n",
        "        im_name = self.im_names[index]\n",
        "        c_name = {}\n",
        "        c = {}\n",
        "        cm = {}\n",
        "        for key in self.c_names:\n",
        "            c_name[key] = self.c_names[key][index]\n",
        "            c[key] = Image.open(osp.join(self.data_path, 'cloth', c_name[key])).convert('RGB')\n",
        "            c[key] = transforms.Resize(self.fine_width, interpolation=2)(c[key])\n",
        "            cm[key] = Image.open(osp.join(self.data_path, 'cloth-mask', c_name[key]))\n",
        "            cm[key] = transforms.Resize(self.fine_width, interpolation=0)(cm[key])\n",
        "\n",
        "            c[key] = self.transform(c[key])  # [-1,1]\n",
        "            cm_array = np.array(cm[key])\n",
        "            cm_array = (cm_array >= 128).astype(np.float32)\n",
        "            cm[key] = torch.from_numpy(cm_array)  # [0,1]\n",
        "            cm[key].unsqueeze_(0)\n",
        "\n",
        "        # person image\n",
        "        im_pil_big = Image.open(osp.join(self.data_path, 'image', im_name))\n",
        "        im_pil = transforms.Resize(self.fine_width, interpolation=2)(im_pil_big)\n",
        "        \n",
        "        im = self.transform(im_pil)\n",
        "\n",
        "        # load parsing image\n",
        "        parse_name = im_name.replace('.jpg', '.png')\n",
        "        im_parse_pil_big = Image.open(osp.join(self.data_path, 'image-parse-v3', parse_name))\n",
        "        im_parse_pil = transforms.Resize(self.fine_width, interpolation=0)(im_parse_pil_big)\n",
        "        parse = torch.from_numpy(np.array(im_parse_pil)[None]).long()\n",
        "        im_parse = self.transform(im_parse_pil.convert('RGB'))\n",
        "        \n",
        "        labels = {\n",
        "            0:  ['background',  [0, 10]],\n",
        "            1:  ['hair',        [1, 2]],\n",
        "            2:  ['face',        [4, 13]],\n",
        "            3:  ['upper',       [5, 6, 7]],\n",
        "            4:  ['bottom',      [9, 12]],\n",
        "            5:  ['left_arm',    [14]],\n",
        "            6:  ['right_arm',   [15]],\n",
        "            7:  ['left_leg',    [16]],\n",
        "            8:  ['right_leg',   [17]],\n",
        "            9:  ['left_shoe',   [18]],\n",
        "            10: ['right_shoe',  [19]],\n",
        "            11: ['socks',       [8]],\n",
        "            12: ['noise',       [3, 11]]\n",
        "        }\n",
        "\n",
        "        parse_map = torch.FloatTensor(20, self.fine_height, self.fine_width).zero_()\n",
        "        parse_map = parse_map.scatter_(0, parse, 1.0)\n",
        "        new_parse_map = torch.FloatTensor(self.semantic_nc, self.fine_height, self.fine_width).zero_()\n",
        "        \n",
        "        for i in range(len(labels)):\n",
        "            for label in labels[i][1]:\n",
        "                new_parse_map[i] += parse_map[label]\n",
        "        \n",
        "        parse_onehot = torch.FloatTensor(1, self.fine_height, self.fine_width).zero_()\n",
        "        for i in range(len(labels)):\n",
        "            for label in labels[i][1]:\n",
        "                parse_onehot[0] += parse_map[label] * i\n",
        "\n",
        "        # load image-parse-agnostic\n",
        "        image_parse_agnostic = Image.open(osp.join(self.data_path, 'image-parse-agnostic-v3.2', parse_name))\n",
        "        image_parse_agnostic = transforms.Resize(self.fine_width, interpolation=0)(image_parse_agnostic)\n",
        "        parse_agnostic = torch.from_numpy(np.array(image_parse_agnostic)[None]).long()\n",
        "        image_parse_agnostic = self.transform(image_parse_agnostic.convert('RGB'))\n",
        "\n",
        "        parse_agnostic_map = torch.FloatTensor(20, self.fine_height, self.fine_width).zero_()\n",
        "        parse_agnostic_map = parse_agnostic_map.scatter_(0, parse_agnostic, 1.0)\n",
        "        new_parse_agnostic_map = torch.FloatTensor(self.semantic_nc, self.fine_height, self.fine_width).zero_()\n",
        "        for i in range(len(labels)):\n",
        "            for label in labels[i][1]:\n",
        "                new_parse_agnostic_map[i] += parse_agnostic_map[label]\n",
        "                \n",
        "\n",
        "        # parse cloth & parse cloth mask\n",
        "        pcm = new_parse_map[3:4]\n",
        "        im_c = im * pcm + (1 - pcm)\n",
        "        \n",
        "        # load pose points\n",
        "        if 'jpg' in im_name:\n",
        "            pose_name = im_name.replace('.jpg', '_rendered.png')\n",
        "        else:\n",
        "            pose_name = im_name.replace('.png', '_rendered.png')\n",
        "        pose_map = Image.open(osp.join(self.data_path, 'openpose_img', pose_name))\n",
        "        pose_map = transforms.Resize(self.fine_width, interpolation=2)(pose_map)\n",
        "        pose_map = self.transform(pose_map)  # [-1,1]\n",
        "\n",
        "        if 'jpg' in im_name:\n",
        "            pose_name = im_name.replace('.jpg', '_keypoints.json')\n",
        "        else:\n",
        "            pose_name = im_name.replace('.png', '_keypoints.json')\n",
        "        with open(osp.join(self.data_path, 'openpose_json', pose_name), 'r') as f:\n",
        "            pose_label = json.load(f)\n",
        "            pose_data = pose_label['people'][0]['pose_keypoints_2d']\n",
        "            pose_data = np.array(pose_data)\n",
        "            pose_data = pose_data.reshape((-1, 3))[:, :2]\n",
        "\n",
        "        \n",
        "        # load densepose\n",
        "        densepose_name = im_name.replace('image', 'image-densepose')\n",
        "        densepose_map = Image.open(osp.join(self.data_path, 'image-densepose', densepose_name))\n",
        "        densepose_map = transforms.Resize(self.fine_width, interpolation=2)(densepose_map)\n",
        "        densepose_map = self.transform(densepose_map)  # [-1,1]\n",
        "        agnostic = self.get_agnostic(im_pil_big, im_parse_pil_big, pose_data)\n",
        "        agnostic = transforms.Resize(self.fine_width, interpolation=2)(agnostic)\n",
        "        agnostic = self.transform(agnostic)\n",
        "        \n",
        "\n",
        "\n",
        "        result = {\n",
        "            'c_name':   c_name,     # for visualization\n",
        "            'im_name':  im_name,    # for visualization or ground truth\n",
        "            # intput 1 (clothfloww)\n",
        "            'cloth':    c,          # for input\n",
        "            'cloth_mask':     cm,   # for input\n",
        "            # intput 2 (segnet)\n",
        "            'parse_agnostic': new_parse_agnostic_map,\n",
        "            'densepose': densepose_map,\n",
        "            'pose': pose_map,       # for conditioning\n",
        "            # GT\n",
        "            'parse_onehot' : parse_onehot,  # Cross Entropy\n",
        "            'parse': new_parse_map, # GAN Loss real\n",
        "            'pcm': pcm,             # L1 Loss & vis\n",
        "            'parse_cloth': im_c,    # VGG Loss & vis\n",
        "            # visualization\n",
        "            'image':    im,         # for visualization\n",
        "            'agnostic' : agnostic\n",
        "            }\n",
        "        \n",
        "        return result\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.im_names)\n",
        "    \n",
        "\n",
        "class CPDataLoader(object):\n",
        "    def __init__(self, opt, dataset):\n",
        "        super(CPDataLoader, self).__init__()\n",
        "        if opt.shuffle :\n",
        "            train_sampler = torch.utils.data.sampler.RandomSampler(dataset)\n",
        "        else:\n",
        "            train_sampler = None\n",
        "\n",
        "        self.data_loader = torch.utils.data.DataLoader(\n",
        "                dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),\n",
        "                num_workers=opt.workers, pin_memory=True, drop_last=True, sampler=train_sampler)\n",
        "        self.dataset = dataset\n",
        "        self.data_iter = self.data_loader.__iter__()\n",
        "\n",
        "    def next_batch(self):\n",
        "        try:\n",
        "            batch = self.data_iter.__next__()\n",
        "        except StopIteration:\n",
        "            self.data_iter = self.data_loader.__iter__()\n",
        "            batch = self.data_iter.__next__()\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iFA_tMkRGpH",
        "outputId": "0ac43231-5650-4818-f00f-0d41f07b6c72"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/HR-VITON/cp_dataset_test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run app"
      ],
      "metadata": {
        "id": "GnK_R_lXRcNJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "bO5lhCjRsqTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280539d2-9c85-49fe-f8a0-4ab3f69eb16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def load_image():\n",
        "    col1, col2 = st.columns(2)\n",
        "    \n",
        "    image_human_name = ''\n",
        "    image_shirt_name = ''\n",
        "    with col1:\n",
        "        st.header(\"Your image\")\n",
        "        st.text('You need to upload your photo to try on the clothes')\n",
        "        path = '/content/human_input/'\n",
        "        option = st.selectbox(\n",
        "        \"How do you want to import photos?\",\n",
        "        (\"Upload from your device\", \"Link human image\"),\n",
        "        )\n",
        "        if option == 'Upload from your device':\n",
        "            image_human = st.file_uploader(label='Pick an image of the human')\n",
        "            if image_human is not None:\n",
        "                image_human_name = 'aperson.jpg'\n",
        "                with open(os.path.join(path,image_human_name),\"wb+\") as f:\n",
        "                    f.write(image_human.getbuffer())\n",
        "                image_human = Image.open(image_human)\n",
        "                image_human = image_human.resize((200, 300))\n",
        "                st.image(image_human)\n",
        "        else:\n",
        "            link_image = st.text_input(\n",
        "                \"Enter link of your image 👇\",\n",
        "            )\n",
        "            if link_image:\n",
        "                try:\n",
        "                    response = requests.get(link_image)\n",
        "                    if response.status_code == 200:\n",
        "                        image_human = Image.open(BytesIO(response.content))\n",
        "                        image_human = image_human.resize((200, 300))\n",
        "                        st.image(link_image, caption='Valid image URL')\n",
        "                        image_human_name = \"a\"\n",
        "                        with open(os.path.join(path,image_human_name),\"wb+\") as f:\n",
        "                            f.write(image_human.getbuffer())\n",
        "                    else:\n",
        "                        st.write('Invalid image URL')\n",
        "                except:\n",
        "                    st.write('Invalid image URL') \n",
        "\n",
        "\n",
        "\n",
        "    with col2:\n",
        "        st.header(\"Your shirt\")\n",
        "        st.text('You need to upload a photo of the shirt you want to try on')\n",
        "        path = '/content/shirt_input/'\n",
        "        option = st.selectbox(\n",
        "        \"How do you want to import photos?\",\n",
        "        (\"Upload from your device\", \"Link shirt image\"),\n",
        "        )\n",
        "        if option == 'Upload from your device':\n",
        "            image_shirt = st.file_uploader(label='Pick an image of the shirt')\n",
        "            if image_shirt is not None:\n",
        "                image_shirt_name = 'ashirt.png'\n",
        "                print(image_shirt_name)\n",
        "                with open(os.path.join(path, image_shirt_name),\"wb+\") as f:\n",
        "                    f.write(image_shirt.getbuffer())\n",
        "                image_shirt = Image.open(image_shirt)\n",
        "                image_shirt = image_shirt.resize((200, 300))\n",
        "                st.image(image_shirt)\n",
        "        else:\n",
        "            link_image = st.text_input(\n",
        "                \"Enter link of your shirt image 👇\",\n",
        "            )\n",
        "            if link_image:\n",
        "                try:\n",
        "                    response = requests.get(link_image)\n",
        "                    if response.status_code == 200:\n",
        "                        image_shirt = Image.open(BytesIO(response.content))\n",
        "                        image_shirt = image_shirt.resize((200, 300))\n",
        "                        st.image(link_image, caption='Valid image URL')\n",
        "                        image_shirt_name = \"a\"\n",
        "                        with open(os.path.join(path, 'ashirt.png'),\"wb+\") as f:\n",
        "                            f.write(image_shirt.getbuffer())\n",
        "                    else:\n",
        "                        st.write('Invalid image URL')\n",
        "                except:\n",
        "                    st.write('Invalid image URL')  \n",
        "    return image_human_name, image_shirt_name\n",
        "\n",
        "def load_result():\n",
        "    st.header(\"Your result\")\n",
        "    st.image(\"https://static.streamlit.io/examples/dog.jpg\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    human, shirt = \"\", \"\"\n",
        "    human, shirt = load_image()\n",
        "    print(human, shirt)\n",
        "    if st.button(\"Run application\"):\n",
        "        if human ==\"\" or shirt ==\"\":\n",
        "            st.error('You must have human image and shirt image', icon=\"🚨\")\n",
        "        else:\n",
        "            subprocess.run([\"python\", \"openpose.py\"])\n",
        "            print(\"Hihi\")\n",
        "            subprocess.run([\"bash\", \"openpose.sh\"])\n",
        "            subprocess.run([\"python\", \"humanparse1.py\"])\n",
        "            subprocess.run([\"bash\", \"humanparse1.sh\"])\n",
        "            subprocess.run([\"python\", \"humanparse2.py\"])\n",
        "            subprocess.run([\"bash\", \"humanparse2.sh\"])\n",
        "            subprocess.run([\"bash\", \"humanparse3.sh\"])\n",
        "            subprocess.run([\"python\", \"densepose1.py\"])\n",
        "            # subprocess.run([\"bash\", \"densepose.sh\"])\n",
        "            subprocess.run([\"python\", \"densepose2.py\"])\n",
        "            subprocess.run([\"python\", \"clothmask.py\"])\n",
        "            subprocess.run([\"python\", \"viton.py\"])\n",
        "            subprocess.run([\"bash\", \"viton.sh\"])\n",
        "            st.header(\"Your result\")\n",
        "            if 'res.png' in os.listdir('/content'):\n",
        "                img = Image.open('/content/res.png')\n",
        "                st.image(img)\n",
        "                os.remove('/content/shirt_input/'+shirt)\n",
        "                os.remove('/content/human_input/'+human)\n",
        "\n",
        "st.title(\"Minathon 2023\")\n",
        "st.header(\"Team name: mInAThoN 2o2E\")\n",
        "st.header(\"My project: Virtual Try-On\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/Output\n",
        "!mkdir /content/Output"
      ],
      "metadata": {
        "id": "azP7BdOYfEuW"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "9FRwOfc2lkGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e7cbee-ab9f-43dc-d887-7ebab7ca356d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from pyngrok) (5.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19790 sha256=0022468e595a3944eecd66b4eb3210e71947264b62184ed6a7d2612d8188580c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/89/59/49d4249e00957e94813ac136a335d10ed2e09a856c5096f95c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2OAUGl1HDHvJlopyYozcvQj03mi_2z1yRNEHPY2QiTWtrCc7c\") #ngrok.com"
      ],
      "metadata": {
        "id": "n8zaknEellRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6493412f-ba34-4738-f554-54fc7f805a9e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py --server.port 80 &\n",
        "url = ngrok.connect(port = '80')\n",
        "print(url)"
      ],
      "metadata": {
        "id": "JGnyavzvlpZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5305f83a-d4c5-4219-c757-a73814f16e4a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "NgrokTunnel: \"http://27d5-34-142-179-200.ngrok-free.app\" -> \"http://localhost:80\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.run([\"python\", \"openpose.py\"])\n",
        "print(\"Hihi\")\n",
        "subprocess.run([\"bash\", \"openpose.sh\"])\n",
        "subprocess.run([\"python\", \"humanparse1.py\"])\n",
        "subprocess.run([\"bash\", \"humanparse1.sh\"])\n",
        "subprocess.run([\"python\", \"humanparse2.py\"])\n",
        "subprocess.run([\"bash\", \"humanparse2.sh\"])\n",
        "# subprocess.run([\"bash\", \"humanparse3.sh\"])\n",
        "subprocess.run([\"python\", \"densepose1.py\"])\n",
        "# subprocess.run([\"bash\", \"densepose.sh\"])\n",
        "subprocess.run([\"python\", \"densepose2.py\"])\n",
        "subprocess.run([\"python\", \"clothmask.py\"])\n",
        "subprocess.run([\"python\", \"viton.py\"])\n",
        "subprocess.run([\"bash\", \"viton.sh\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK_mZ72xB5l1",
        "outputId": "abfcac1e-5d61-4070-c582-e4d1b9f99174"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hihi\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['bash', 'viton.sh'], returncode=1)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}